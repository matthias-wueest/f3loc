{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#!/usr/bin/env python\n","# coding: utf-8"]},{"cell_type":"markdown","metadata":{},"source":["In[ ]:"]},{"cell_type":"markdown","metadata":{},"source":["In[1]:"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import argparse\n","import os\n","import time"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import torch\n","import tqdm\n","import yaml\n","from attrdict import AttrDict\n","from torch.utils.data import DataLoader\n","import matplotlib.image as mpimg "]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/cluster/home/wueestm/anaconda3/envs/f3loc/lib/python3.8/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n","  warnings.warn(\"Can't initialize NVML\")\n"]}],"source":["from modules.comp.comp_d_net_pl import *\n","from modules.mono.depth_net_pl import *\n","from modules.mv.mv_depth_net_pl import *\n","from utils.data_utils import *\n","from utils.localization_utils import *\n","#from utils.data_utils import TrajDataset_hge_customized_cropped"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["from src.helper_functions import *\n","from src.depth_image_functions import *"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["### Test"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["after first imports\n"]}],"source":["import cv2\n","import numpy as np\n","#### prepare data\n","#rgb_file = 'data/kitti_demo/rgb/0000000050.png'\n","#depth_file = 'data/kitti_demo/depth/0000000050.png'\n","#intrinsic = [707.0493, 707.0493, 604.0814, 180.5066]\n","#gt_depth_scale = 256.0\n","print(\"after first imports\")\n","#model_name_ls = [\"ViT-Small\", \"ViT-Large\", \"ViT-giant2\"]\n","#rgb_filename_ls = ['00000-0', '00090-0', '00120-0']\n","model_name_ls = [\"ViT-Large\"]\n","rgb_filename_ls = ['00000-0', '00090-0', '00120-0']\n","#rgb_filename_ls = ['aligned_image_0', 'aligned_image_90', 'aligned_image_120']\n","\n","rgb_filename = '00000-0'#, '00090-0', '00120-0'\n","\n","rgb_file = 'metric3d/data/hge_customized_complete/non-aligned/rgb/' + rgb_filename + \".png\"\n","rgb_origin = cv2.imread(rgb_file)[:, :, ::-1]"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["image_path = rgb_file\n","img_l = cv2.imread(image_path, cv2.IMREAD_COLOR)\n","img_l = cv2.cvtColor(img_l, cv2.COLOR_BGR2RGB)\n"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"data":{"text/plain":["(1920, 1440, 3)"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["rgb_origin.shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["###"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["In[ ]:"]},{"cell_type":"markdown","metadata":{},"source":["In[2]:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["net_type = \"d\"\n","#dataset = \"gibson_t\" # \"gibson_g\" # \n","dataset = \"hge_customized_cropped\"\n","#dataset = \"hge_customized_complete\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if dataset == \"gibson_t\":\n","    dataset_path = \"/cluster/project/cvg/data/gibson/Gibson_Floorplan_Localization_Dataset\"\n","    #evol_path = \"./evol_path/gibson_f/gt\" #evol_path = \"./evol_path/gibson_f/mono\"\n","    evol_path = \"./evol_path/gibson_f/\" #evol_path = \"./evol_path/gibson_f/mono\"\n","    desdf_resolution = 0.1\n","    orn_slice = 36\n","elif dataset == \"hge_customized_cropped\":\n","    dataset_path = \"/cluster/project/cvg/data/lamar/HGE_customized_cropped\"\n","    evol_path = \"./evol_path/hge_customized_cropped/gt\"\n","    desdf_resolution = 0.1\n","    orn_slice = 36\n","    #desdf_resolution = 1\n","    #orn_slice = 360\n","elif dataset ==\"hge_customized_complete\":\n","    dataset_path = \"/cluster/project/cvg/data/lamar/HGE_customized_complete\"\n","    evol_path = \"./evol_path/hge_customized_complete/gt/gravity_align\"\n","    desdf_resolution = 0.1\n","    orn_slice = 36"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["ckpt_path = \"./logs\"\n","checkpoint_path = \"./tb_logs/my_model/version_42/checkpoints/epoch=19-step=19100.ckpt\"\n","#checkpoint_path = \"./tb_logs/my_model/version_48/checkpoints/epoch=26-step=25785.ckpt\"\n","traj_len = 100#8#100#100#50"]},{"cell_type":"markdown","metadata":{},"source":["In[3]:"]},{"cell_type":"markdown","metadata":{},"source":["get device"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(\"======= USING DEVICE : \", device, \" =======\")"]},{"cell_type":"markdown","metadata":{},"source":["In[4]:"]},{"cell_type":"markdown","metadata":{},"source":["paths"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dataset_dir = os.path.join(dataset_path, dataset)\n","depth_dir = dataset_dir#args.dataset_path\n","log_dir = ckpt_path\n","desdf_path = os.path.join(dataset_path, \"desdf\")\n","evol_path = evol_path"]},{"cell_type":"markdown","metadata":{},"source":["In[5]:"]},{"cell_type":"markdown","metadata":{},"source":["depth file suffix"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if (net_type == \"d\") & ((dataset == \"hge_customized_cropped\") or (dataset ==\"hge_customized_complete\")):\n","    depth_suffix = \"depth90\"\n","elif net_type == \"d\":\n","    depth_suffix = \"depth40\"\n","else:\n","    depth_suffix = \"depth160\""]},{"cell_type":"markdown","metadata":{},"source":["In[6]:"]},{"cell_type":"markdown","metadata":{},"source":["instantiate dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["traj_l = traj_len\n","split_file = os.path.join(dataset_dir, \"split.yaml\")\n","with open(split_file, \"r\") as f:\n","    split = AttrDict(yaml.safe_load(f))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if ((dataset == \"hge_customized_cropped\") or (dataset ==\"hge_customized_complete\")):\n","\n","    # test_set = TrajDataset_hge_customized_cropped(\n","    #     dataset_dir,\n","    #     split.test,\n","    #     L=traj_l,\n","    #     depth_dir=depth_dir,\n","    #     depth_suffix=depth_suffix,\n","    #     add_rp=False,\n","    #     roll=0,\n","    #     pitch=0,\n","    #     without_depth=False, #without_depth=True,  \n","    # )\n","    # test_set = TrajDataset_hge_customized_cropped_gravity_align(\n","    #     dataset_dir,\n","    #     split.test,\n","    #     L=traj_l,\n","    #     depth_dir=depth_dir,\n","    #     depth_suffix=depth_suffix,\n","    #     add_rp=False,\n","    #     roll=0,\n","    #     pitch=0,\n","    #     without_depth=False, #without_depth=True,  \n","    # )\n","    test_set = TrajDataset_hge_customized_metric3d(\n","        dataset_dir,\n","        split.test,\n","        L=traj_l,\n","        depth_dir=depth_dir,\n","        depth_suffix=depth_suffix,\n","        add_rp=True,\n","        roll=0.000001,\n","        pitch=0.000001,\n","        without_depth=False, #without_depth=True,  \n","    )\n","else:\n","    test_set = TrajDataset(\n","        dataset_dir,\n","        split.test,\n","        L=traj_l,\n","        depth_dir=depth_dir,\n","        depth_suffix=depth_suffix,\n","        add_rp=False,\n","        roll=0,\n","        pitch=0,\n","        without_depth=False, #without_depth=True,  \n","    )  "]},{"cell_type":"markdown","metadata":{},"source":["In[ ]:"]},{"cell_type":"markdown","metadata":{},"source":["In[7]:"]},{"cell_type":"markdown","metadata":{},"source":["logs"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["log_error = True\n","log_timing = True\n","log_extended = True"]},{"cell_type":"markdown","metadata":{},"source":["In[8]:"]},{"cell_type":"markdown","metadata":{},"source":["# parameters"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["L = 3  # number of the source frames\n","# D = 128  # number of the depth planes\n","# d_min = 0.1  # minimum depth\n","# d_max = 15.0  # maximum depth\n","# d_hyp = -0.2  # depth transform (uniform sampling in d**d_hyp)\n","F_W = 3 / 8  # camera intrinsic, focal length / image width\n","trans_thresh = 0.005  # translation threshold (variance) if using comp_s"]},{"cell_type":"markdown","metadata":{},"source":["parameters<br>\n","L = 0  # number of the source frames"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["D = 128  # number of depth planes\n","d_min = 0.1  # minimum depth\n","d_max = 100  # maximum depth\n","d_hyp = 0.2  # depth transform (uniform sampling in d**d_hyp)"]},{"cell_type":"markdown","metadata":{},"source":["In[9]:"]},{"cell_type":"markdown","metadata":{},"source":["models"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if net_type == \"mvd\" or net_type == \"comp_s\":\n","    # instaciate model\n","    mv_net = mv_depth_net_pl.load_from_checkpoint(\n","        checkpoint_path=os.path.join(log_dir, \"mv.ckpt\"),\n","        D=D,\n","        d_min=d_min,\n","        d_max=d_max,\n","        d_hyp=d_hyp,\n","    ).to(device)\n","if net_type == \"d\" or net_type == \"comp_s\":\n","    # instaciate model\n","    #checkpoint_path = os.path.join(log_dir, \"mono.ckpt\")\n","    d_net = depth_net_pl.load_from_checkpoint(\n","        checkpoint_path=checkpoint_path,\n","        d_min=d_min,\n","        d_max=d_max,\n","        d_hyp=d_hyp,\n","        D=D,\n","    ).to(device)\n","if net_type == \"comp\":\n","    mv_net_pl = mv_depth_net_pl(D=D, d_hyp=d_hyp, F_W=F_W)\n","    mono_net_pl = depth_net_pl(d_min=d_min, d_max=d_max, d_hyp=d_hyp, D=D, F_W=F_W)\n","    comp_net = comp_d_net_pl.load_from_checkpoint(\n","        checkpoint_path=os.path.join(log_dir, \"comp.ckpt\"),\n","        mv_net=mv_net_pl.net,\n","        mono_net=mono_net_pl.encoder,\n","        L=L,\n","        d_min=d_min,\n","        d_max=d_max,\n","        d_hyp=d_hyp,\n","        D=D,\n","        F_W=F_W,\n","        use_pred=True,\n","    ).to(device)\n","#     comp_net.eval()  # this is needed to disable batchnorm"]},{"cell_type":"markdown","metadata":{},"source":["In[10]:"]},{"cell_type":"markdown","metadata":{},"source":["get desdf for the scene"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"load desdf ...\")\n","desdfs = {}\n","# for scene in tqdm.tqdm(test_set.scene_names):\n","#     desdfs[scene] = np.load(\n","#         os.path.join(desdf_path, scene, \"desdf.npy\"), allow_pickle=True\n","#     ).item()\n","# #    desdfs[scene][\"desdf\"][desdfs[scene][\"desdf\"] > 10] = 10  # truncate\n","# print(\"desdf shape: \", desdfs[scene][\"desdf\"].shape)"]},{"cell_type":"markdown","metadata":{},"source":["# Improved i.t.o. memory"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["scene = test_set.scene_names[0]\n","desdfs[\"desdf\"] = np.load(\n","        os.path.join(desdf_path, scene, \"desdf.npy\"), allow_pickle=True\n","    ).item()\n","###"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if ((dataset == \"hge_customized_cropped\") or (dataset ==\"hge_customized_complete\")):\n","\n","    # Correspondences for calibration\n","    floorplan_correspondences = np.array([[1516, 490.5], [1532, 2162], [350, 1515], [391, 135]])\n","    trajectory_correspondences = np.array([[-12.600, 10.103], [77.981, 9.882], [42.641, -54.348], [-33.099, -51.229]])\n","\n","    # Test the transformation on the source points\n","    pts_src = trajectory_correspondences\n","    pts_dst = floorplan_correspondences\n","    affine_matrix = find_affine_transform(pts_src, pts_dst)\n","    transformed_pts = np.array([apply_affine_transformation(pt, affine_matrix) for pt in pts_src])\n","    pixel_per_meter = (affine_matrix[0,1] + affine_matrix[1,0]) / 2\n","    print(\"Affine transformation matrix:\\n\", affine_matrix)\n","    print(\"Destination points:\\n\", pts_dst)\n","    print(\"Transformed points:\\n\", transformed_pts)\n","    print(\"pixel_per_meter:\", pixel_per_meter)"]},{"cell_type":"markdown","metadata":{},"source":["In[14]:"]},{"cell_type":"markdown","metadata":{},"source":["get the ground truth pose file"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"load poses and maps ...\")\n","maps = {}\n","gt_poses = {}\n","for scene in tqdm.tqdm(test_set.scene_names):\n","    # load map\n","    occ = cv2.imread(os.path.join(dataset_dir, scene, \"map.png\"))[:, :, 0]\n","    maps[scene] = occ\n","    h = occ.shape[0]\n","    w = occ.shape[1]\n","\n","    # single trajectory\n","    poses = np.zeros([0, 3], dtype=np.float32)\n","    # get poses\n","    poses_file = os.path.join(dataset_dir, scene, \"poses.txt\")\n","\n","    # read poses\n","    with open(poses_file, \"r\") as f:\n","        poses_txt = [line.strip() for line in f.readlines()]\n","    traj_len = len(poses_txt)\n","    traj_len -= traj_len % traj_l\n","    for state_id in range(traj_len):\n","        if dataset == \"hge_customized_cropped\":\n","            # get pose\n","            pose_ls = poses_txt[state_id].split(\" \")\n","            pose = np.array(pose_ls, dtype=np.float32)\n","            # world to map \n","            (x,y), th = world_to_map(position_world=pose[:2], \n","                                        orientation_world_rad=pose[2], \n","                                        affine_matrix=affine_matrix, \n","                                        floorplan_correspondences=floorplan_correspondences)\n","        elif dataset ==\"hge_customized_complete\":\n","            # get pose\n","            pose_ls = poses_txt[state_id].split(\" \")\n","            pose = np.array(pose_ls, dtype=np.float32)\n","            # world to map \n","            (x,y), th = world_to_map_hge_complete(position_world=pose[:2], \n","                                        orientation_world_rad=pose[2], \n","                                        affine_matrix=affine_matrix)\n","        else:\n","            # get pose\n","            pose = poses_txt[state_id].split(\" \")\n","            x = float(pose[0])\n","            y = float(pose[1])\n","            th = float(pose[2])\n","            # from world coordinate to map coordinate\n","            x = x / 0.01 + w / 2\n","            y = y / 0.01 + h / 2\n","        poses = np.concatenate(\n","            (poses, np.expand_dims(np.array((x, y, th), dtype=np.float32), 0)),\n","            axis=0,\n","        )\n","    gt_poses[scene] = poses"]},{"cell_type":"markdown","metadata":{},"source":["In[ ]:"]},{"cell_type":"markdown","metadata":{},"source":["In[ ]:"]},{"cell_type":"markdown","metadata":{},"source":["original_resolution = 0.01<br>\n","desdf_resolution = 0.1<br>\n","resolution_ratio = desdf_resolution / original_resolution<br>\n","resolution_ratio"]},{"cell_type":"markdown","metadata":{},"source":["In[ ]:"]},{"cell_type":"markdown","metadata":{},"source":["original_resolution = 1/pixel_per_meter<br>\n","desdf_resolution = 1<br>\n","resolution_ratio = desdf_resolution / original_resolution<br>\n","resolution_ratio"]},{"cell_type":"markdown","metadata":{},"source":["In[15]:"]},{"cell_type":"markdown","metadata":{},"source":["record stats"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["RMSEs = []\n","success_10 = []  # Success @ 1m\n","success_5 = []  # Success @ 0.5m\n","success_3 = []  # Success @ 0.3m\n","success_2 = []  # Success @ 0.2m"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["matching_time = 0\n","iteration_time = 0\n","feature_extraction_time = 0\n","n_iter = 0"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"Length Test Set: \", len(test_set))"]},{"cell_type":"markdown","metadata":{},"source":["loop the over scenes"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"length of test_set: \", len(test_set))\n","#for data_idx in tqdm.tqdm(range(36, len(test_set))):\n","#for data_idx in tqdm.tqdm(range(len(test_set))):\n","for data_idx in tqdm.tqdm(range(0,1)):\n","#for data_idx in tqdm.tqdm(range(5,len(test_set))):\n","#for data_idx in tqdm.tqdm(range(16,len(test_set))):\n","    print(\"Before loading data\")\n","    data = test_set[data_idx]\n","    print(\"After loading data\")\n","    # get the scene name according to the data_idx\n","    scene_idx = np.sum(data_idx >= np.array(test_set.scene_start_idx)) - 1\n","    scene = test_set.scene_names[scene_idx]\n","\n","    # get idx within scene\n","    idx_within_scene = data_idx - test_set.scene_start_idx[scene_idx]\n","\n","    # get desdf\n","    #desdf = desdfs[scene]\n","    desdf = desdfs[\"desdf\"]\n","    print(\"After getting desdf\")\n","\n","    # get reference pose in map coordinate and in scene coordinate\n","    poses_map = gt_poses[scene][\n","        idx_within_scene * traj_l : idx_within_scene * traj_l + traj_l, :\n","    ]\n","\n","    # map to desdf \n","    if ((dataset == \"hge_customized_cropped\") or (dataset ==\"hge_customized_complete\")):\n","        original_resolution = 1/pixel_per_meter\n","        resolution_ratio = desdf_resolution / original_resolution\n","        gt_pose_desdf = poses_map.copy()\n","        gt_pose_desdf[:, 0] = (gt_pose_desdf[:, 0] - desdf[\"l\"]) / resolution_ratio\n","        gt_pose_desdf[:, 1] = (gt_pose_desdf[:, 1] - desdf[\"t\"]) / resolution_ratio\n","        gt_pose_desdf[:, 2] = gt_pose_desdf[:, 2]\n","        masks = torch.tensor(data[\"masks\"], device=device).unsqueeze(0)\n","    else:\n","        original_resolution = 0.01\n","        gt_pose_desdf = poses_map.copy()\n","        gt_pose_desdf[:, 0] = (gt_pose_desdf[:, 0] - desdf[\"l\"]) / 10\n","        gt_pose_desdf[:, 1] = (gt_pose_desdf[:, 1] - desdf[\"t\"]) / 10\n","    print(\"After map to desdf\")\n","    imgs = torch.tensor(data[\"imgs\"], device=device).unsqueeze(0)\n","    print(len(data[\"poses\"]))\n","    poses = torch.tensor(data[\"poses\"], device=device).unsqueeze(0)\n","    print(\"tensor poses\")\n","\n","    # set prior as uniform distribution\n","    prior = torch.tensor(\n","        np.ones_like(desdf[\"desdf\"]) / desdf[\"desdf\"].size, device=device\n","    ).to(torch.float32)\n","    print(\"prior loaded to gpu\")\n","    pred_poses_map = []\n","\n","    # record stats extended: per trajectory\n","    if (evol_path is not None) and (log_extended==True):\n","        metric_depth_l1_loss_ls = []\n","        metric_depth_shape_loss_ls = []\n","        metric_ray_l1_loss_ls = []\n","        metric_observation_position_err_ls = []\n","        metric_observation_orientation_err_ls = []\n","        metric_posterior_position_err_ls = []\n","        metric_posterior_orientation_err_ls = []\n","        bf_current_pose_ls = []\n","        bf_transition_ls = []\n","    print(\"start looping over sequences\")\n","\n","\n","\n","    # loop over the sequences\n","    for t in range(90, 91): #range(traj_l - L):\n","        print(\"t = \", t)\n","        start_iter = time.time()\n","        feature_extraction_start = time.time()\n","        # form input\n","        input_dict = {}\n","        if net_type == \"mvd\" or net_type == \"comp\" or net_type == \"comp_s\":\n","            input_dict.update(\n","                {\n","                    \"ref_img\": imgs[:, t + L, :, :, :],\n","                    \"src_img\": imgs[:, t : t + L, :, :, :],\n","                    \"ref_pose\": poses[:, t + L, :],\n","                    \"src_pose\": poses[:, t : t + L, :],\n","                    \"ref_mask\": None,  # no masks because the dataset has zero roll pitch\n","                    \"src_mask\": None,  # no masks because the dataset has zero roll pitch\n","                }\n","            )\n","        if net_type == \"d\" or net_type == \"comp_s\":\n","            input_dict.update(\n","                {\n","                    \"img\": imgs[:, t + L, :, :, :],\n","                    #\"mask\": None,  # no masks because the dataset has zero roll pitch\n","                    \"mask\": masks[:, t + L, :, :],\n","                }\n","            )\n","        # check which model to use if hardcoded selection\n","        if net_type == \"comp_s\":\n","            # calculate the relative poses\n","            pose_var = (\n","                torch.cat(\n","                    (input_dict[\"ref_pose\"].unsqueeze(1), input_dict[\"src_pose\"]),\n","                    dim=1,\n","                )\n","                .squeeze(0)\n","                .var(dim=0)[:2]\n","                .sum()\n","            )\n","            if pose_var < trans_thresh:\n","                use_mv = False\n","                use_mono = True\n","            else:\n","                use_mv = True\n","                use_mono = False\n","        #print(\"Before inference\")\n","\n","        # inference\n","        if net_type == \"mvd\" or (net_type == \"comp_s\" and use_mv):\n","            pred_dict = mv_net.net(input_dict)\n","            pred_depths = pred_dict[\"d\"]\n","            pred_depths = pred_depths.squeeze(0).detach().cpu().numpy()\n","        elif net_type == \"d\" or (net_type == \"comp_s\" and use_mono):\n","            # ### Trained model:\n","            # pred_depths, attn_2d, prob = d_net.encoder(\n","            #     input_dict[\"img\"], input_dict[\"mask\"]\n","            # )\n","            # pred_depths = pred_depths.squeeze(0).detach().cpu().numpy()\n","            # ###\n","\n","            ### GT:\n","            pred_depths = np.array(data[\"gt_depth\"])[t + L,:]\n","            ###\n","\n","            #print(type(pred_depths))\n","            #print(pred_depths.shape)\n","        elif net_type == \"comp\":\n","            pred_dict = comp_net.comp_d_net(input_dict)\n","            pred_depths = pred_dict[\"d_comp\"]\n","            pred_depths = pred_depths.squeeze(0).detach().cpu().numpy()\n","        print(\"Got pred_depths\")\n","\n","        #pred_depths = pred_depths.squeeze(0).detach().cpu().numpy()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["gt_depths = np.array(data[\"gt_depth\"])[t,:]\n","gt_depths"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["euler_angles = data[\"euler_angles\"][t]# + L]\n","euler_angles"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["depth_image = np.load(\"metric3d/data/hge_customized_complete/non-aligned/pred_depth/ViT-Large/pred_depth_00090-0.npy\")\n","\n","if True:\n","    # Plot and save the depth map\n","    plt.figure(figsize=(4, 4))\n","    plt.imshow(depth_image, cmap='gray', vmin=0, vmax=40)  # Use 'gray' colormap\n","    plt.axis('off')  # Hide the axes\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["roll = euler_angles[0]\n","pitch = euler_angles[1]\n","yaw = 0#euler_angles[2]#+ np.pi/2\n","\n","#gravity_align(imgs[l, :, :, :], r=pitch, p=-(roll+np.pi/2), mode=1, K=self.K)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["aligned_depth_image = gravity_align_depth_optimized(depth_image, r=pitch, p=-(roll+np.pi/2), K=np.array([[1596, 0, 960], [0, 1596, 720], [0, 0, 1]]))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#aligned_depth_image = gravity_align(depth_image, r=pitch, p=-(roll+np.pi/2), K=np.array([[1596, 0, 960], [0, 1596, 720], [0, 0, 1]]), mode=1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if True:\n","    # Plot and save the depth map\n","    plt.figure(figsize=(4, 4))\n","    plt.imshow(aligned_depth_image, cmap='gray', vmin=0, vmax=40)  # Use 'gray' colormap\n","    plt.axis('off')  # Hide the axes\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.figure(figsize=(4*1440/1960, 4))\n","\n","p_arr = np.arange(50, 110, 10)\n","\n","for p in p_arr:\n","    #p = 90  \n","    percentiles = column_percentile_and_downsample(aligned_depth_image, p)\n","    plt.plot(percentiles, label = str(p))\n","    \n","plt.plot(gt_depths, label = \"gt\", color=\"k\")\n","plt.legend()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","\n","def column_percentile_and_downsample(depth_img, p):\n","    \"\"\"\n","    Compute the p percentile of all non-zero values per column in the depth image and downsample the result.\n","    Input:\n","        depth_img: input depth image (2D array)\n","        p: percentile (between 0 and 100)\n","    Output:\n","        downsampled_percentiles: downsampled array of p percentiles for each column\n","    \"\"\"\n","    # Initialize an array to store the percentiles\n","    percentiles = np.zeros(depth_img.shape[1])\n","    \n","    # Iterate over each column\n","    for col in range(depth_img.shape[1]):\n","        # Get the non-zero values in the column\n","        non_zero_values = depth_img[:, col][depth_img[:, col] > 0]\n","        \n","        if non_zero_values.size > 0:\n","            # Compute the p percentile for non-zero values\n","            percentiles[col] = np.percentile(non_zero_values, p)\n","        else:\n","            # If there are no non-zero values, set the percentile to zero\n","            percentiles[col] = 0\n","\n","    # Downsample the percentiles array to 1/16 of its original size\n","    downsampled_percentiles = percentiles[::16]\n","\n","    return downsampled_percentiles\n","\n","# Example usage:\n","# aligned_depth_img = ... (your aligned depth image here)\n","# p = 90  # for example, the 90th percentile\n","# downsampled_percentiles = column_percentile_nonzero_and_downsample(aligned_depth_img, p)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","\n","def gravity_align_depth_optimized(depth_img, r, p, K=np.array([[240, 0, 320], [0, 240, 240], [0, 0, 1]]).astype(np.float32)):\n","    \"\"\"\n","    Align the depth image with gravity direction\n","    Input:\n","        depth_img: input depth image\n","        r: roll\n","        p: pitch\n","        K: camera intrinsics\n","    Output:\n","        aligned_depth_img: gravity aligned depth image\n","    \"\"\"\n","    # Calculate R_gc from roll and pitch\n","    p = -p  # This is because the pitch axis of robot and camera is in the opposite direction\n","    cr = np.cos(r)\n","    sr = np.sin(r)\n","    cp = np.cos(p)\n","    sp = np.sin(p)\n","\n","    # Compute R_cg first\n","    # Pitch\n","    R_x = np.array([[1, 0, 0], [0, cp, sp], [0, -sp, cp]])\n","\n","    # Roll\n","    R_z = np.array([[cr, sr, 0], [-sr, cr, 0], [0, 0, 1]])\n","\n","    R_cg = R_z @ R_x\n","    R_gc = R_cg.T\n","\n","    # Get the shape of the depth image\n","    h, w = depth_img.shape\n","\n","    # Generate grid of (u, v) pixel coordinates\n","    u, v = np.meshgrid(np.arange(w), np.arange(h))\n","\n","    # Back-project to 3D\n","    z = depth_img.flatten()\n","    x = (u.flatten() - K[0, 2]) * z / K[0, 0]\n","    y = (v.flatten() - K[1, 2]) * z / K[1, 1]\n","\n","    # Stack to get 3D points\n","    points_3D = np.vstack((x, y, z))\n","\n","    # Rotate points\n","    rotated_points_3D = R_gc @ points_3D\n","\n","    # Project back to 2D\n","    x_rot = (rotated_points_3D[0, :] * K[0, 0] / rotated_points_3D[2, :]) + K[0, 2]\n","    y_rot = (rotated_points_3D[1, :] * K[1, 1] / rotated_points_3D[2, :]) + K[1, 2]\n","    z_rot = rotated_points_3D[2, :]\n","\n","    # Round and cast to int\n","    x_rot = np.round(x_rot).astype(int)\n","    y_rot = np.round(y_rot).astype(int)\n","\n","    # Create an empty aligned depth image\n","    aligned_depth_img = np.zeros_like(depth_img)\n","\n","    # Mask for valid indices\n","    valid_mask = (x_rot >= 0) & (x_rot < w) & (y_rot >= 0) & (y_rot < h)\n","\n","    # Filter valid points\n","    x_rot = x_rot[valid_mask]\n","    y_rot = y_rot[valid_mask]\n","    z_rot = z_rot[valid_mask]\n","\n","    # Use numpy's advanced indexing to assign the values\n","    aligned_depth_img[y_rot, x_rot] = z_rot\n","\n","    return aligned_depth_img\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["aligned_depth_image = gravity_align_depth(depth_image, roll=pitch, pitch=-(roll+np.pi/2), yaw=0)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["aligned_depth_image = gravity_align(depth_image, r=pitch, p=-(roll+np.pi/2), mode=1, K=np.array([[1596, 0, 960], [0, 1596, 720], [0, 0, 1]]))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if True:\n","    # Plot and save the depth map\n","    plt.figure(figsize=(4, 4))\n","    plt.imshow(aligned_depth_image, cmap='gray', vmin=0, vmax=40)  # Use 'gray' colormap\n","    plt.axis('off')  # Hide the axes\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["aligned_depth_image2 = gravity_align2(depth_image, r=pitch, p=-(roll+np.pi/2), mode=1, K=np.array([[1596, 0, 960], [0, 1596, 720], [0, 0, 1]]))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if True:\n","    # Plot and save the depth map\n","    plt.figure(figsize=(4, 4))\n","    plt.imshow(aligned_depth_image2, cmap='gray', vmin=0, vmax=40)  # Use 'gray' colormap\n","    plt.axis('off')  # Hide the axes\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["aligned_depth_image3 = gravity_align3(depth_image, r=pitch, p=-(roll+np.pi/2), K=np.array([[1596, 0, 960], [0, 1596, 720], [0, 0, 1]]))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if True:\n","    # Plot and save the depth map\n","    plt.figure(figsize=(4, 4))\n","    plt.imshow(aligned_depth_image3, cmap='gray', vmin=0, vmax=40)  # Use 'gray' colormap\n","    plt.axis('off')  # Hide the axes\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","import scipy.ndimage\n","\n","def gravity_align5(\n","    depth_img,\n","    r,\n","    p,\n","    K=np.array([[240, 0, 320], [0, 240, 240], [0, 0, 1]]).astype(np.float32),\n","):\n","    \"\"\"\n","    Align the depth image with gravity direction\n","    Input:\n","        depth_img: input depth image\n","        r: roll\n","        p: pitch\n","        K: camera intrinsics\n","    Output:\n","        aligned_depth_img: gravity aligned depth image\n","    \"\"\"\n","    # Calculate R_gc from roll and pitch\n","    p = -p  # This is because the pitch axis of robot and camera is in the opposite direction\n","    cr = np.cos(r)\n","    sr = np.sin(r)\n","    cp = np.cos(p)\n","    sp = np.sin(p)\n","\n","    # Compute R_cg first\n","    # Pitch\n","    R_x = np.array([[1, 0, 0], [0, cp, sp], [0, -sp, cp]])\n","\n","    # Roll\n","    R_z = np.array([[cr, sr, 0], [-sr, cr, 0], [0, 0, 1]])\n","\n","    R_cg = R_z @ R_x\n","    R_gc = R_cg.T\n","\n","    # Get the shape of the depth image\n","    h, w = depth_img.shape\n","\n","    # Generate grid of (u, v) pixel coordinates\n","    u, v = np.meshgrid(np.arange(w), np.arange(h))\n","    \n","    # Flatten the depth image\n","    z = depth_img.flatten()\n","\n","    # Back-project to 3D\n","    x = (u.flatten() - K[0, 2]) * z / K[0, 0]\n","    y = (v.flatten() - K[1, 2]) * z / K[1, 1]\n","\n","    # Stack to get 3D points\n","    points_3D = np.vstack((x, y, z))\n","\n","    # Rotate points\n","    rotated_points_3D = R_gc @ points_3D\n","\n","    # Project back to 2D\n","    x_rot = rotated_points_3D[0, :] * K[0, 0] / rotated_points_3D[2, :] + K[0, 2]\n","    y_rot = rotated_points_3D[1, :] * K[1, 1] / rotated_points_3D[2, :] + K[1, 2]\n","\n","    # Reshape coordinates to match depth image shape\n","    x_rot = x_rot.reshape(h, w)\n","    y_rot = y_rot.reshape(h, w)\n","\n","    # Use map_coordinates for efficient interpolation\n","    aligned_depth_img = scipy.ndimage.map_coordinates(\n","        depth_img, [y_rot, x_rot], order=1, mode='nearest'\n","    )\n","\n","    return aligned_depth_img\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["((aligned_depth_image2 == aligned_depth_image3) == False).sum()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["((aligned_depth_image2 == aligned_depth_image3) == True).sum()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","import cv2\n","\n","def gravity_align3(\n","    depth_img,\n","    r,\n","    p,\n","    K=np.array([[240, 0, 320], [0, 240, 240], [0, 0, 1]]).astype(np.float32),\n","):\n","    \"\"\"\n","    Align the depth image with gravity direction\n","    Input:\n","        depth_img: input depth image\n","        r: roll\n","        p: pitch\n","        K: camera intrinsics\n","    Output:\n","        aligned_depth_img: gravity aligned depth image\n","    \"\"\"\n","    # Calculate R_gc from roll and pitch\n","    p = -p  # This is because the pitch axis of robot and camera is in the opposite direction\n","    cr = np.cos(r)\n","    sr = np.sin(r)\n","    cp = np.cos(p)\n","    sp = np.sin(p)\n","\n","    # Compute R_cg first\n","    # Pitch\n","    R_x = np.array([[1, 0, 0], [0, cp, sp], [0, -sp, cp]])\n","\n","    # Roll\n","    R_z = np.array([[cr, sr, 0], [-sr, cr, 0], [0, 0, 1]])\n","\n","    R_cg = R_z @ R_x\n","    R_gc = R_cg.T\n","\n","    # Get the shape of the depth image\n","    h, w = depth_img.shape\n","\n","    # Generate grid of (u, v) pixel coordinates\n","    u, v = np.meshgrid(np.arange(w), np.arange(h))\n","    u = u.flatten()\n","    v = v.flatten()\n","\n","    # Flatten the depth image\n","    z = depth_img.flatten()\n","\n","    # Back-project to 3D\n","    x = (u - K[0, 2]) * z / K[0, 0]\n","    y = (v - K[1, 2]) * z / K[1, 1]\n","\n","    # Stack to get 3D points\n","    points_3D = np.vstack((x, y, z))\n","\n","    # Rotate points\n","    rotated_points_3D = R_gc @ points_3D\n","\n","    # Project back to 2D\n","    x_rot = rotated_points_3D[0, :] * K[0, 0] / rotated_points_3D[2, :] + K[0, 2]\n","    y_rot = rotated_points_3D[1, :] * K[1, 1] / rotated_points_3D[2, :] + K[1, 2]\n","    z_rot = rotated_points_3D[2, :]\n","\n","    # Create an empty aligned depth image\n","    aligned_depth_img = np.zeros_like(depth_img)\n","\n","    # Fill the aligned depth image\n","    for i in range(len(x_rot)):\n","        x_i = int(round(x_rot[i]))\n","        y_i = int(round(y_rot[i]))\n","        if 0 <= x_i < w and 0 <= y_i < h:\n","            aligned_depth_img[y_i, x_i] = z_rot[i]\n","\n","    return aligned_depth_img\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","import cv2\n","\n","def gravity_align2(\n","    img,\n","    r,\n","    p,\n","    K=np.array([[240, 0, 320], [0, 240, 240], [0, 0, 1]]).astype(np.float32),\n","    mode=1,  # Use nearest-neighbor interpolation for depth images\n","):\n","    \"\"\"\n","    Align the depth image with gravity direction\n","    Input:\n","        img: input depth image\n","        r: roll\n","        p: pitch\n","        K: camera intrinsics\n","        mode: interpolation mode for warping, default: 1 - 'nearest'\n","    Output:\n","        aligned_img: gravity aligned depth image\n","    \"\"\"\n","    # Calculate R_gc from roll and pitch\n","    # From gravity to camera, yaw->pitch->roll\n","    # From camera to gravity, roll->pitch->yaw\n","    p = -p  # This is because the pitch axis of robot and camera is in the opposite direction\n","    cr = np.cos(r)\n","    sr = np.sin(r)\n","    cp = np.cos(p)\n","    sp = np.sin(p)\n","\n","    # Compute R_cg first\n","    # Pitch\n","    R_x = np.array([[1, 0, 0], [0, cp, sp], [0, -sp, cp]])\n","\n","    # Roll\n","    R_z = np.array([[cr, sr, 0], [-sr, cr, 0], [0, 0, 1]])\n","\n","    R_cg = R_z @ R_x\n","    R_gc = R_cg.T\n","\n","    # Get shape\n","    h, w = img.shape[:2]\n","\n","    # Directly compute the homography\n","    persp_M = K @ R_gc @ np.linalg.inv(K)\n","\n","    aligned_img = cv2.warpPerspective(\n","        img, persp_M, (w, h), flags=cv2.INTER_NEAREST  # Use nearest-neighbor interpolation for depth\n","    )\n","\n","    return aligned_img"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def gravity_align(\n","    img,\n","    r,\n","    p,\n","    K=np.array([[240, 0, 320], [0, 240, 240], [0, 0, 1]]).astype(np.float32),\n","    mode=0,\n","):\n","    \"\"\"\n","    Align the image with gravity direction\n","    Input:\n","        img: input image\n","        r: roll\n","        p: pitch\n","        K: camera intrisics\n","        mode: interpolation mode for warping, default: 0 - 'linear', else 1 - 'nearest'\n","    Output:\n","        aligned_img: gravity aligned image\n","    \"\"\"\n","    # calculate R_gc from roll and pitch\n","    # From gravity to camera, yaw->pitch->roll\n","    # From camera to gravity, roll->pitch->yaw\n","    p = (\n","        -p\n","    )  # this is because the pitch axis of robot and camera is in the opposite direction\n","    cr = np.cos(r)\n","    sr = np.sin(r)\n","    cp = np.cos(p)\n","    sp = np.sin(p)\n","\n","    # compute R_cg first\n","    # pitch\n","    R_x = np.array([[1, 0, 0], [0, cp, sp], [0, -sp, cp]])\n","\n","    # roll\n","    R_z = np.array([[cr, sr, 0], [-sr, cr, 0], [0, 0, 1]])\n","\n","    R_cg = R_z @ R_x\n","    R_gc = R_cg.T\n","\n","    # get shape\n","    h, w = list(img.shape[:2])\n","\n","    # directly compute the homography\n","    persp_M = K @ R_gc @ np.linalg.inv(K)\n","\n","    aligned_img = cv2.warpPerspective(\n","        img, persp_M, (w, h), flags=cv2.INTER_NEAREST if mode == 1 else cv2.INTER_LINEAR\n","    )\n","\n","    return aligned_img"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","\n","def get_rotation_matrix(roll, pitch, yaw):\n","    R_x = np.array([\n","        [1, 0, 0],\n","        [0, np.cos(roll), -np.sin(roll)],\n","        [0, np.sin(roll), np.cos(roll)]\n","    ])\n","\n","    R_y = np.array([\n","        [np.cos(pitch), 0, np.sin(pitch)],\n","        [0, 1, 0],\n","        [-np.sin(pitch), 0, np.cos(pitch)]\n","    ])\n","\n","    R_z = np.array([\n","        [np.cos(yaw), -np.sin(yaw), 0],\n","        [np.sin(yaw), np.cos(yaw), 0],\n","        [0, 0, 1]\n","    ])\n","\n","    return R_z @ R_y @ R_x"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def gravity_align_depth(depth_image, roll, pitch, yaw, K=np.array([[1596, 0, 960], [0, 1596, 720], [0, 0, 1]])):\n","    \n","    fx, fy, cx, cy = K[0, 0], K[1, 1], K[0, 2], K[1, 2]\n","\n","    height, width = depth_image.shape\n","    R = get_rotation_matrix(roll, pitch, yaw)\n","    aligned_depth = np.zeros_like(depth_image)\n","\n","    for v in range(height):\n","        for u in range(width):\n","            d = depth_image[v, u]\n","            if d > 0:  # Ignore invalid depth\n","                X = (u - cx) * d / fx\n","                Y = (v - cy) * d / fy\n","                Z = d\n","                \n","                point = np.array([X, Y, Z])\n","                aligned_point = R @ point\n","                \n","                X_prime, Y_prime, Z_prime = aligned_point\n","                u_prime = int(fx * X_prime / Z_prime + cx)\n","                v_prime = int(fy * Y_prime / Z_prime + cy)\n","                \n","                if 0 <= u_prime < width and 0 <= v_prime < height:\n","                    aligned_depth[v_prime, u_prime] = Z_prime\n","\n","    return aligned_depth"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","\n","def get_rotation_matrix(roll, pitch, yaw):\n","    # Assuming get_rotation_matrix is defined elsewhere or needs to be implemented.\n","    # This is a placeholder implementation.\n","    # The correct implementation should be provided based on the context.\n","    Rx = np.array([[1, 0, 0],\n","                   [0, np.cos(roll), -np.sin(roll)],\n","                   [0, np.sin(roll), np.cos(roll)]])\n","    Ry = np.array([[np.cos(pitch), 0, np.sin(pitch)],\n","                   [0, 1, 0],\n","                   [-np.sin(pitch), 0, np.cos(pitch)]])\n","    Rz = np.array([[np.cos(yaw), -np.sin(yaw), 0],\n","                   [np.sin(yaw), np.cos(yaw), 0],\n","                   [0, 0, 1]])\n","    \n","    return Rz @ Ry @ Rx\n","\n","def gravity_align_depth(depth_image, roll, pitch, yaw, K=np.array([[1596, 0, 960], [0, 1596, 720], [0, 0, 1]])):\n","    \n","    fx, fy, cx, cy = K[0, 0], K[1, 1], K[0, 2], K[1, 2]\n","    height, width = depth_image.shape\n","    R = get_rotation_matrix(roll, pitch, yaw)\n","    u, v = np.meshgrid(np.arange(width), np.arange(height))\n","    \n","    valid_mask = depth_image > 0\n","    d = depth_image[valid_mask]\n","    \n","    u_valid = u[valid_mask]\n","    v_valid = v[valid_mask]\n","    \n","    X = (u_valid - cx) * d / fx\n","    Y = (v_valid - cy) * d / fy\n","    Z = d\n","    \n","    points = np.vstack((X, Y, Z)).T\n","    aligned_points = points @ R.T\n","    \n","    X_prime = aligned_points[:, 0]\n","    Y_prime = aligned_points[:, 1]\n","    Z_prime = aligned_points[:, 2]\n","    \n","    u_prime = np.round(fx * X_prime / Z_prime + cx).astype(int)\n","    v_prime = np.round(fy * Y_prime / Z_prime + cy).astype(int)\n","    \n","    valid_primes = (u_prime >= 0) & (u_prime < width) & (v_prime >= 0) & (v_prime < height)\n","    aligned_depth = np.zeros_like(depth_image)\n","    aligned_depth[v_prime[valid_primes], u_prime[valid_primes]] = Z_prime[valid_primes]\n","\n","    return aligned_depth\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pred_depths"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["input_dict.update(\n","                {\n","                    \"img\": imgs[:, t + L, :, :, :],\n","                    #\"mask\": None,  # no masks because the dataset has zero roll pitch\n","                    \"mask\": masks[:, t + L, :, :],\n","                }\n","            )"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["input_dict[\"img\"].shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["input_dict[\"img\"].max()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["input_dict[\"img\"].min()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["image_np.shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["depth_image = get_depth_image_metric3d(rgb_origin=image_np)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# Assuming image_tensor is already loaded with shape (1920, 1440, 3)\n","image_tensor = input_dict[\"img\"][0]\n","\n","# Convert the tensor to a NumPy array with dtype uint8\n","image_np = image_tensor.numpy().astype(np.uint8)\n","\n","# Plot the image\n","plt.imshow(image_np)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["        # get rays from depth\n","        if ((dataset == \"hge_customized_cropped\") or (dataset ==\"hge_customized_complete\")):\n","            fov_desdf = 49#50\n","            dv = 360/orn_slice\n","            V = fov_desdf / dv\n","            pred_rays = get_ray_from_depth(pred_depths, V=V, dv=dv, F_W=1596/1440)\n","            pred_rays = torch.tensor(pred_rays, device=device)\n","        else:\n","            pred_rays = get_ray_from_depth(pred_depths)\n","            pred_rays = torch.tensor(pred_rays, device=device)\n","        feature_extraction_end = time.time()\n","        matching_start = time.time()\n","        print(\"Before localize\")\n","\n","        # use the prediction to localize, produce observation likelihood\n","        if ((dataset == \"hge_customized_cropped\") or (dataset ==\"hge_customized_complete\")):\n","            likelihood, likelihood_2d, likelihood_orn, likelihood_pred = localize_noflip(\n","                torch.tensor(desdf[\"desdf\"]).to(prior.device),\n","                pred_rays.to(prior.device),\n","                return_np=False,\n","                orn_slice=orn_slice,\n","                lambd=40\n","            )\n","        else:\n","            likelihood, likelihood_2d, likelihood_orn, likelihood_pred = localize(\n","                torch.tensor(desdf[\"desdf\"]).to(prior.device),\n","                pred_rays.to(prior.device),\n","                return_np=False,\n","                orn_slice=orn_slice\n","            )\n","            \n","        matching_end = time.time()\n","\n","        # multiply with the prior\n","        posterior = prior * likelihood.to(prior.device)\n","        posterior = posterior / posterior.sum()\n","        \n","        print(f\"prior shape: {prior.shape}\")\n","        print(f\"prior range: {prior.min().item()} to {prior.max().item()}\")\n","        print(f\"prior sum: {prior.sum()}\")\n","        print(f\"likelihood shape: {likelihood.shape}\")\n","        print(f\"likelihood range: {likelihood.min().item()} to {likelihood.max().item()}\")\n","        print(f\"likelihood sum: {likelihood.sum()}\")\n","        print(f\"posterior shape: {posterior.shape}\")\n","        print(f\"posterior range: {posterior.min().item()} to {posterior.max().item()}\")\n","        print(f\"posterior sum: {posterior.sum()}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["        # reduce the posterior along orientation for 2d visualization\n","        posterior_2d, orientations = torch.max(posterior, dim=2)\n","\n","        # compute prior_2d for visualization\n","        prior_2d, _ = torch.max(prior, dim=2)\n","\n","        # maximum of the posterior as result\n","        pose_y, pose_x = torch.where(posterior_2d == posterior_2d.max())\n","        if pose_y.shape[0] > 1:\n","            pose_y = pose_y[0].unsqueeze(0)\n","            pose_x = pose_x[0].unsqueeze(0)\n","        orn = orientations[pose_y, pose_x]\n","\n","        # from orientation indices to radians\n","        orn = orn / orn_slice * 2 * torch.pi\n","        pose = torch.cat((pose_x, pose_y, orn)).detach().cpu().numpy()\n","        if ((dataset == \"hge_customized_cropped\") or (dataset ==\"hge_customized_complete\")):\n","            pose_in_map = pose.copy()\n","            pose_in_map[0] = pose_in_map[0] * resolution_ratio  + desdf[\"l\"]\n","            pose_in_map[1] = pose_in_map[1] * resolution_ratio  + desdf[\"t\"]\n","        else:\n","            pose_in_map = pose.copy()\n","            pose_in_map[0] = pose_in_map[0] * 10 + desdf[\"l\"]\n","            pose_in_map[1] = pose_in_map[1] * 10 + desdf[\"t\"]\n","        pred_poses_map.append(pose_in_map)\n","\n","        #print(\"Before preparing metrics\")\n","        if (evol_path is not None) & (log_extended==True):\n","\n","            #### Prepare Metrics ------------------------------------------------\n","\n","            ### Quality of depth prediction: Predicted depth vs. GT depth\n","            # Get variables\n","            predicted_depths = pred_depths\n","            gt_depths = np.array(data[\"gt_depth\"])[t + L,:]\n","            # Get metrics\n","            metric_depth_l1_loss = F.l1_loss(torch.tensor(predicted_depths), torch.tensor(gt_depths)).item()\n","            metric_depth_shape_loss = F.cosine_similarity(torch.tensor(predicted_depths), torch.tensor(gt_depths), dim=-1).mean().item()\n","\n","            ### Quality of matching: Matched ray vs. Predicted ray\n","            # Get variables\n","            idx_orn = int(likelihood_orn[likelihood_2d == likelihood_2d.max()][0])\n","            idx_x = int(likelihood_pred[1])\n","            idx_y = int(likelihood_pred[0])\n","            desdf_loc = torch.tensor(desdf[\"desdf\"])\n","            V = pred_rays.shape[0]\n","            pad_front = V // 2\n","            pad_back = V - pad_front\n","            pad_desdf = F.pad(desdf_loc, [pad_front, pad_back], mode=\"circular\")\n","            desdf_rays = pad_desdf[idx_x, idx_y, idx_orn: idx_orn + V]\n","            if ((dataset == \"hge_customized_cropped\") or (dataset ==\"hge_customized_complete\")):\n","                matched_rays = desdf_rays.cpu().numpy().copy()\n","            else:\n","                matched_rays = desdf_rays.cpu().numpy()[::-1].copy()  # move to CPU before converting to numpy\n","            predicted_rays = pred_rays.cpu().numpy()  # move pred_rays to CPU before converting to numpy\n","            # Get metrics\n","            metric_ray_l1_loss = F.l1_loss(torch.tensor(matched_rays), torch.tensor(predicted_rays)).item()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["            ### Quality of localization per observation: Predicted pose vs. GT pose\n","            if dataset == \"hge_customized_cropped\":\n","                # Get variables\n","                observation_pose_in_map = np.empty(3)\n","                # desdf to map\n","                observation_pose_in_map[0] = likelihood_pred[0] * resolution_ratio + desdf[\"l\"]\n","                observation_pose_in_map[1] = likelihood_pred[1] * resolution_ratio + desdf[\"t\"]\n","                observation_pose_in_map[2] = likelihood_pred[2]\n","                # map to world\n","                observation_predicted_position, observation_predicted_orientation_rad = map_to_world(observation_pose_in_map[:2], observation_pose_in_map[2], affine_matrix, floorplan_correspondences)\n","                observation_predicted_orientation = observation_predicted_orientation_rad/np.pi*180\n","                gt_position, gt_orientation_rad = map_to_world(poses_map[t + L,:2], poses_map[t + L,2], affine_matrix, floorplan_correspondences)\n","                gt_orientation = gt_orientation_rad/np.pi*180\n","            elif dataset == \"hge_customized_complete\":\n","                # Get variables\n","                observation_pose_in_map = np.empty(3)\n","                # desdf to map\n","                observation_pose_in_map[0] = likelihood_pred[0] * resolution_ratio + desdf[\"l\"]\n","                observation_pose_in_map[1] = likelihood_pred[1] * resolution_ratio + desdf[\"t\"]\n","                observation_pose_in_map[2] = likelihood_pred[2]\n","                # map to world\n","                observation_predicted_position, observation_predicted_orientation_rad = map_to_world_hge_complete(observation_pose_in_map[:2], observation_pose_in_map[2], affine_matrix)\n","                observation_predicted_orientation = observation_predicted_orientation_rad/np.pi*180\n","                gt_position, gt_orientation_rad = map_to_world_hge_complete(poses_map[t + L,:2], poses_map[t + L,2], affine_matrix)\n","                gt_orientation = gt_orientation_rad/np.pi*180\n","            else:\n","                # Get variables\n","                observation_pose_in_map = np.empty(3)\n","                # desdf to map\n","                observation_pose_in_map[0] = likelihood_pred[0] * 10 + desdf[\"l\"]\n","                observation_pose_in_map[1] = likelihood_pred[1] * 10 + desdf[\"t\"]\n","                observation_pose_in_map[2] = likelihood_pred[2]\n","                # map to world\n","                observation_predicted_position = observation_pose_in_map[:2]*0.01\n","                observation_predicted_orientation = observation_pose_in_map[2]/np.pi*180\n","                gt_position = poses_map[t + L,:2]*0.01\n","                gt_orientation = poses_map[t + L,2]/np.pi*180\n","\n","            # Get metrics\n","            metric_observation_position_err = np.linalg.norm(observation_predicted_position - gt_position)\n","            metric_observation_orientation_err = min((observation_predicted_orientation - gt_orientation) % 360, 360 - ((observation_predicted_orientation - gt_orientation) % 360))\n","\n","            ### Quality of localization filter: Predicted posterior pose vs. GT pose\n","            if dataset == \"hge_customized_cropped\":\n","                # Get variables\n","                # map to world\n","                posterior_predicted_position, posterior_predicted_orientation_rad = map_to_world(pose_in_map[:2], pose_in_map[2], affine_matrix, floorplan_correspondences)\n","                posterior_predicted_orientation = posterior_predicted_orientation_rad/np.pi*180\n","                gt_position, gt_orientation_rad = map_to_world(poses_map[t + L,:2], poses_map[t + L,2], affine_matrix, floorplan_correspondences)\n","                gt_orientation = gt_orientation_rad/np.pi*180\n","            elif dataset == \"hge_customized_complete\":\n","                # Get variables\n","                # map to world\n","                posterior_predicted_position, posterior_predicted_orientation_rad = map_to_world_hge_complete(pose_in_map[:2], pose_in_map[2], affine_matrix)\n","                posterior_predicted_orientation = posterior_predicted_orientation_rad/np.pi*180\n","                gt_position, gt_orientation_rad = map_to_world_hge_complete(poses_map[t + L,:2], poses_map[t + L,2], affine_matrix)\n","                gt_orientation = gt_orientation_rad/np.pi*180\n","            else:\n","                # Get variables\n","                # map to world\n","                posterior_predicted_position = pose_in_map[:2]*0.01\n","                posterior_predicted_orientation = pose_in_map[2]/np.pi*180\n","                gt_position = poses_map[t + L,:2]*0.01\n","                gt_orientation = poses_map[t + L,2]/np.pi*180\n","\n","            # Get metrics\n","            metric_posterior_position_err = np.linalg.norm(posterior_predicted_position - gt_position)\n","            metric_posterior_orientation_err = min((posterior_predicted_orientation - gt_orientation) % 360, 360 - (posterior_predicted_orientation - gt_orientation) % 360)\n","\n","            ### Log metrics\n","            metric_depth_l1_loss_ls.append(metric_depth_l1_loss)\n","            metric_depth_shape_loss_ls.append(metric_depth_shape_loss)\n","            metric_ray_l1_loss_ls.append(metric_ray_l1_loss)\n","            metric_observation_position_err_ls.append(metric_observation_position_err)\n","            metric_observation_orientation_err_ls.append(metric_observation_orientation_err)\n","            metric_posterior_position_err_ls.append(metric_posterior_position_err)\n","            metric_posterior_orientation_err_ls.append(metric_posterior_orientation_err)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["            #### Plot Figure ------------------------------------------------\n","            #print(\"Before preparing plots\")\n","            fig = plt.figure(1, figsize=(4*5, 3.4))\n","            fig.clf()\n","\n","            ### Image\n","            ax = fig.add_subplot(1, 5, 1)\n","            if ((dataset == \"hge_customized_cropped\") or (dataset ==\"hge_customized_complete\")):\n","                img_path = os.path.join(dataset_dir, scene, \"rgb\", '{:05d}'.format((idx_within_scene * traj_l) + (t + L)) + \"-0.jpg\")\n","            else:\n","                img_path = os.path.join(dataset_dir, scene, \"rgb\", '{:05d}'.format((idx_within_scene * traj_l) + (t + L)) + \".png\")\n","            img = mpimg.imread(img_path)\n","            ax.imshow(img)\n","            ax.axis('off')\n","            ax.set_title(\"t=\" + str(t))\n","\n","            ### Quality of depth prediction: Predicted depth vs. GT depth\n","            ax = fig.add_subplot(1, 5, 2)\n","            ax.plot(gt_depths, label=\"GT Depths\")\n","            ax.plot(predicted_depths, label=\"Predicted Depths\")\n","            ax.set_title(\"L1 Loss: \" + str(np.round(metric_depth_l1_loss, 2)) + \" / Shape Loss: \" + str(np.round(metric_depth_shape_loss,2)))\n","            ax.legend()\n","            ax.set_xlabel(\"Index\")\n","            ax.set_ylabel(\"Depth [m]\")\n","            if ((dataset == \"hge_customized_cropped\") or (dataset ==\"hge_customized_complete\")):\n","                ax.set_ylim([0,80])  \n","            else:\n","                ax.set_ylim([0,10])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["            ### Quality of matching: Matched ray vs. Predicted ray\n","            ax = fig.add_subplot(1, 5, 3)\n","            ax.plot(matched_rays, label=\"Matched Rays\", marker='o')\n","            ax.plot(predicted_rays, label=\"Predicted Rays\", marker='o')\n","            ax.set_title(\"L1 Loss: \" + str(np.round(metric_ray_l1_loss, 2)))\n","            ax.legend()\n","            ax.set_xlabel(\"Ray Index\")\n","            ax.set_ylabel(\"Length [m]\")\n","            if ((dataset == \"hge_customized_cropped\") or (dataset ==\"hge_customized_complete\")):\n","                ax.set_ylim([0,80])  \n","            else:\n","                ax.set_ylim([0,10])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["            ### Quality of localization per observation: Predicted pose vs. GT pose\n","            s = 0.25\n","            ax = fig.add_subplot(1, 5, 4)\n","            if ((dataset == \"hge_customized_cropped\") or (dataset ==\"hge_customized_complete\")):\n","                ax.imshow(likelihood_2d, cmap=\"coolwarm\")\n","            else:\n","                ax.imshow(likelihood_2d, origin=\"lower\", cmap=\"coolwarm\")\n","            ax.set_title(\"Error: \" + str(np.round(metric_observation_position_err,2)) + \"m / \" + str(np.round(metric_observation_orientation_err,2)) + \"°\")\n","            ax.axis(\"off\")\n","            ax.quiver(\n","                likelihood_pred[0],\n","                likelihood_pred[1],\n","                s*np.cos(likelihood_pred[2]),\n","                s*np.sin(likelihood_pred[2]),\n","                color=\"blue\",\n","                width=s*0.2,\n","                scale_units=\"inches\",\n","                units=\"inches\",\n","                scale=1,\n","                headwidth=3,\n","                headlength=3,\n","                headaxislength=3,\n","                minlength=s*0.1,\n","                angles = \"xy\"\n","            )\n","            ax.quiver(\n","                gt_pose_desdf[t + L, 0],\n","                gt_pose_desdf[t + L, 1],\n","                s*np.cos(gt_pose_desdf[t + L, 2]),\n","                s*np.sin(gt_pose_desdf[t + L, 2]),\n","                color=\"green\",\n","                width=s*0.2,\n","                scale_units=\"inches\",\n","                units=\"inches\",\n","                scale=1,\n","                headwidth=3,\n","                headlength=3,\n","                headaxislength=3,\n","                minlength=s*0.1,\n","                angles = \"xy\"\n","            )\n","\n","            ### Quality of localization per observation: Predicted pose vs. GT pose\n","            ax = fig.add_subplot(1, 5, 5)\n","            if ((dataset == \"hge_customized_cropped\") or (dataset ==\"hge_customized_complete\")):\n","                ax.imshow(posterior_2d.detach().cpu().numpy(), cmap=\"coolwarm\")\n","            else:\n","                ax.imshow(posterior_2d.detach().cpu().numpy(), origin=\"lower\", cmap=\"coolwarm\")\n","            ax.set_title(\"Error: \" + str(np.round(metric_posterior_position_err,2)) + \"m / \" + str(np.round(metric_posterior_orientation_err,2)) + \"°\")\n","            ax.axis(\"off\")\n","            ax.quiver(\n","                pose[0],\n","                pose[1],\n","                s*np.cos(pose[2]),\n","                s*np.sin(pose[2]),\n","                color=\"blue\",\n","                width=s*0.2,\n","                scale_units=\"inches\",\n","                units=\"inches\",\n","                scale=1,\n","                headwidth=3,\n","                headlength=3,\n","                headaxislength=3,\n","                minlength=s*0.1,\n","                angles = \"xy\"\n","            )\n","            ax.quiver(\n","                gt_pose_desdf[t + L, 0],\n","                gt_pose_desdf[t + L, 1],\n","                s*np.cos(gt_pose_desdf[t + L, 2]),\n","                s*np.sin(gt_pose_desdf[t + L, 2]),\n","                color=\"green\",\n","                width=s*0.2,\n","                scale_units=\"inches\",\n","                units=\"inches\",\n","                scale=1,\n","                headwidth=3,\n","                headlength=3,\n","                headaxislength=3,\n","                minlength=s*0.1,\n","                angles = \"xy\"\n","            )\n","            plt.tight_layout()\n","            if not os.path.exists(\n","                os.path.join(evol_path, \"pretty_filter_extended\", str(data_idx))\n","            ):\n","                os.makedirs(os.path.join(evol_path, \"pretty_filter_extended\", str(data_idx)))\n","            fig.savefig(\n","                os.path.join(\n","                    evol_path, \"pretty_filter_extended\", str(data_idx), str(t) + \".png\"\n","                )\n","            )\n","        elif (evol_path is not None) & (log_extended==False):\n","\n","            # plot posterior 2d\n","            fig = plt.figure(0, figsize=(20, 20))\n","            fig.clf()\n","            ax = fig.add_subplot(1, 2, 2)\n","            ax.imshow(\n","                posterior_2d.detach().cpu().numpy(), origin=\"lower\", cmap=\"coolwarm\"\n","            )\n","            ax.quiver(\n","                pose[0],\n","                pose[1],\n","                np.cos(pose[2]),\n","                np.sin(pose[2]),\n","                color=\"blue\",\n","                width=0.2,\n","                scale_units=\"inches\",\n","                units=\"inches\",\n","                scale=1,\n","                headwidth=3,\n","                headlength=3,\n","                headaxislength=3,\n","                minlength=0.1,\n","            )\n","            ax.quiver(\n","                gt_pose_desdf[t + L, 0],\n","                gt_pose_desdf[t + L, 1],\n","                np.cos(gt_pose_desdf[t + L, 2]),\n","                np.sin(gt_pose_desdf[t + L, 2]),\n","                color=\"green\",\n","                width=0.2,\n","                scale_units=\"inches\",\n","                units=\"inches\",\n","                scale=1,\n","                headwidth=3,\n","                headlength=3,\n","                headaxislength=3,\n","                minlength=0.1,\n","            )\n","            ax.axis(\"off\")\n","            ax.set_title(str(t) + \" posterior\")\n","            ax = fig.add_subplot(1, 2, 1)\n","            ax.imshow(likelihood_2d, origin=\"lower\", cmap=\"coolwarm\")\n","            ax.set_title(str(t) + \" likelihood\")\n","            ax.axis(\"off\")\n","            ax.quiver(\n","                likelihood_pred[0],\n","                likelihood_pred[1],\n","                np.cos(likelihood_pred[2]),\n","                np.sin(likelihood_pred[2]),\n","                color=\"blue\",\n","                width=0.2,\n","                scale_units=\"inches\",\n","                units=\"inches\",\n","                scale=1,\n","                headwidth=3,\n","                headlength=3,\n","                headaxislength=3,\n","                minlength=0.1,\n","            )\n","            ax.quiver(\n","                gt_pose_desdf[t + L, 0],\n","                gt_pose_desdf[t + L, 1],\n","                np.cos(gt_pose_desdf[t + L, 2]),\n","                np.sin(gt_pose_desdf[t + L, 2]),\n","                color=\"green\",\n","                width=0.2,\n","                scale_units=\"inches\",\n","                units=\"inches\",\n","                scale=1,\n","                headwidth=3,\n","                headlength=3,\n","                headaxislength=3,\n","                minlength=0.1,\n","            )\n","            if not os.path.exists(\n","                os.path.join(evol_path, \"pretty_filter\", str(data_idx))\n","            ):\n","                os.makedirs(os.path.join(evol_path, \"pretty_filter\", str(data_idx)))\n","            fig.savefig(\n","                os.path.join(\n","                    evol_path, \"pretty_filter\", str(data_idx), str(t) + \".png\"\n","                )\n","            )\n","\n","        #print(\"Before transition\")\n","        # transition\n","        # use ground truth to compute transitions, use relative poses\n","        if t + L == traj_l - 1:\n","            continue\n","        if ((dataset == \"hge_customized_cropped\") or (dataset ==\"hge_customized_complete\")):\n","            current_pose_desdf = torch.from_numpy(gt_pose_desdf[t + L, :])\n","            next_pose_desdf = torch.from_numpy(gt_pose_desdf[t + L + 1, :])\n","            #current_pose_desdf = current_pose_desdf[[1, 0, 2]]\n","            #next_pose_desdf = next_pose_desdf[[1, 0, 2]]\n","            #current_pose = current_pose_desdf\n","            #next_pose = next_pose_desdf\n","            #current_pose = current_pose_desdf*desdf_resolution\n","            #next_pose = next_pose_desdf*desdf_resolution\n","            current_pose = torch.tensor([current_pose_desdf[0] * desdf_resolution, current_pose_desdf[1] * desdf_resolution, current_pose_desdf[2]], device=current_pose_desdf.device)\n","            next_pose = torch.tensor([next_pose_desdf[0] * desdf_resolution, next_pose_desdf[1] * desdf_resolution, next_pose_desdf[2]], device=next_pose_desdf.device)\n","            transition = get_rel_pose(current_pose, next_pose)\n","#            prior = transit(\n","#                posterior, transition, sig_o=0.1, sig_x=0.1, sig_y=0.1, tsize=7, rsize=7, resolution=1\n","#            )\n","#            prior = transit(\n","#                posterior, transition, sig_o=0.1, sig_x=0.1, sig_y=0.1, tsize=7*200+1, rsize=7*10+1, resolution=desdf_resolution\n","#            )\n","            prior = transit(\n","                posterior, transition, sig_o=0.1, sig_x=0.1, sig_y=0.1, tsize=7*10+1, rsize=7*10+1, resolution=desdf_resolution\n","            )\n","\n","            # ### State 0\n","            # prior = transit(\n","            #     posterior, transition, sig_o=0.1*10, sig_x=0.1*10, sig_y=0.1*10, tsize=7*10+1, rsize=7*10+1, resolution=desdf_resolution\n","            # )\n","        else:\n","            current_pose = poses[0, t + L, :]\n","            next_pose = poses[0, t + L + 1, :]\n","            transition = get_rel_pose(current_pose, next_pose)\n","            prior = transit(\n","                posterior, transition, sig_o=0.1, sig_x=0.1, sig_y=0.1, tsize=7, rsize=7\n","            )\n","        bf_current_pose_ls.append(current_pose.cpu().numpy())\n","        bf_transition_ls.append(transition.cpu().numpy())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["        print(\"transition: \", transition)\n","    \n","        end_iter = time.time()\n","        matching_time += matching_end - matching_start\n","        feature_extraction_time += feature_extraction_end - feature_extraction_start\n","        iteration_time += end_iter - start_iter\n","        n_iter += 1\n","\n","    # Evaluate quality of localization per observation\n","    if (evol_path is not None) and (log_extended==True):\n","        acc_record = np.array(metric_observation_position_err_ls)\n","        acc_orn_record = np.array(metric_observation_orientation_err_ls)\n","        \n","        # Calculate recalls\n","        recall_10m = np.sum(acc_record < 10) / acc_record.shape[0]\n","        recall_5m = np.sum(acc_record < 5) / acc_record.shape[0]\n","        recall_2m = np.sum(acc_record < 2) / acc_record.shape[0]\n","        recall_1m = np.sum(acc_record < 1) / acc_record.shape[0]\n","        recall_0_5m = np.sum(acc_record < 0.5) / acc_record.shape[0]\n","        recall_0_1m = np.sum(acc_record < 0.1) / acc_record.shape[0]\n","        recall_1m_30deg = np.sum(np.logical_and(acc_record < 1, acc_orn_record < 30)) / acc_record.shape[0]\n","\n","        # Print recalls\n","        print(\"10m recall = \", recall_10m)\n","        print(\"5m recall = \", recall_5m)\n","        print(\"2m recall = \", recall_2m)\n","        print(\"1m recall = \", recall_1m)\n","        print(\"0.5m recall = \", recall_0_5m)\n","        print(\"0.1m recall = \", recall_0_1m)\n","        print(\"1m 30 deg recall = \", recall_1m_30deg)\n","\n","        # Plot bar plot of recalls\n","        recalls = [recall_0_1m, recall_0_5m, recall_1m, recall_1m_30deg, recall_2m, recall_5m, recall_10m]\n","        labels = ['0.1m', '0.5m', '1m', '1m 30°', '2m', '5m', '10m']\n","        fig = plt.figure(3, figsize=(6, 6))\n","        ax = fig.add_subplot(1, 1, 1)\n","        ax.bar(labels, recalls)\n","        #ax.set_xlabel('Recall Type', fontsize=14)\n","        ax.set_ylabel('Recall', fontsize=14)\n","        ax.tick_params(axis='x', labelsize=14)\n","        ax.tick_params(axis='y', labelsize=14)\n","        ax.set_ylim([0, 1])\n","        ax.grid(True)\n","        plt.show()\n","        fig.savefig(os.path.join(evol_path, \"pretty_filter_extended\", str(data_idx), \"recalls\" + \".png\"))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["    if (evol_path is not None) and (log_extended==True):\n","        \n","        pred_poses_map = np.stack(pred_poses_map)\n","\n","        ## Error\n","        error = (\n","            ((pred_poses_map[-(traj_l - L):, :2] - poses_map[-(traj_l - L):, :2]) ** 2).sum(axis=1)\n","            ** 0.5\n","        ) * original_resolution"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["        ## Bounding Box Diagonal\n","        # Calculate bounding box diagonal for each step\n","        bounding_box_diagonals = []\n","        for i in range(1, len(poses_map[-(traj_l - L):, :2]) + 1):\n","            current_positions = poses_map[-(traj_l - L):, :2][:i]\n","            diagonal_length = minimum_bounding_box(current_positions) * original_resolution\n","            bounding_box_diagonals.append(diagonal_length)\n","        bounding_box_diagonal = np.array(bounding_box_diagonals)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["        ## Distance Travelled\n","        # Calculate the differences between each consecutive pair of positions\n","        differences = np.diff(poses_map[-(traj_l - L):, :2], axis=0)\n","        # Calculate the Euclidean distance for each pair\n","        distances = np.sqrt((differences ** 2).sum(axis=1))* original_resolution\n","        # Calculate the accumulated distance for each timestamp\n","        distance_travelled = np.concatenate(([0], np.cumsum(distances)))\n","\n","        ## Plot Error vs. Distance Travelled\n","        fig = plt.figure(4, figsize=(6, 9))\n","        fig.clf()\n","        ax = fig.add_subplot(3, 1, 1)\n","        ax.plot(error)\n","        ax.grid()\n","        ax.set_xlabel(\"Step [-]\", fontsize=14)\n","        ax.set_ylabel(\"Error [m]\", fontsize=14)\n","        ax.tick_params(axis='x', labelsize=14)\n","        ax.tick_params(axis='y', labelsize=14) \n","        if ((dataset == \"hge_customized_cropped\") or (dataset ==\"hge_customized_complete\")):\n","            ax.set_ylim([0,50])  \n","        else:\n","            ax.set_ylim([0,5])\n","        ax = fig.add_subplot(3, 1, 2)\n","        ax.plot(distance_travelled, error)\n","        ax.grid()\n","        ax.set_xlabel(\"Distance Travelled [m]\", fontsize=14)\n","        ax.set_ylabel(\"Error [m]\", fontsize=14)\n","        ax.tick_params(axis='x', labelsize=14)\n","        ax.tick_params(axis='y', labelsize=14)\n","        if ((dataset == \"hge_customized_cropped\") or (dataset ==\"hge_customized_complete\")):\n","            ax.set_ylim([0,50])  \n","        else:\n","            ax.set_ylim([0,5])\n","        ax = fig.add_subplot(3, 1, 3)\n","        ax.plot(bounding_box_diagonal, error)\n","        ax.grid()\n","        ax.set_xlabel(\"Bounding Box Diagonal [m]\", fontsize=14)\n","        ax.set_ylabel(\"Error [m]\", fontsize=14)\n","        ax.tick_params(axis='x', labelsize=14)\n","        ax.tick_params(axis='y', labelsize=14)\n","        if ((dataset == \"hge_customized_cropped\") or (dataset ==\"hge_customized_complete\")):\n","            ax.set_ylim([0,50])\n","        else:\n","            ax.set_ylim([0,5])\n","        fig.tight_layout()\n","        if not os.path.exists(os.path.join(evol_path, \"pretty_filter_extended\", str(data_idx))):\n","            os.makedirs(os.path.join(evol_path, \"pretty_filter_extended\", str(data_idx)))\n","        fig.savefig(os.path.join(evol_path, \"pretty_filter_extended\", str(data_idx), \"error_evolution\" + \".png\"))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["    if (evol_path is not None) and (log_extended==True):    \n","            fig = plt.figure(5, figsize=(2*6, 4*3))\n","            fig.clf()\n","            ax = fig.add_subplot(4, 2, 1)\n","            ax.plot(metric_depth_l1_loss_ls)\n","            ax.grid()\n","            ax.set_xlabel(\"Step [-]\", fontsize=14)\n","            ax.set_ylabel(\"Error [m]\", fontsize=14)\n","            ax.tick_params(axis='x', labelsize=14)\n","            ax.tick_params(axis='y', labelsize=14) \n","            if ((dataset == \"hge_customized_cropped\") or (dataset ==\"hge_customized_complete\")):\n","                ax.set_ylim([0,50])\n","            else:\n","                ax.set_ylim([0,5])\n","            ax.set_title(\"Depth Mean Absolute Error\", fontsize=14)\n","            ax = fig.add_subplot(4, 2, 2)\n","            ax.plot(metric_depth_shape_loss_ls)\n","            ax.grid()\n","            ax.set_xlabel(\"Step [-]\", fontsize=14)\n","            ax.set_ylabel(\"Similarity [-]\", fontsize=14)\n","            ax.tick_params(axis='x', labelsize=14)\n","            ax.tick_params(axis='y', labelsize=14)\n","            ax.set_ylim([-1.05, 1.05])\n","            ax.set_title(\"Depth Cosine Similarity\", fontsize=14)\n","            ax = fig.add_subplot(4, 2, 3)\n","            ax.plot(metric_ray_l1_loss_ls)\n","            ax.grid()\n","            ax.set_xlabel(\"Step [-]\", fontsize=14)\n","            ax.set_ylabel(\"Error [m]\", fontsize=14)\n","            ax.tick_params(axis='x', labelsize=14)\n","            ax.tick_params(axis='y', labelsize=14)\n","            if ((dataset == \"hge_customized_cropped\") or (dataset ==\"hge_customized_complete\")):\n","                ax.set_ylim([0,50])\n","            else:\n","                ax.set_ylim([0,5])\n","            ax.set_title(\"Ray Length Mean Abs. Error\", fontsize=14)\n","\n","            # Create an empty and invisible subplot at position (4, 2, 4)\n","            ax = fig.add_subplot(4, 2, 4)\n","            ax.axis('off')  # Turn off the axis for this subplot\n","            ax = fig.add_subplot(4, 2, 5)\n","            ax.plot(metric_observation_position_err_ls)\n","            ax.grid()\n","            ax.set_xlabel(\"Step [-]\", fontsize=14)\n","            ax.set_ylabel(\"Error [m]\", fontsize=14)\n","            ax.tick_params(axis='x', labelsize=14)\n","            ax.tick_params(axis='y', labelsize=14)\n","            if ((dataset == \"hge_customized_cropped\") or (dataset ==\"hge_customized_complete\")):\n","                ax.set_ylim([0,50])\n","            else:\n","                ax.set_ylim([0,5])\n","            ax.set_title(\"Observation Position Abs. Error\", fontsize=14)\n","            ax = fig.add_subplot(4, 2, 6)\n","            ax.plot(metric_observation_orientation_err_ls)\n","            ax.grid()\n","            ax.set_xlabel(\"Step [-]\", fontsize=14)\n","            ax.set_ylabel(\"Error [°]\", fontsize=14)\n","            ax.tick_params(axis='x', labelsize=14)\n","            ax.tick_params(axis='y', labelsize=14)\n","            ax.set_ylim([0, 185])\n","            ax.set_title(\"Observation Orientation Abs. Error\", fontsize=14)\n","            ax = fig.add_subplot(4, 2, 7)\n","            ax.plot(metric_posterior_position_err_ls)\n","            ax.grid()\n","            ax.set_xlabel(\"Step [-]\", fontsize=14)\n","            ax.set_ylabel(\"Error [m]\", fontsize=14)\n","            ax.tick_params(axis='x', labelsize=14)\n","            ax.tick_params(axis='y', labelsize=14)\n","            if ((dataset == \"hge_customized_cropped\") or (dataset ==\"hge_customized_complete\")):\n","                ax.set_ylim([0,50])\n","            else:\n","                ax.set_ylim([0,5])\n","            ax.set_title(\"Posterior Position Abs. Error\", fontsize=14)\n","            ax = fig.add_subplot(4, 2, 8)\n","            ax.plot(metric_posterior_orientation_err_ls)\n","            ax.grid()\n","            ax.set_xlabel(\"Step [-]\", fontsize=14)\n","            ax.set_ylabel(\"Error [°]\", fontsize=14)\n","            ax.tick_params(axis='x', labelsize=14)\n","            ax.tick_params(axis='y', labelsize=14)\n","            ax.set_ylim([0, 185])\n","            ax.set_title(\"Posterior Orientation Abs. Error\", fontsize=14)\n","            fig.tight_layout()\n","            if not os.path.exists(os.path.join(evol_path, \"pretty_filter_extended\", str(data_idx))):\n","                os.makedirs(os.path.join(evol_path, \"pretty_filter_extended\", str(data_idx)))\n","            fig.savefig(os.path.join(evol_path, \"pretty_filter_extended\", str(data_idx), \"metrics\" + \".png\"))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["    if (evol_path is not None) and (log_extended==True):    \n","            fig = plt.figure(6, figsize=(1*6, 4*3))\n","            fig.clf()\n","            ax = fig.add_subplot(6, 1, 1)\n","            ax.plot([arr[0] for arr in bf_current_pose_ls])\n","            ax.grid()\n","            ax.set_xlabel(\"Step [-]\", fontsize=14)\n","            ax.set_ylabel(\"Position [m]\", fontsize=14)\n","            ax.tick_params(axis='x', labelsize=14)\n","            ax.tick_params(axis='y', labelsize=14) \n","            #if dataset == \"hge_customized_cropped\":\n","            #    ax.set_ylim([0,500])\n","            #else:\n","            #    ax.set_ylim([0,500])\n","            ax.set_title(\"Current Pose: 0\", fontsize=14)\n","            ax = fig.add_subplot(6, 1, 2)\n","            ax.plot([arr[1] for arr in bf_current_pose_ls])\n","            ax.grid()\n","            ax.set_xlabel(\"Step [-]\", fontsize=14)\n","            ax.set_ylabel(\"Position [m]\", fontsize=14)\n","            ax.tick_params(axis='x', labelsize=14)\n","            ax.tick_params(axis='y', labelsize=14)\n","            ax.set_title(\"Current Pose: 1\", fontsize=14)\n","            ax = fig.add_subplot(6, 1, 3)\n","            ax.plot([arr[2] for arr in bf_current_pose_ls])\n","            ax.grid()\n","            ax.set_xlabel(\"Step [-]\", fontsize=14)\n","            ax.set_ylabel(\"Orientation [rad]\", fontsize=14)\n","            ax.tick_params(axis='x', labelsize=14)\n","            ax.tick_params(axis='y', labelsize=14)\n","            #if dataset == \"hge_customized_cropped\":\n","            #    ax.set_ylim([-2*np.pi,500])\n","            #else:\n","            #    ax.set_ylim([0,500])\n","            ax.set_title(\"Current Pose: 2\", fontsize=14)\n","            ax = fig.add_subplot(6, 1, 4)\n","            ax.plot([arr[0] for arr in bf_transition_ls])\n","            ax.grid()\n","            ax.set_xlabel(\"Step [-]\", fontsize=14)\n","            ax.set_ylabel(\"Position [m]\", fontsize=14)\n","            ax.tick_params(axis='x', labelsize=14)\n","            ax.tick_params(axis='y', labelsize=14)\n","            #if dataset == \"hge_customized_cropped\":\n","            #    ax.set_ylim([0,500])\n","            #else:\n","            #    ax.set_ylim([0,500])\n","            ax.set_title(\"Transition: 0\", fontsize=14)\n","            ax = fig.add_subplot(6, 1, 5)\n","            ax.plot([arr[1] for arr in bf_transition_ls])\n","            ax.grid()\n","            ax.set_xlabel(\"Step [-]\", fontsize=14)\n","            ax.set_ylabel(\"Position [m]\", fontsize=14)\n","            ax.tick_params(axis='x', labelsize=14)\n","            ax.tick_params(axis='y', labelsize=14)\n","            #if dataset == \"hge_customized_cropped\":\n","            #    ax.set_ylim([0,500])\n","            #else:\n","            #    ax.set_ylim([0,500])\n","            ax.set_title(\"Transition: 1\", fontsize=14)\n","            ax = fig.add_subplot(6, 1, 6)\n","            ax.plot([arr[2] for arr in bf_transition_ls])\n","            ax.grid()\n","            ax.set_xlabel(\"Step [-]\", fontsize=14)\n","            ax.set_ylabel(\"Orientation [rad]\", fontsize=14)\n","            ax.tick_params(axis='x', labelsize=14)\n","            ax.tick_params(axis='y', labelsize=14)\n","            #if dataset == \"hge_customized_cropped\":\n","            #    ax.set_ylim([0,500])\n","            #else:\n","            #    ax.set_ylim([0,500])\n","            ax.set_title(\"Transition: 2\", fontsize=14)\n","            fig.tight_layout()\n","            if not os.path.exists(os.path.join(evol_path, \"pretty_filter_extended\", str(data_idx))):\n","                os.makedirs(os.path.join(evol_path, \"pretty_filter_extended\", str(data_idx)))\n","            fig.savefig(os.path.join(evol_path, \"pretty_filter_extended\", str(data_idx), \"bugfixing\" + \".png\"))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["    if log_error:\n","        pred_poses_map = np.stack(pred_poses_map)\n","        # record success rate, from map to global\n","        last_errors = (\n","            ((pred_poses_map[-10:, :2] - poses_map[-10:, :2]) ** 2).sum(axis=1)\n","            ** 0.5\n","        ) * original_resolution\n","        # compute RMSE\n","        RMSE = (\n","            ((pred_poses_map[-10:, :2] - poses_map[-10:, :2]) ** 2)\n","            .sum(axis=1)\n","            .mean()\n","        ) ** 0.5 * original_resolution\n","        RMSEs.append(RMSE)\n","        print(\"last_errors\", last_errors)\n","        if all(last_errors < 1):\n","            success_10.append(True)\n","        else:\n","            success_10.append(False)\n","        if all(last_errors < 0.5):\n","            success_5.append(True)\n","        else:\n","            success_5.append(False)\n","        if all(last_errors < 0.3):\n","            success_3.append(True)\n","        else:\n","            success_3.append(False)\n","        if all(last_errors < 0.2):\n","            success_2.append(True)\n","        else:\n","            success_2.append(False)"]},{"cell_type":"markdown","metadata":{},"source":["In[ ]:"]},{"cell_type":"markdown","metadata":{},"source":["if log_error:<br>\n","    RMSEs = np.array(RMSEs)<br>\n","    success_10 = np.array(success_10)<br>\n","    success_5 = np.array(success_5)<br>\n","    success_3 = np.array(success_3)<br>\n","    success_2 = np.array(success_2)<br>\n","<br>\n","    print(\"============================================\")<br>\n","    print(\"1.0 success rate : \", success_10.sum() / len(test_set))<br>\n","    print(\"0.5 success rate : \", success_5.sum() / len(test_set))<br>\n","    print(\"0.3 success rate : \", success_3.sum() / len(test_set))<br>\n","    print(\"0.2 success rate : \", success_2.sum() / len(test_set))<br>\n","    print(\"mean RMSE succeeded : \", RMSEs[success_10].mean())<br>\n","    print(\"mean RMSE all : \", RMSEs.mean())<br>\n","<br>\n","<br>\n","# In[ ]:<br>\n","<br>\n","<br>\n","if log_timing:<br>\n","    feature_extraction_time = feature_extraction_time / n_iter<br>\n","    matching_time = matching_time / n_iter<br>\n","    iteration_time = iteration_time / n_iter<br>\n","<br>\n","    print(\"============================================\")<br>\n","    print(\"feature_extraction_time : \", feature_extraction_time)<br>\n","    print(\"matching_time : \", matching_time)<br>\n","    print(\"iteration_time : \", iteration_time)"]},{"cell_type":"markdown","metadata":{},"source":["In[ ]:"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":2}
