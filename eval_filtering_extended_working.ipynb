{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d12881",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bef8ca8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/home/wueestm/anaconda3/envs/f3loc/lib/python3.8/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import tqdm\n",
    "import yaml\n",
    "from attrdict import AttrDict\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.image as mpimg \n",
    "\n",
    "from modules.comp.comp_d_net_pl import *\n",
    "from modules.mono.depth_net_pl import *\n",
    "from modules.mv.mv_depth_net_pl import *\n",
    "from utils.data_utils import *\n",
    "from utils.localization_utils import *\n",
    "#from utils.data_utils import TrajDataset_hge_customized_cropped\n",
    "\n",
    "from src.helper_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcafc8ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m desdf_HGE_complete \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdesdf_complete_orn_slice_36_resolution_01.npy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m desdf_HGE_complete\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/anaconda3/envs/f3loc/lib/python3.8/site-packages/numpy/lib/npyio.py:432\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mopen_memmap(file, mode\u001b[38;5;241m=\u001b[39mmmap_mode,\n\u001b[1;32m    430\u001b[0m                                   max_header_size\u001b[38;5;241m=\u001b[39mmax_header_size)\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 432\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_pickle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mmax_header_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_header_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;66;03m# Try a pickle\u001b[39;00m\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_pickle:\n",
      "File \u001b[0;32m~/anaconda3/envs/f3loc/lib/python3.8/site-packages/numpy/lib/format.py:801\u001b[0m, in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[0m\n\u001b[1;32m    798\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m isfileobj(fp):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;66;03m# We can use the fast fromfile() function.\u001b[39;00m\n\u001b[0;32m--> 801\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    802\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    803\u001b[0m         \u001b[38;5;66;03m# This is not a real file. We have to read it the\u001b[39;00m\n\u001b[1;32m    804\u001b[0m         \u001b[38;5;66;03m# memory-intensive way.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# not correctly instantiate zero-width string dtypes; see\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# https://github.com/numpy/numpy/pull/6430\u001b[39;00m\n\u001b[1;32m    814\u001b[0m         array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39mndarray(count, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "desdf_HGE_complete = np.load(\"desdf_complete_orn_slice_36_resolution_01.npy\")\n",
    "desdf_HGE_complete.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eaf0a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "desdf_HGE_complete.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b475ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "desdf_HGE_complete.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53bfd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "desdf_try = np.load(\"desdf_cropped_orn_slice_144_resolution_attempt_2.npy\")\n",
    "desdf_try.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cfe5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "desdf_try.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0279764",
   "metadata": {},
   "outputs": [],
   "source": [
    "desdf_try.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b76a883",
   "metadata": {},
   "outputs": [],
   "source": [
    "desdf = np.load(\"desdf_cropped_orn_slice_144_resolution_1.npy\")\n",
    "desdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f7dd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "desdf.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c59cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "desdf.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cffa37b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8994408e",
   "metadata": {},
   "outputs": [],
   "source": [
    "desdf_try = np.load(\"desdf_cropped_orn_slice_360_resolution_1.npy\")\n",
    "desdf_try.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d2d564",
   "metadata": {},
   "outputs": [],
   "source": [
    "desdf_try = np.load(\"desdf_cropped.npy\")\n",
    "desdf_try.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddce81f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd603430",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "net_type = \"d\"\n",
    "#dataset = \"gibson_t\" # \"gibson_g\" # \n",
    "#dataset = \"hge_customized_cropped\"\n",
    "dataset = \"hge_customized_complete\"\n",
    "\n",
    "if dataset == \"gibson_t\":\n",
    "    dataset_path = \"/cluster/project/cvg/data/gibson/Gibson_Floorplan_Localization_Dataset\"\n",
    "    evol_path = \"./evol_path/gibson_f/gt\" #evol_path = \"./evol_path/gibson_f/mono\"\n",
    "    desdf_resolution = 0.1\n",
    "    orn_slice = 36\n",
    "elif ((dataset == \"hge_customized_cropped\") or (dataset ==\"hge_customized_complete\")):\n",
    "    #dataset_path = \"/cluster/project/cvg/data/lamar/HGE_customized_cropped\"\n",
    "    dataset_path = \"/cluster/project/cvg/data/lamar/HGE_customized_complete\"\n",
    "    evol_path = \"./evol_path/hge_customized_cropped/gt\"\n",
    "    desdf_resolution = 0.1\n",
    "    orn_slice = 36\n",
    "\n",
    "ckpt_path = \"./logs\"\n",
    "traj_len = 20#8#100#100#50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cceb3dc9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= USING DEVICE :  cpu  =======\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# get device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"======= USING DEVICE : \", device, \" =======\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9708446",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# paths\n",
    "dataset_dir = os.path.join(dataset_path, dataset)\n",
    "depth_dir = dataset_dir#args.dataset_path\n",
    "log_dir = ckpt_path\n",
    "desdf_path = os.path.join(dataset_path, \"desdf\")\n",
    "evol_path = evol_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e2acfd8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# depth file suffix\n",
    "if (net_type == \"d\") & ((dataset == \"hge_customized_cropped\") or (dataset ==\"hge_customized_complete\")):\n",
    "    depth_suffix = \"depth90\"\n",
    "elif net_type == \"d\":\n",
    "    depth_suffix = \"depth40\"\n",
    "else:\n",
    "    depth_suffix = \"depth160\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ab6b9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate dataset\n",
    "traj_l = traj_len\n",
    "split_file = os.path.join(dataset_dir, \"split.yaml\")\n",
    "with open(split_file, \"r\") as f:\n",
    "    split = AttrDict(yaml.safe_load(f))\n",
    "\n",
    "if ((dataset == \"hge_customized_cropped\") or (dataset ==\"hge_customized_complete\")):\n",
    "    test_set = TrajDataset_hge_customized_cropped(\n",
    "        dataset_dir,\n",
    "        split.test,\n",
    "        L=traj_l,\n",
    "        depth_dir=depth_dir,\n",
    "        depth_suffix=depth_suffix,\n",
    "        add_rp=False,\n",
    "        roll=0,\n",
    "        pitch=0,\n",
    "        without_depth=False, #without_depth=True,  \n",
    "    )\n",
    "\n",
    "else:\n",
    "    test_set = TrajDataset(\n",
    "        dataset_dir,\n",
    "        split.test,\n",
    "        L=traj_l,\n",
    "        depth_dir=depth_dir,\n",
    "        depth_suffix=depth_suffix,\n",
    "        add_rp=False,\n",
    "        roll=0,\n",
    "        pitch=0,\n",
    "        without_depth=False, #without_depth=True,  \n",
    "    )  \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b92edc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab6400b8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# logs\n",
    "log_error = True\n",
    "log_timing = True\n",
    "log_extended = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2c48c93",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# parameters\n",
    "L = 3  # number of the source frames\n",
    "D = 128  # number of the depth planes\n",
    "d_min = 0.1  # minimum depth\n",
    "d_max = 15.0  # maximum depth\n",
    "d_hyp = -0.2  # depth transform (uniform sampling in d**d_hyp)\n",
    "F_W = 3 / 8  # camera intrinsic, focal length / image width\n",
    "trans_thresh = 0.005  # translation threshold (variance) if using comp_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fea6164",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# models\n",
    "if net_type == \"mvd\" or net_type == \"comp_s\":\n",
    "    # instaciate model\n",
    "    mv_net = mv_depth_net_pl.load_from_checkpoint(\n",
    "        checkpoint_path=os.path.join(log_dir, \"mv.ckpt\"),\n",
    "        D=D,\n",
    "        d_min=d_min,\n",
    "        d_max=d_max,\n",
    "        d_hyp=d_hyp,\n",
    "    ).to(device)\n",
    "if net_type == \"d\" or net_type == \"comp_s\":\n",
    "    # instaciate model\n",
    "    d_net = depth_net_pl.load_from_checkpoint(\n",
    "        checkpoint_path=os.path.join(log_dir, \"mono.ckpt\"),\n",
    "        d_min=d_min,\n",
    "        d_max=d_max,\n",
    "        d_hyp=d_hyp,\n",
    "        D=D,\n",
    "    ).to(device)\n",
    "if net_type == \"comp\":\n",
    "    mv_net_pl = mv_depth_net_pl(D=D, d_hyp=d_hyp, F_W=F_W)\n",
    "    mono_net_pl = depth_net_pl(d_min=d_min, d_max=d_max, d_hyp=d_hyp, D=D, F_W=F_W)\n",
    "    comp_net = comp_d_net_pl.load_from_checkpoint(\n",
    "        checkpoint_path=os.path.join(log_dir, \"comp.ckpt\"),\n",
    "        mv_net=mv_net_pl.net,\n",
    "        mono_net=mono_net_pl.encoder,\n",
    "        L=L,\n",
    "        d_min=d_min,\n",
    "        d_max=d_max,\n",
    "        d_hyp=d_hyp,\n",
    "        D=D,\n",
    "        F_W=F_W,\n",
    "        use_pred=True,\n",
    "    ).to(device)\n",
    "    comp_net.eval()  # this is needed to disable batchnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78133e3a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# get desdf for the scene\n",
    "print(\"load desdf ...\")\n",
    "desdfs = {}\n",
    "for scene in tqdm.tqdm(test_set.scene_names):\n",
    "    desdfs[scene] = np.load(\n",
    "        os.path.join(desdf_path, scene, \"desdf.npy\"), allow_pickle=True\n",
    "    ).item()\n",
    "#    desdfs[scene][\"desdf\"][desdfs[scene][\"desdf\"] > 10] = 10  # truncate\n",
    "#print(\"desdf shape: \", desdfs[scene][\"desdf\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6850b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.scene_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59a785d",
   "metadata": {},
   "outputs": [],
   "source": [
    "desdfs[\"ios_2022-07-01_15.58.10_000\"][\"desdf\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8926fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.scene_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bc1f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "desdfs[\"Stilwell\"][\"desdf\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325def8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "desdfs[\"Stilwell\"][\"desdf\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740c776e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset == \"hge_customized_cropped\":\n",
    "\n",
    "    # Correspondences for calibration\n",
    "    floorplan_correspondences = np.array([[1516, 490.5], [1532, 2162], [350, 1515], [391, 135]])\n",
    "    trajectory_correspondences = np.array([[-12.600, 10.103], [77.981, 9.882], [42.641, -54.348], [-33.099, -51.229]])\n",
    "\n",
    "    # Test the transformation on the source points\n",
    "    pts_src = trajectory_correspondences\n",
    "    pts_dst = floorplan_correspondences\n",
    "    affine_matrix = find_affine_transform(pts_src, pts_dst)\n",
    "    transformed_pts = np.array([apply_affine_transformation(pt, affine_matrix) for pt in pts_src])\n",
    "    pixel_per_meter = (affine_matrix[0,1] + affine_matrix[1,0]) / 2\n",
    "\n",
    "    print(\"Affine transformation matrix:\\n\", affine_matrix)\n",
    "    print(\"Destination points:\\n\", pts_dst)\n",
    "    print(\"Transformed points:\\n\", transformed_pts)\n",
    "    print(\"pixel_per_meter:\", pixel_per_meter)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70b5ec0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# get the ground truth pose file\n",
    "print(\"load poses and maps ...\")\n",
    "maps = {}\n",
    "gt_poses = {}\n",
    "for scene in tqdm.tqdm(test_set.scene_names):\n",
    "    # load map\n",
    "    occ = cv2.imread(os.path.join(dataset_dir, scene, \"map.png\"))[:, :, 0]\n",
    "    maps[scene] = occ\n",
    "    h = occ.shape[0]\n",
    "    w = occ.shape[1]\n",
    "\n",
    "    # single trajectory\n",
    "    poses = np.zeros([0, 3], dtype=np.float32)\n",
    "    # get poses\n",
    "    poses_file = os.path.join(dataset_dir, scene, \"poses.txt\")\n",
    "\n",
    "    # read poses\n",
    "    with open(poses_file, \"r\") as f:\n",
    "        poses_txt = [line.strip() for line in f.readlines()]\n",
    "\n",
    "    traj_len = len(poses_txt)\n",
    "    traj_len -= traj_len % traj_l\n",
    "    for state_id in range(traj_len):\n",
    "\n",
    "        if dataset == \"hge_customized_cropped\":\n",
    "            # get pose\n",
    "            pose_ls = poses_txt[state_id].split(\" \")\n",
    "            pose = np.array(pose_ls, dtype=np.float32)\n",
    "            # world to map \n",
    "            (x,y), th = world_to_map(position_world=pose[:2], \n",
    "                                        orientation_world_rad=pose[2], \n",
    "                                        affine_matrix=affine_matrix, \n",
    "                                        floorplan_correspondences=floorplan_correspondences)\n",
    "        else:\n",
    "            # get pose\n",
    "            pose = poses_txt[state_id].split(\" \")\n",
    "            x = float(pose[0])\n",
    "            y = float(pose[1])\n",
    "            th = float(pose[2])\n",
    "            # from world coordinate to map coordinate\n",
    "            x = x / 0.01 + w / 2\n",
    "            y = y / 0.01 + h / 2\n",
    "\n",
    "        poses = np.concatenate(\n",
    "            (poses, np.expand_dims(np.array((x, y, th), dtype=np.float32), 0)),\n",
    "            axis=0,\n",
    "        )\n",
    "\n",
    "    gt_poses[scene] = poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034a28eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d803e131",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# original_resolution = 0.01\n",
    "# desdf_resolution = 0.1\n",
    "# resolution_ratio = desdf_resolution / original_resolution\n",
    "# resolution_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fa3c8b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# original_resolution = 1/pixel_per_meter\n",
    "# desdf_resolution = 1\n",
    "# resolution_ratio = desdf_resolution / original_resolution\n",
    "# resolution_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4a21df",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# record stats\n",
    "RMSEs = []\n",
    "success_10 = []  # Success @ 1m\n",
    "success_5 = []  # Success @ 0.5m\n",
    "success_3 = []  # Success @ 0.3m\n",
    "success_2 = []  # Success @ 0.2m\n",
    "\n",
    "matching_time = 0\n",
    "iteration_time = 0\n",
    "feature_extraction_time = 0\n",
    "n_iter = 0\n",
    "\n",
    "print(\"Length Test Set: \", len(test_set))\n",
    "\n",
    "# loop the over scenes\n",
    "print(\"length of test_set: \", len(test_set))\n",
    "#for data_idx in tqdm.tqdm(range(36, len(test_set))):\n",
    "#for data_idx in tqdm.tqdm(range(len(test_set))):\n",
    "for data_idx in tqdm.tqdm(range(1)):\n",
    "\n",
    "    print(\"Before loading data\")\n",
    "    data = test_set[data_idx]\n",
    "    print(\"After loading data\")\n",
    "    # get the scene name according to the data_idx\n",
    "    scene_idx = np.sum(data_idx >= np.array(test_set.scene_start_idx)) - 1\n",
    "    scene = test_set.scene_names[scene_idx]\n",
    "\n",
    "    # get idx within scene\n",
    "    idx_within_scene = data_idx - test_set.scene_start_idx[scene_idx]\n",
    "\n",
    "    # get desdf\n",
    "    desdf = desdfs[scene]\n",
    "\n",
    "    # get reference pose in map coordinate and in scene coordinate\n",
    "    poses_map = gt_poses[scene][\n",
    "        idx_within_scene * traj_l : idx_within_scene * traj_l + traj_l, :\n",
    "    ]\n",
    "\n",
    "    # map to desdf \n",
    "    if dataset == \"hge_customized_cropped\":\n",
    "        original_resolution = 1/pixel_per_meter\n",
    "        resolution_ratio = desdf_resolution / original_resolution\n",
    "        gt_pose_desdf = poses_map.copy()\n",
    "        gt_pose_desdf[:, 0] = (gt_pose_desdf[:, 0] - desdf[\"l\"]) / resolution_ratio\n",
    "        gt_pose_desdf[:, 1] = (gt_pose_desdf[:, 1] - desdf[\"t\"]) / resolution_ratio\n",
    "        gt_pose_desdf[:, 2] = gt_pose_desdf[:, 2] \n",
    "    else:\n",
    "        original_resolution = 0.01\n",
    "        gt_pose_desdf = poses_map.copy()\n",
    "        gt_pose_desdf[:, 0] = (gt_pose_desdf[:, 0] - desdf[\"l\"]) / 10\n",
    "        gt_pose_desdf[:, 1] = (gt_pose_desdf[:, 1] - desdf[\"t\"]) / 10\n",
    "\n",
    "    imgs = torch.tensor(data[\"imgs\"], device=device).unsqueeze(0)\n",
    "    poses = torch.tensor(data[\"poses\"], device=device).unsqueeze(0)\n",
    "\n",
    "    # set prior as uniform distribution\n",
    "    prior = torch.tensor(\n",
    "        np.ones_like(desdf[\"desdf\"]) / desdf[\"desdf\"].size, device=imgs.device\n",
    "    ).to(torch.float32)\n",
    "\n",
    "    pred_poses_map = []\n",
    "\n",
    "    # record stats extended: per trajectory\n",
    "    if (evol_path is not None) and (log_extended==True):\n",
    "        metric_depth_l1_loss_ls = []\n",
    "        metric_depth_shape_loss_ls = []\n",
    "        metric_ray_l1_loss_ls = []\n",
    "        metric_observation_position_err_ls = []\n",
    "        metric_observation_orientation_err_ls = []\n",
    "        metric_posterior_position_err_ls = []\n",
    "        metric_posterior_orientation_err_ls = []\n",
    "\n",
    "    # loop over the sequences\n",
    "    for t in range(traj_l - L):\n",
    "        print(\"t = \", t)\n",
    "        start_iter = time.time()\n",
    "        feature_extraction_start = time.time()\n",
    "        # form input\n",
    "        input_dict = {}\n",
    "#        if net_type == \"mvd\" or net_type == \"comp\" or net_type == \"comp_s\":\n",
    "#            input_dict.update(\n",
    "#                {\n",
    "#                    \"ref_img\": imgs[:, t + L, :, :, :],\n",
    "#                    \"src_img\": imgs[:, t : t + L, :, :, :],\n",
    "#                    \"ref_pose\": poses[:, t + L, :],\n",
    "#                    \"src_pose\": poses[:, t : t + L, :],\n",
    "#                    \"ref_mask\": None,  # no masks because the dataset has zero roll pitch\n",
    "#                    \"src_mask\": None,  # no masks because the dataset has zero roll pitch\n",
    "#                }\n",
    "#            )\n",
    "#        if net_type == \"d\" or net_type == \"comp_s\":\n",
    "#            input_dict.update(\n",
    "#                {\n",
    "#                    \"img\": imgs[:, t + L, :, :, :],\n",
    "#                    \"mask\": None,  # no masks because the dataset has zero roll pitch\n",
    "#                }\n",
    "#            )\n",
    "        # check which model to use if hardcoded selection\n",
    "        if net_type == \"comp_s\":\n",
    "            # calculate the relative poses\n",
    "            pose_var = (\n",
    "                torch.cat(\n",
    "                    (input_dict[\"ref_pose\"].unsqueeze(1), input_dict[\"src_pose\"]),\n",
    "                    dim=1,\n",
    "                )\n",
    "                .squeeze(0)\n",
    "                .var(dim=0)[:2]\n",
    "                .sum()\n",
    "            )\n",
    "            if pose_var < trans_thresh:\n",
    "                use_mv = False\n",
    "                use_mono = True\n",
    "            else:\n",
    "                use_mv = True\n",
    "                use_mono = False\n",
    "        print(\"Before inference\")\n",
    "        # inference\n",
    "        if net_type == \"mvd\" or (net_type == \"comp_s\" and use_mv):\n",
    "            pred_dict = mv_net.net(input_dict)\n",
    "            pred_depths = pred_dict[\"d\"]\n",
    "            pred_depths = pred_depths.squeeze(0).detach().cpu().numpy()\n",
    "        elif net_type == \"d\" or (net_type == \"comp_s\" and use_mono):\n",
    "            # ### Trained model:\n",
    "            # pred_depths, attn_2d, prob = d_net.encoder(\n",
    "            #     input_dict[\"img\"], input_dict[\"mask\"]\n",
    "            # )\n",
    "            # pred_depths = pred_depths.squeeze(0).detach().cpu().numpy()\n",
    "            # ###\n",
    "\n",
    "            ### GT:\n",
    "            pred_depths = np.array(data[\"gt_depth\"])[t + L,:]\n",
    "            ###\n",
    "\n",
    "            #print(type(pred_depths))\n",
    "            #print(pred_depths.shape)\n",
    "        elif net_type == \"comp\":\n",
    "            pred_dict = comp_net.comp_d_net(input_dict)\n",
    "            pred_depths = pred_dict[\"d_comp\"]\n",
    "            pred_depths = pred_depths.squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "        #pred_depths = pred_depths.squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "        # get rays from depth\n",
    "        if dataset == \"hge_customized_cropped\":\n",
    "            fov_desdf = 49#50\n",
    "            dv = 360/orn_slice\n",
    "            V = fov_desdf / dv\n",
    "            pred_rays = get_ray_from_depth(pred_depths, V=V, dv=dv, F_W=1596/1440)\n",
    "            pred_rays = torch.tensor(pred_rays, device=device)\n",
    "        else:\n",
    "            pred_rays = get_ray_from_depth(pred_depths)\n",
    "            pred_rays = torch.tensor(pred_rays, device=device)\n",
    "\n",
    "        feature_extraction_end = time.time()\n",
    "\n",
    "        matching_start = time.time()\n",
    "        print(\"Before localize\")\n",
    "        # use the prediction to localize, produce observation likelihood\n",
    "        if dataset == \"hge_customized_cropped\":\n",
    "            likelihood, likelihood_2d, likelihood_orn, likelihood_pred = localize_noflip(\n",
    "                torch.tensor(desdf[\"desdf\"]).to(prior.device),\n",
    "                pred_rays.to(prior.device),\n",
    "                return_np=False,\n",
    "                orn_slice=orn_slice,\n",
    "                lambd=400\n",
    "            )\n",
    "        else:\n",
    "            likelihood, likelihood_2d, likelihood_orn, likelihood_pred = localize(\n",
    "                torch.tensor(desdf[\"desdf\"]).to(prior.device),\n",
    "                pred_rays.to(prior.device),\n",
    "                return_np=False,\n",
    "                orn_slice=orn_slice\n",
    "            )\n",
    "            \n",
    "        matching_end = time.time()\n",
    "\n",
    "        # multiply with the prior\n",
    "        posterior = prior * likelihood.to(prior.device)\n",
    "        posterior = posterior / posterior.sum()\n",
    "\n",
    "        # reduce the posterior along orientation for 2d visualization\n",
    "        posterior_2d, orientations = torch.max(posterior, dim=2)\n",
    "\n",
    "        # compute prior_2d for visualization\n",
    "        prior_2d, _ = torch.max(prior, dim=2)\n",
    "\n",
    "        # maximum of the posterior as result\n",
    "        pose_y, pose_x = torch.where(posterior_2d == posterior_2d.max())\n",
    "        if pose_y.shape[0] > 1:\n",
    "            pose_y = pose_y[0].unsqueeze(0)\n",
    "            pose_x = pose_x[0].unsqueeze(0)\n",
    "        orn = orientations[pose_y, pose_x]\n",
    "\n",
    "        # from orientation indices to radians\n",
    "        orn = orn / orn_slice * 2 * torch.pi\n",
    "        pose = torch.cat((pose_x, pose_y, orn)).detach().cpu().numpy()\n",
    "\n",
    "        if dataset == \"hge_customized_cropped\":\n",
    "            pose_in_map = pose.copy()\n",
    "            pose_in_map[0] = pose_in_map[0] * resolution_ratio + desdf[\"l\"]\n",
    "            pose_in_map[1] = pose_in_map[1] * resolution_ratio + desdf[\"t\"]\n",
    "        else:\n",
    "            pose_in_map = pose.copy()\n",
    "            pose_in_map[0] = pose_in_map[0] * 10 + desdf[\"l\"]\n",
    "            pose_in_map[1] = pose_in_map[1] * 10 + desdf[\"t\"]\n",
    "\n",
    "        pred_poses_map.append(pose_in_map)\n",
    "\n",
    "        print(\"Before preparing metrics\")\n",
    "        if (evol_path is not None) & (log_extended==True):\n",
    "\n",
    "            #### Prepare Metrics ------------------------------------------------\n",
    "\n",
    "            ### Quality of depth prediction: Predicted depth vs. GT depth\n",
    "            # Get variables\n",
    "            predicted_depths = pred_depths\n",
    "            gt_depths = np.array(data[\"gt_depth\"])[t + L,:]\n",
    "            # Get metrics\n",
    "            metric_depth_l1_loss = F.l1_loss(torch.tensor(predicted_depths), torch.tensor(gt_depths)).item()\n",
    "            metric_depth_shape_loss = F.cosine_similarity(torch.tensor(predicted_depths), torch.tensor(gt_depths), dim=-1).mean().item()\n",
    "\n",
    "            ### Quality of matching: Matched ray vs. Predicted ray\n",
    "            # Get variables\n",
    "            idx_orn = int(likelihood_orn[likelihood_2d == likelihood_2d.max()][0])\n",
    "            idx_x = int(likelihood_pred[1])\n",
    "            idx_y = int(likelihood_pred[0])\n",
    "            desdf_loc = torch.tensor(desdf[\"desdf\"])\n",
    "            V = pred_rays.shape[0]\n",
    "            pad_front = V // 2\n",
    "            pad_back = V - pad_front\n",
    "            pad_desdf = F.pad(desdf_loc, [pad_front, pad_back], mode=\"circular\")\n",
    "            desdf_rays = pad_desdf[idx_x, idx_y, idx_orn: idx_orn + V]\n",
    "            if dataset == \"hge_customized_cropped\":\n",
    "                matched_rays = desdf_rays.cpu().numpy().copy()\n",
    "            else:\n",
    "                matched_rays = desdf_rays.cpu().numpy()[::-1].copy()  # move to CPU before converting to numpy\n",
    "            predicted_rays = pred_rays.cpu().numpy()  # move pred_rays to CPU before converting to numpy\n",
    "            # Get metrics\n",
    "            metric_ray_l1_loss = F.l1_loss(torch.tensor(matched_rays), torch.tensor(predicted_rays)).item()\n",
    "\n",
    "\n",
    "            ### Quality of localization per observation: Predicted pose vs. GT pose\n",
    "            if dataset == \"hge_customized_cropped\":\n",
    "                # Get variables\n",
    "                observation_pose_in_map = np.empty(3)\n",
    "                # desdf to map\n",
    "                observation_pose_in_map[0] = likelihood_pred[0] * resolution_ratio + desdf[\"l\"]\n",
    "                observation_pose_in_map[1] = likelihood_pred[1] * resolution_ratio + desdf[\"t\"]\n",
    "                observation_pose_in_map[2] = likelihood_pred[2]\n",
    "                # map to world\n",
    "                observation_predicted_position, observation_predicted_orientation_rad = map_to_world(observation_pose_in_map[:2], observation_pose_in_map[2], affine_matrix, floorplan_correspondences)\n",
    "                observation_predicted_orientation = observation_predicted_orientation_rad/np.pi*180\n",
    "                gt_position, gt_orientation_rad = map_to_world(poses_map[t + L,:2], poses_map[t + L,2], affine_matrix, floorplan_correspondences)\n",
    "                gt_orientation = gt_orientation_rad/np.pi*180\n",
    "            else:\n",
    "                # Get variables\n",
    "                observation_pose_in_map = np.empty(3)\n",
    "                # desdf to map\n",
    "                observation_pose_in_map[0] = likelihood_pred[0] * 10 + desdf[\"l\"]\n",
    "                observation_pose_in_map[1] = likelihood_pred[1] * 10 + desdf[\"t\"]\n",
    "                observation_pose_in_map[2] = likelihood_pred[2]\n",
    "                # map to world\n",
    "                observation_predicted_position = observation_pose_in_map[:2]*0.01\n",
    "                observation_predicted_orientation = observation_pose_in_map[2]/np.pi*180\n",
    "                gt_position = poses_map[t + L,:2]*0.01\n",
    "                gt_orientation = poses_map[t + L,2]/np.pi*180\n",
    "\n",
    "            # Get metrics\n",
    "            metric_observation_position_err = np.linalg.norm(observation_predicted_position - gt_position)\n",
    "            metric_observation_orientation_err = observation_predicted_orientation - gt_orientation #min((observation_predicted_orientation - gt_orientation) % 360, 360 - ((observation_predicted_orientation - gt_orientation) % 360))\n",
    "\n",
    "            ### Quality of localization filter: Predicted posterior pose vs. GT pose\n",
    "            if dataset == \"hge_customized_cropped\":\n",
    "                # Get variables\n",
    "                # map to world\n",
    "                posterior_predicted_position, posterior_predicted_orientation_rad = map_to_world(pose_in_map[:2], pose_in_map[2], affine_matrix, floorplan_correspondences)\n",
    "                posterior_predicted_orientation = posterior_predicted_orientation_rad/np.pi*180\n",
    "                gt_position, gt_orientation_rad = map_to_world(poses_map[t + L,:2], poses_map[t + L,2], affine_matrix, floorplan_correspondences)\n",
    "                gt_orientation = gt_orientation_rad/np.pi*180\n",
    "            else:\n",
    "                # Get variables\n",
    "                # map to world\n",
    "                posterior_predicted_position = pose_in_map[:2]*0.01\n",
    "                posterior_predicted_orientation = pose_in_map[2]/np.pi*180\n",
    "                gt_position = poses_map[t + L,:2]*0.01\n",
    "                gt_orientation = poses_map[t + L,2]/np.pi*180\n",
    "\n",
    "            # Get metrics\n",
    "            metric_posterior_position_err = np.linalg.norm(posterior_predicted_position - gt_position)\n",
    "            metric_posterior_orientation_err = posterior_predicted_orientation - gt_orientation #min((posterior_predicted_orientation - gt_orientation) % 360, 360 - (posterior_predicted_orientation - gt_orientation) % 360)\n",
    "\n",
    "\n",
    "            ### Log metrics\n",
    "            metric_depth_l1_loss_ls.append(metric_depth_l1_loss)\n",
    "            metric_depth_shape_loss_ls.append(metric_depth_shape_loss)\n",
    "            metric_ray_l1_loss_ls.append(metric_ray_l1_loss)\n",
    "            metric_observation_position_err_ls.append(metric_observation_position_err)\n",
    "            metric_observation_orientation_err_ls.append(metric_observation_orientation_err)\n",
    "            metric_posterior_position_err_ls.append(metric_posterior_position_err)\n",
    "            metric_posterior_orientation_err_ls.append(metric_posterior_orientation_err)\n",
    "\n",
    "\n",
    "            #### Plot Figure ------------------------------------------------\n",
    "            print(\"Before preparing plots\")\n",
    "            fig = plt.figure(1, figsize=(4*5, 3.4))\n",
    "            fig.clf()\n",
    "\n",
    "            ### Image\n",
    "            ax = fig.add_subplot(1, 5, 1)\n",
    "            if dataset == \"hge_customized_cropped\":\n",
    "                img_path = os.path.join(dataset_dir, scene, \"rgb\", '{:05d}'.format((idx_within_scene * traj_l) + (t + L)) + \"-0.jpg\")\n",
    "            else:\n",
    "                img_path = os.path.join(dataset_dir, scene, \"rgb\", '{:05d}'.format((idx_within_scene * traj_l) + (t + L)) + \".png\")\n",
    "            img = mpimg.imread(img_path)\n",
    "            ax.imshow(img)\n",
    "            ax.axis('off')\n",
    "            ax.set_title(\"t=\" + str(t))\n",
    "\n",
    "            ### Quality of depth prediction: Predicted depth vs. GT depth\n",
    "            ax = fig.add_subplot(1, 5, 2)\n",
    "            ax.plot(gt_depths, label=\"GT Depths\")\n",
    "            ax.plot(predicted_depths, label=\"Predicted Depths\")\n",
    "            ax.set_title(\"L1 Loss: \" + str(np.round(metric_depth_l1_loss, 2)) + \" / Shape Loss: \" + str(np.round(metric_depth_shape_loss,2)))\n",
    "            ax.legend()\n",
    "            ax.set_xlabel(\"Index\")\n",
    "            ax.set_ylabel(\"Depth [m]\")\n",
    "            if dataset == \"hge_customized_cropped\":\n",
    "                ax.set_ylim([0,80])  \n",
    "            else:\n",
    "                ax.set_ylim([0,10])\n",
    "\n",
    "\n",
    "            ### Quality of matching: Matched ray vs. Predicted ray\n",
    "            ax = fig.add_subplot(1, 5, 3)\n",
    "            ax.plot(matched_rays, label=\"Matched Rays\", marker='o')\n",
    "            ax.plot(predicted_rays, label=\"Predicted Rays\", marker='o')\n",
    "            ax.set_title(\"L1 Loss: \" + str(np.round(metric_ray_l1_loss, 2)))\n",
    "            ax.legend()\n",
    "            ax.set_xlabel(\"Ray Index\")\n",
    "            ax.set_ylabel(\"Length [m]\")\n",
    "            if dataset == \"hge_customized_cropped\":\n",
    "                ax.set_ylim([0,80])  \n",
    "            else:\n",
    "                ax.set_ylim([0,10])\n",
    "\n",
    "\n",
    "            ### Quality of localization per observation: Predicted pose vs. GT pose\n",
    "            s = 0.25\n",
    "            ax = fig.add_subplot(1, 5, 4)\n",
    "            if dataset == \"hge_customized_cropped\":\n",
    "                ax.imshow(likelihood_2d, cmap=\"coolwarm\", origin=\"lower\")\n",
    "            else:\n",
    "                ax.imshow(likelihood_2d, origin=\"lower\", cmap=\"coolwarm\")\n",
    "            ax.set_title(\"Error: \" + str(np.round(metric_observation_position_err,2)) + \"m / \" + str(np.round(metric_observation_orientation_err,2)) + \"°\")\n",
    "            ax.axis(\"off\")\n",
    "            ax.quiver(\n",
    "                likelihood_pred[0],\n",
    "                likelihood_pred[1],\n",
    "                s*np.cos(likelihood_pred[2]),\n",
    "                s*np.sin(likelihood_pred[2]),\n",
    "                color=\"blue\",\n",
    "                width=s*0.2,\n",
    "                scale_units=\"inches\",\n",
    "                units=\"inches\",\n",
    "                scale=1,\n",
    "                headwidth=3,\n",
    "                headlength=3,\n",
    "                headaxislength=3,\n",
    "                minlength=s*0.1,\n",
    "                angles = \"xy\"\n",
    "            )\n",
    "            ax.quiver(\n",
    "                gt_pose_desdf[t + L, 0],\n",
    "                gt_pose_desdf[t + L, 1],\n",
    "                s*np.cos(gt_pose_desdf[t + L, 2]),\n",
    "                s*np.sin(gt_pose_desdf[t + L, 2]),\n",
    "                color=\"green\",\n",
    "                width=s*0.2,\n",
    "                scale_units=\"inches\",\n",
    "                units=\"inches\",\n",
    "                scale=1,\n",
    "                headwidth=3,\n",
    "                headlength=3,\n",
    "                headaxislength=3,\n",
    "                minlength=s*0.1,\n",
    "                angles = \"xy\"\n",
    "            )\n",
    "\n",
    "            ### Quality of localization per observation: Predicted pose vs. GT pose\n",
    "            ax = fig.add_subplot(1, 5, 5)\n",
    "            if dataset == \"hge_customized_cropped\":\n",
    "                ax.imshow(posterior_2d.detach().cpu().numpy(), origin=\"lower\", cmap=\"coolwarm\")\n",
    "            else:\n",
    "                ax.imshow(posterior_2d.detach().cpu().numpy(), origin=\"lower\", cmap=\"coolwarm\")\n",
    "            ax.set_title(\"Error: \" + str(np.round(metric_posterior_position_err,2)) + \"m / \" + str(np.round(metric_posterior_orientation_err,2)) + \"°\")\n",
    "            ax.axis(\"off\")\n",
    "            ax.quiver(\n",
    "                pose[0],\n",
    "                pose[1],\n",
    "                s*np.cos(pose[2]),\n",
    "                s*np.sin(pose[2]),\n",
    "                color=\"blue\",\n",
    "                width=s*0.2,\n",
    "                scale_units=\"inches\",\n",
    "                units=\"inches\",\n",
    "                scale=1,\n",
    "                headwidth=3,\n",
    "                headlength=3,\n",
    "                headaxislength=3,\n",
    "                minlength=s*0.1,\n",
    "                angles = \"xy\"\n",
    "            )\n",
    "            ax.quiver(\n",
    "                gt_pose_desdf[t + L, 0],\n",
    "                gt_pose_desdf[t + L, 1],\n",
    "                s*np.cos(gt_pose_desdf[t + L, 2]),\n",
    "                s*np.sin(gt_pose_desdf[t + L, 2]),\n",
    "                color=\"green\",\n",
    "                width=s*0.2,\n",
    "                scale_units=\"inches\",\n",
    "                units=\"inches\",\n",
    "                scale=1,\n",
    "                headwidth=3,\n",
    "                headlength=3,\n",
    "                headaxislength=3,\n",
    "                minlength=s*0.1,\n",
    "                angles = \"xy\"\n",
    "            )\n",
    "            plt.tight_layout()\n",
    "\n",
    "            if not os.path.exists(\n",
    "                os.path.join(evol_path, \"pretty_filter_extended\", str(data_idx))\n",
    "            ):\n",
    "                os.makedirs(os.path.join(evol_path, \"pretty_filter_extended\", str(data_idx)))\n",
    "            fig.savefig(\n",
    "                os.path.join(\n",
    "                    evol_path, \"pretty_filter_extended\", str(data_idx), str(t) + \".png\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "        elif (evol_path is not None) & (log_extended==False):\n",
    "            # plot posterior 2d\n",
    "            fig = plt.figure(0, figsize=(20, 20))\n",
    "            fig.clf()\n",
    "            ax = fig.add_subplot(1, 2, 2)\n",
    "            ax.imshow(\n",
    "                posterior_2d.detach().cpu().numpy(), origin=\"lower\", cmap=\"coolwarm\"\n",
    "            )\n",
    "            ax.quiver(\n",
    "                pose[0],\n",
    "                pose[1],\n",
    "                np.cos(pose[2]),\n",
    "                np.sin(pose[2]),\n",
    "                color=\"blue\",\n",
    "                width=0.2,\n",
    "                scale_units=\"inches\",\n",
    "                units=\"inches\",\n",
    "                scale=1,\n",
    "                headwidth=3,\n",
    "                headlength=3,\n",
    "                headaxislength=3,\n",
    "                minlength=0.1,\n",
    "            )\n",
    "            ax.quiver(\n",
    "                gt_pose_desdf[t + L, 0],\n",
    "                gt_pose_desdf[t + L, 1],\n",
    "                np.cos(gt_pose_desdf[t + L, 2]),\n",
    "                np.sin(gt_pose_desdf[t + L, 2]),\n",
    "                color=\"green\",\n",
    "                width=0.2,\n",
    "                scale_units=\"inches\",\n",
    "                units=\"inches\",\n",
    "                scale=1,\n",
    "                headwidth=3,\n",
    "                headlength=3,\n",
    "                headaxislength=3,\n",
    "                minlength=0.1,\n",
    "            )\n",
    "            ax.axis(\"off\")\n",
    "            ax.set_title(str(t) + \" posterior\")\n",
    "\n",
    "            ax = fig.add_subplot(1, 2, 1)\n",
    "            ax.imshow(likelihood_2d, origin=\"lower\", cmap=\"coolwarm\")\n",
    "            ax.set_title(str(t) + \" likelihood\")\n",
    "            ax.axis(\"off\")\n",
    "            ax.quiver(\n",
    "                likelihood_pred[0],\n",
    "                likelihood_pred[1],\n",
    "                np.cos(likelihood_pred[2]),\n",
    "                np.sin(likelihood_pred[2]),\n",
    "                color=\"blue\",\n",
    "                width=0.2,\n",
    "                scale_units=\"inches\",\n",
    "                units=\"inches\",\n",
    "                scale=1,\n",
    "                headwidth=3,\n",
    "                headlength=3,\n",
    "                headaxislength=3,\n",
    "                minlength=0.1,\n",
    "            )\n",
    "            ax.quiver(\n",
    "                gt_pose_desdf[t + L, 0],\n",
    "                gt_pose_desdf[t + L, 1],\n",
    "                np.cos(gt_pose_desdf[t + L, 2]),\n",
    "                np.sin(gt_pose_desdf[t + L, 2]),\n",
    "                color=\"green\",\n",
    "                width=0.2,\n",
    "                scale_units=\"inches\",\n",
    "                units=\"inches\",\n",
    "                scale=1,\n",
    "                headwidth=3,\n",
    "                headlength=3,\n",
    "                headaxislength=3,\n",
    "                minlength=0.1,\n",
    "            )\n",
    "\n",
    "            if not os.path.exists(\n",
    "                os.path.join(evol_path, \"pretty_filter\", str(data_idx))\n",
    "            ):\n",
    "                os.makedirs(os.path.join(evol_path, \"pretty_filter\", str(data_idx)))\n",
    "            fig.savefig(\n",
    "                os.path.join(\n",
    "                    evol_path, \"pretty_filter\", str(data_idx), str(t) + \".png\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "        print(\"Before transition\")\n",
    "        # transition\n",
    "        # use ground truth to compute transitions, use relative poses\n",
    "        if t + L == traj_l - 1:\n",
    "            continue\n",
    "\n",
    "        if dataset == \"hge_customized_cropped\":\n",
    "            current_pose_desdf = torch.from_numpy(gt_pose_desdf[t + L, :])\n",
    "            next_pose_desdf = torch.from_numpy(gt_pose_desdf[t + L + 1, :])\n",
    "            current_pose_desdf = current_pose_desdf[[1, 0, 2]]\n",
    "            next_pose_desdf = next_pose_desdf[[1, 0, 2]]\n",
    "            transition = get_rel_pose(current_pose_desdf, next_pose_desdf)\n",
    "#            prior = transit(\n",
    "#                posterior, transition, sig_o=0.1, sig_x=0.1, sig_y=0.1, tsize=7, rsize=7, resolution=1\n",
    "#            )\n",
    "            prior = transit(\n",
    "                posterior, transition, sig_o=0.1, sig_x=0.1, sig_y=0.1, tsize=7, rsize=7, resolution=1\n",
    "            )\n",
    "        else:\n",
    "            current_pose = poses[0, t + L, :]\n",
    "            next_pose = poses[0, t + L + 1, :]\n",
    "\n",
    "            transition = get_rel_pose(current_pose, next_pose)\n",
    "            prior = transit(\n",
    "                posterior, transition, sig_o=0.1, sig_x=0.1, sig_y=0.1, tsize=7, rsize=7\n",
    "            )\n",
    "\n",
    "        end_iter = time.time()\n",
    "        matching_time += matching_end - matching_start\n",
    "        feature_extraction_time += feature_extraction_end - feature_extraction_start\n",
    "        iteration_time += end_iter - start_iter\n",
    "        n_iter += 1\n",
    "\n",
    "    # Evaluate quality of localization per observation\n",
    "    if (evol_path is not None) and (log_extended==True):\n",
    "        acc_record = np.array(metric_observation_position_err_ls)\n",
    "        acc_orn_record = np.array(metric_observation_orientation_err_ls)\n",
    "\n",
    "        \n",
    "        # Calculate recalls\n",
    "        recall_10m = np.sum(acc_record < 10) / acc_record.shape[0]\n",
    "        recall_5m = np.sum(acc_record < 5) / acc_record.shape[0]\n",
    "        recall_2m = np.sum(acc_record < 2) / acc_record.shape[0]\n",
    "        recall_1m = np.sum(acc_record < 1) / acc_record.shape[0]\n",
    "        recall_0_5m = np.sum(acc_record < 0.5) / acc_record.shape[0]\n",
    "        recall_0_1m = np.sum(acc_record < 0.1) / acc_record.shape[0]\n",
    "        recall_1m_30deg = np.sum(np.logical_and(acc_record < 1, acc_orn_record < 30)) / acc_record.shape[0]\n",
    "\n",
    "        # Print recalls\n",
    "        print(\"10m recall = \", recall_10m)\n",
    "        print(\"5m recall = \", recall_5m)\n",
    "        print(\"2m recall = \", recall_2m)\n",
    "        print(\"1m recall = \", recall_1m)\n",
    "        print(\"0.5m recall = \", recall_0_5m)\n",
    "        print(\"0.1m recall = \", recall_0_1m)\n",
    "        print(\"1m 30 deg recall = \", recall_1m_30deg)\n",
    "\n",
    "        # Plot bar plot of recalls\n",
    "        recalls = [recall_0_1m, recall_0_5m, recall_1m, recall_1m_30deg, recall_2m, recall_5m, recall_10m]\n",
    "        labels = ['0.1m', '0.5m', '1m', '1m 30°', '2m', '5m', '10m']\n",
    "        fig = plt.figure(3, figsize=(6, 6))\n",
    "        ax = fig.add_subplot(1, 1, 1)\n",
    "        ax.bar(labels, recalls)\n",
    "        #ax.set_xlabel('Recall Type', fontsize=14)\n",
    "        ax.set_ylabel('Recall', fontsize=14)\n",
    "        ax.tick_params(axis='x', labelsize=14)\n",
    "        ax.tick_params(axis='y', labelsize=14)\n",
    "        ax.set_ylim([0, 1])\n",
    "        ax.grid(True)\n",
    "        plt.show()\n",
    "        fig.savefig(os.path.join(evol_path, \"pretty_filter_extended\", str(data_idx), \"recalls\" + \".png\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if (evol_path is not None) and (log_extended==True):\n",
    "        \n",
    "        pred_poses_map = np.stack(pred_poses_map)\n",
    "\n",
    "        ## Error\n",
    "        error = (\n",
    "            ((pred_poses_map[-(traj_l - L):, :2] - poses_map[-(traj_l - L):, :2]) ** 2).sum(axis=1)\n",
    "            ** 0.5\n",
    "        ) * original_resolution\n",
    "\n",
    "\n",
    "        ## Bounding Box Diagonal\n",
    "        # Calculate bounding box diagonal for each step\n",
    "        bounding_box_diagonals = []\n",
    "        for i in range(1, len(poses_map[-(traj_l - L):, :2]) + 1):\n",
    "            current_positions = poses_map[-(traj_l - L):, :2][:i]\n",
    "            diagonal_length = minimum_bounding_box(current_positions) * original_resolution\n",
    "            bounding_box_diagonals.append(diagonal_length)\n",
    "        bounding_box_diagonal = np.array(bounding_box_diagonals)\n",
    "\n",
    "\n",
    "        ## Distance Travelled\n",
    "        # Calculate the differences between each consecutive pair of positions\n",
    "        differences = np.diff(poses_map[-(traj_l - L):, :2], axis=0)\n",
    "        # Calculate the Euclidean distance for each pair\n",
    "        distances = np.sqrt((differences ** 2).sum(axis=1))* original_resolution\n",
    "        # Calculate the accumulated distance for each timestamp\n",
    "        distance_travelled = np.concatenate(([0], np.cumsum(distances)))\n",
    "\n",
    "        ## Plot Error vs. Distance Travelled\n",
    "        fig = plt.figure(4, figsize=(6, 9))\n",
    "        fig.clf()\n",
    "        ax = fig.add_subplot(3, 1, 1)\n",
    "        ax.plot(error)\n",
    "        ax.grid()\n",
    "        ax.set_xlabel(\"Step [-]\", fontsize=14)\n",
    "        ax.set_ylabel(\"Error [m]\", fontsize=14)\n",
    "        ax.tick_params(axis='x', labelsize=14)\n",
    "        ax.tick_params(axis='y', labelsize=14) \n",
    "        if dataset == \"hge_customized_cropped\":\n",
    "            ax.set_ylim([0,50])  \n",
    "        else:\n",
    "            ax.set_ylim([0,5])\n",
    "        ax = fig.add_subplot(3, 1, 2)\n",
    "        ax.plot(distance_travelled, error)\n",
    "        ax.grid()\n",
    "        ax.set_xlabel(\"Distance Travelled [m]\", fontsize=14)\n",
    "        ax.set_ylabel(\"Error [m]\", fontsize=14)\n",
    "        ax.tick_params(axis='x', labelsize=14)\n",
    "        ax.tick_params(axis='y', labelsize=14)\n",
    "        if dataset == \"hge_customized_cropped\":\n",
    "            ax.set_ylim([0,50])  \n",
    "        else:\n",
    "            ax.set_ylim([0,5])\n",
    "        ax = fig.add_subplot(3, 1, 3)\n",
    "        ax.plot(bounding_box_diagonal, error)\n",
    "        ax.grid()\n",
    "        ax.set_xlabel(\"Bounding Box Diagonal [m]\", fontsize=14)\n",
    "        ax.set_ylabel(\"Error [m]\", fontsize=14)\n",
    "        ax.tick_params(axis='x', labelsize=14)\n",
    "        ax.tick_params(axis='y', labelsize=14)\n",
    "        if dataset == \"hge_customized_cropped\":\n",
    "            ax.set_ylim([0,50])\n",
    "        else:\n",
    "            ax.set_ylim([0,5])\n",
    "        fig.tight_layout()\n",
    "\n",
    "        if not os.path.exists(os.path.join(evol_path, \"pretty_filter_extended\", str(data_idx))):\n",
    "            os.makedirs(os.path.join(evol_path, \"pretty_filter_extended\", str(data_idx)))\n",
    "        fig.savefig(os.path.join(evol_path, \"pretty_filter_extended\", str(data_idx), \"error_evolution\" + \".png\"))\n",
    "\n",
    "\n",
    "\n",
    "    if (evol_path is not None) and (log_extended==True):    \n",
    "            fig = plt.figure(5, figsize=(2*6, 4*3))\n",
    "            \n",
    "            ax = fig.add_subplot(4, 2, 1)\n",
    "            ax.plot(metric_depth_l1_loss_ls)\n",
    "            ax.grid()\n",
    "            ax.set_xlabel(\"Step [-]\", fontsize=14)\n",
    "            ax.set_ylabel(\"Error [m]\", fontsize=14)\n",
    "            ax.tick_params(axis='x', labelsize=14)\n",
    "            ax.tick_params(axis='y', labelsize=14) \n",
    "            if dataset == \"hge_customized_cropped\":\n",
    "                ax.set_ylim([0,50])\n",
    "            else:\n",
    "                ax.set_ylim([0,5])\n",
    "            ax.set_title(\"Depth Mean Absolute Error\", fontsize=14)\n",
    "\n",
    "            ax = fig.add_subplot(4, 2, 2)\n",
    "            ax.plot(metric_depth_shape_loss_ls)\n",
    "            ax.grid()\n",
    "            ax.set_xlabel(\"Step [-]\", fontsize=14)\n",
    "            ax.set_ylabel(\"Similarity [-]\", fontsize=14)\n",
    "            ax.tick_params(axis='x', labelsize=14)\n",
    "            ax.tick_params(axis='y', labelsize=14)\n",
    "            ax.set_ylim([-1.05, 1.05])\n",
    "            ax.set_title(\"Depth Cosine Similarity\", fontsize=14)\n",
    "\n",
    "            ax = fig.add_subplot(4, 2, 3)\n",
    "            ax.plot(metric_ray_l1_loss_ls)\n",
    "            ax.grid()\n",
    "            ax.set_xlabel(\"Step [-]\", fontsize=14)\n",
    "            ax.set_ylabel(\"Error [m]\", fontsize=14)\n",
    "            ax.tick_params(axis='x', labelsize=14)\n",
    "            ax.tick_params(axis='y', labelsize=14)\n",
    "            if dataset == \"hge_customized_cropped\":\n",
    "                ax.set_ylim([0,50])\n",
    "            else:\n",
    "                ax.set_ylim([0,5])\n",
    "            ax.set_title(\"Ray Length Mean Abs. Error\", fontsize=14)\n",
    "\n",
    "            # Create an empty and invisible subplot at position (4, 2, 4)\n",
    "            ax = fig.add_subplot(4, 2, 4)\n",
    "            ax.axis('off')  # Turn off the axis for this subplot\n",
    "\n",
    "            ax = fig.add_subplot(4, 2, 5)\n",
    "            ax.plot(metric_observation_position_err_ls)\n",
    "            ax.grid()\n",
    "            ax.set_xlabel(\"Step [-]\", fontsize=14)\n",
    "            ax.set_ylabel(\"Error [m]\", fontsize=14)\n",
    "            ax.tick_params(axis='x', labelsize=14)\n",
    "            ax.tick_params(axis='y', labelsize=14)\n",
    "            if dataset == \"hge_customized_cropped\":\n",
    "                ax.set_ylim([0,50])\n",
    "            else:\n",
    "                ax.set_ylim([0,5])\n",
    "            ax.set_title(\"Observation Position Abs. Error\", fontsize=14)\n",
    "\n",
    "            ax = fig.add_subplot(4, 2, 6)\n",
    "            ax.plot(metric_observation_orientation_err_ls)\n",
    "            ax.grid()\n",
    "            ax.set_xlabel(\"Step [-]\", fontsize=14)\n",
    "            ax.set_ylabel(\"Error [°]\", fontsize=14)\n",
    "            ax.tick_params(axis='x', labelsize=14)\n",
    "            ax.tick_params(axis='y', labelsize=14)\n",
    "            ax.set_ylim([0, 185])\n",
    "            ax.set_title(\"Observation Orientation Abs. Error\", fontsize=14)\n",
    "\n",
    "            ax = fig.add_subplot(4, 2, 7)\n",
    "            ax.plot(metric_posterior_position_err_ls)\n",
    "            ax.grid()\n",
    "            ax.set_xlabel(\"Step [-]\", fontsize=14)\n",
    "            ax.set_ylabel(\"Error [m]\", fontsize=14)\n",
    "            ax.tick_params(axis='x', labelsize=14)\n",
    "            ax.tick_params(axis='y', labelsize=14)\n",
    "            if dataset == \"hge_customized_cropped\":\n",
    "                ax.set_ylim([0,50])\n",
    "            else:\n",
    "                ax.set_ylim([0,5])\n",
    "            ax.set_title(\"Posterior Position Abs. Error\", fontsize=14)\n",
    "\n",
    "            ax = fig.add_subplot(4, 2, 8)\n",
    "            ax.plot(metric_posterior_orientation_err_ls)\n",
    "            ax.grid()\n",
    "            ax.set_xlabel(\"Step [-]\", fontsize=14)\n",
    "            ax.set_ylabel(\"Error [°]\", fontsize=14)\n",
    "            ax.tick_params(axis='x', labelsize=14)\n",
    "            ax.tick_params(axis='y', labelsize=14)\n",
    "            ax.set_ylim([0, 185])\n",
    "            ax.set_title(\"Posterior Orientation Abs. Error\", fontsize=14)\n",
    "            fig.tight_layout()\n",
    "\n",
    "            if not os.path.exists(os.path.join(evol_path, \"pretty_filter_extended\", str(data_idx))):\n",
    "                os.makedirs(os.path.join(evol_path, \"pretty_filter_extended\", str(data_idx)))\n",
    "            fig.savefig(os.path.join(evol_path, \"pretty_filter_extended\", str(data_idx), \"metrics\" + \".png\"))\n",
    "\n",
    "    if log_error:\n",
    "        pred_poses_map = np.stack(pred_poses_map)\n",
    "        # record success rate, from map to global\n",
    "        last_errors = (\n",
    "            ((pred_poses_map[-10:, :2] - poses_map[-10:, :2]) ** 2).sum(axis=1)\n",
    "            ** 0.5\n",
    "        ) * original_resolution\n",
    "        # compute RMSE\n",
    "        RMSE = (\n",
    "            ((pred_poses_map[-10:, :2] - poses_map[-10:, :2]) ** 2)\n",
    "            .sum(axis=1)\n",
    "            .mean()\n",
    "        ) ** 0.5 * original_resolution\n",
    "        RMSEs.append(RMSE)\n",
    "        print(\"last_errors\", last_errors)\n",
    "        if all(last_errors < 1):\n",
    "            success_10.append(True)\n",
    "        else:\n",
    "            success_10.append(False)\n",
    "\n",
    "        if all(last_errors < 0.5):\n",
    "            success_5.append(True)\n",
    "        else:\n",
    "            success_5.append(False)\n",
    "\n",
    "        if all(last_errors < 0.3):\n",
    "            success_3.append(True)\n",
    "        else:\n",
    "            success_3.append(False)\n",
    "\n",
    "        if all(last_errors < 0.2):\n",
    "            success_2.append(True)\n",
    "        else:\n",
    "            success_2.append(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e28c11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1537454e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7ecb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Gibson t\n",
    "current_pose = poses[0, t + L, :]\n",
    "next_pose = poses[0, t + L + 1, :]\n",
    "\n",
    "transition = get_rel_pose(current_pose, next_pose)\n",
    "prior = transit(\n",
    "    posterior, transition, sig_o=0.1, sig_x=0.1, sig_y=0.1, tsize=7, rsize=7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1870372",
   "metadata": {},
   "outputs": [],
   "source": [
    "poses.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502c4dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "poses.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da2faa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ca8448",
   "metadata": {},
   "outputs": [],
   "source": [
    "poses[0,0,0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7438b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "poses[0,0,0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fbf6c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60610b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "poses[0,14,0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3d668d",
   "metadata": {},
   "outputs": [],
   "source": [
    "poses[0,14,0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099aa578",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e188c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "poses[0,14,0:2] - poses[0,0,0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764f70ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "poses[0,14,0:2] - poses[0,0,0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342947e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "(poses[0,14,2] - poses[0,0,2])/np.pi*180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003a1450",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9d9710",
   "metadata": {},
   "outputs": [],
   "source": [
    "poses[0,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46cf379",
   "metadata": {},
   "outputs": [],
   "source": [
    "poses[0,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3835bda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "(0.3906 - (0.1106))/np.pi*180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72876fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "poses[0,:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b43046",
   "metadata": {},
   "outputs": [],
   "source": [
    "poses[0,:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d205a1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935bb7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "poses[0,:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b148b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "poses[0,:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65226854",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20062af",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9feecef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_pose_desdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c76f1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_pose_desdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536ba985",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cb8192",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9253239d",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_pose_desdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff991963",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_pose_desdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4363ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3089206f",
   "metadata": {},
   "outputs": [],
   "source": [
    "transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50189eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263df19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86e846b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb5b07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(posterior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cb5054",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(posterior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d21e8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(posterior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc7c479",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f3070a",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df12e09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44872d97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4e685d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381be876",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa789d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88318e35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5acaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaafebcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4929e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0895671d",
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1e5209",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c617eaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "### transit\n",
    "prior = transit(\n",
    "    posterior, transition, sig_o=0.1, sig_x=0.1, sig_y=0.1, tsize=7, rsize=7, resolution=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd358e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa07987d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cbed1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "H, W, O = list(posterior.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a6c326",
   "metadata": {},
   "outputs": [],
   "source": [
    "O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71d4ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "filters_trans, filter_rot = get_filters(\n",
    "        transition,\n",
    "        O=O,\n",
    "        sig_o=0.1,\n",
    "        sig_x=0.05,\n",
    "        sig_y=0.05,\n",
    "        tsize=5,\n",
    "        rsize=5,\n",
    "        resolution=0.1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0159a95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "O=360\n",
    "sig_o=0.1\n",
    "sig_x=1#0.05\n",
    "sig_y=1#0.05\n",
    "tsize=7\n",
    "rsize=61\n",
    "resolution=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92be9540",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get the filters according to gaussian\n",
    "grid_y, grid_x = torch.meshgrid(\n",
    "    torch.arange(-(tsize - 1) / 2, (tsize + 1) / 2, 1),\n",
    "    torch.arange(-(tsize - 1) / 2, (tsize + 1) / 2, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b3c9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add units\n",
    "grid_x = grid_x * resolution  # 0.1m\n",
    "grid_y = grid_y * resolution  # 0.1m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e0f0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate center of the gaussian for 36 orientations\n",
    "# center for orientation stays the same\n",
    "center_o = transition[-1]\n",
    "center_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c203273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# center_x and center_y depends on the orientation, in total O different, rotate\n",
    "orns = (\n",
    "    torch.arange(0, O, dtype=torch.float32) / O * 2 * torch.pi\n",
    ")  # (O,)\n",
    "print(orns.shape)\n",
    "print(orns[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cff20d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_th = torch.cos(orns).reshape((O, 1, 1))  # (O, 1, 1)\n",
    "s_th = torch.sin(orns).reshape((O, 1, 1))\n",
    "c_th.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ed8ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "center_x = transition[0] * c_th - transition[1] * s_th  # (O, 1, 1)\n",
    "center_y = transition[0] * s_th + transition[1] * c_th  # (O, 1, 1)\n",
    "center_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c3a01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add uncertainty\n",
    "filters_trans = torch.exp(\n",
    "    -((grid_x - center_x) ** 2) / (sig_x**2) - (grid_y - center_y) ** 2 / (sig_y**2)\n",
    ")  # (O, 5, 5)\n",
    "# normalize\n",
    "filters_trans = filters_trans / filters_trans.sum(-1).sum(-1).reshape((O, 1, 1))\n",
    "filters_trans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b2bfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "filters_trans[0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203ad7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "filters_trans[1,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17711d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "filters_trans[180,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3b15af",
   "metadata": {},
   "outputs": [],
   "source": [
    "transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556efe9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885f0259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rotation filter\n",
    "grid_o = (\n",
    "    torch.arange(-(rsize - 1) / 2, (rsize + 1) / 2, 1, device=transition.device)\n",
    "    / O\n",
    "    * 2\n",
    "    * torch.pi\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c482f1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_o/np.pi*180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59001f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_rot = torch.exp(-((grid_o - center_o) ** 2) / (sig_o**2))  # (5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123fab3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_rot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709694e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f831df",
   "metadata": {},
   "outputs": [],
   "source": [
    "filters_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdf1d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_rot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0feb9fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4767035a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b986191",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b685b2e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd16931",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cce5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "filters_trans, filter_rot = get_filters(\n",
    "        transition,\n",
    "        O=36,\n",
    "        sig_o=1,\n",
    "        sig_x=1,\n",
    "        sig_y=1,\n",
    "        tsize=9,\n",
    "        rsize=9,\n",
    "        resolution=1,\n",
    "    ) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd50b5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "filters_trans.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cd6003",
   "metadata": {},
   "outputs": [],
   "source": [
    "filters_trans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c5ac22",
   "metadata": {},
   "outputs": [],
   "source": [
    "filters_trans[0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a070020",
   "metadata": {},
   "outputs": [],
   "source": [
    "filters_trans[0,1:8,1:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbbbc3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d73bc2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if log_error:\n",
    "    RMSEs = np.array(RMSEs)\n",
    "    success_10 = np.array(success_10)\n",
    "    success_5 = np.array(success_5)\n",
    "    success_3 = np.array(success_3)\n",
    "    success_2 = np.array(success_2)\n",
    "\n",
    "    print(\"============================================\")\n",
    "    print(\"1.0 success rate : \", success_10.sum() / len(test_set))\n",
    "    print(\"0.5 success rate : \", success_5.sum() / len(test_set))\n",
    "    print(\"0.3 success rate : \", success_3.sum() / len(test_set))\n",
    "    print(\"0.2 success rate : \", success_2.sum() / len(test_set))\n",
    "    print(\"mean RMSE succeeded : \", RMSEs[success_10].mean())\n",
    "    print(\"mean RMSE all : \", RMSEs.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80b1d42",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if log_timing:\n",
    "    feature_extraction_time = feature_extraction_time / n_iter\n",
    "    matching_time = matching_time / n_iter\n",
    "    iteration_time = iteration_time / n_iter\n",
    "\n",
    "    print(\"============================================\")\n",
    "    print(\"feature_extraction_time : \", feature_extraction_time)\n",
    "    print(\"matching_time : \", matching_time)\n",
    "    print(\"iteration_time : \", iteration_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf27f01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567d8406",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3272b576",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import os\n",
    "import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def extract_number(filename):\n",
    "    try:\n",
    "        return int(filename.split('.')[0])\n",
    "    except (IndexError, ValueError):\n",
    "        return -1  # Return a default value in case of error\n",
    "\n",
    "def resize_image(image, size):\n",
    "    return image.resize(size, Image.Resampling.LANCZOS)\n",
    "\n",
    "evol_path = \"./evol_path/hge_customized_cropped/gt/pretty_filter_extended/desdf_cropped_orn_slice_360_resolution_1/default/4\"\n",
    "\n",
    "\n",
    "### Create GIF\n",
    "#for data_idx in tqdm.tqdm(range(len(test_set))):\n",
    "#for data_idx in tqdm.tqdm(range(37)):\n",
    "for data_idx in tqdm.tqdm(range(1)):\n",
    "    image_folder = evol_path#os.path.join(evol_path, \"pretty_filter_extended\", str(data_idx))\n",
    "    gif_path = os.path.join(image_folder, 'animation.gif')  # Specify the full path for the output GIF file\n",
    "\n",
    "    # List of image filenames, sorted by the number in their name\n",
    "    image_files = [img for img in os.listdir(image_folder) if img.endswith(\".png\") and extract_number(img) != -1]\n",
    "\n",
    "    # Debug: Print the image filenames to inspect their format\n",
    "    print(\"Image files found:\", image_files)\n",
    "\n",
    "    # Sort the images based on the extracted number\n",
    "    images = sorted(image_files, key=extract_number)\n",
    "\n",
    "    # Read images into a list\n",
    "    image_list = []\n",
    "    target_size = None\n",
    "    for filename in images:\n",
    "        image_path = os.path.join(image_folder, filename)\n",
    "        try:\n",
    "            image = Image.open(image_path)\n",
    "            if target_size is None:\n",
    "                target_size = image.size  # Set the target size to the size of the first image\n",
    "                print(f\"Target size set to: {target_size}\")\n",
    "            image = resize_image(image, target_size)\n",
    "            image_list.append(np.array(image))\n",
    "            print(f\"Loaded image: {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading image {image_path}: {e}\")\n",
    "\n",
    "    # Check if image_list is empty\n",
    "    if not image_list:\n",
    "        print(\"No valid images were loaded. Exiting.\")\n",
    "        continue\n",
    "\n",
    "    # Create a GIF from the list of images\n",
    "    imageio.mimsave(gif_path, image_list, duration=1000)  # Adjust duration as needed\n",
    "\n",
    "    print(f\"GIF saved to {gif_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742a73ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf33e20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad4876f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load desdf ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:11<00:00, 11.30s/it]\n"
     ]
    }
   ],
   "source": [
    "# Adapt desdf file\n",
    "\n",
    "# get desdf for the scene\n",
    "print(\"load desdf ...\")\n",
    "desdfs = {}\n",
    "for scene in tqdm.tqdm(test_set.scene_names):\n",
    "    desdfs[scene] = np.load(\n",
    "        os.path.join(desdf_path, scene, \"desdf.npy\"), allow_pickle=True\n",
    "    )#.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "874c0ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapt file\n",
    "desdf_cropped = {}\n",
    "desdf_cropped[\"l\"]=int(0)\n",
    "desdf_cropped[\"t\"]=int(0)\n",
    "desdf_cropped[\"desdf\"]=desdfs[scene]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a4cda42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save file\n",
    "filename = \"desdf_complete_orn_slice_36_resolution_01_complete.npy\"\n",
    "target_path_desdf = \"/cluster/project/cvg/data/lamar/HGE_customized_complete/\"\n",
    "filename = os.path.join(target_path_desdf, filename)\n",
    "np.save(filename, desdf_cropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17558097",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
