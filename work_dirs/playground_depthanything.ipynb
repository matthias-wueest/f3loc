{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/cluster/home/wueestm/f3loc/Depth-Anything-V2')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "root_directory = \"/cluster/home/wueestm/f3loc/Depth-Anything-V2\"\n",
    "os.chdir(root_directory)\n",
    "root_directory\n",
    "Path.cwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/home/wueestm/anaconda3/envs/f3loc/lib/python3.8/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "xFormers not available\n",
      "xFormers not available\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DepthAnythingV2(\n",
       "  (pretrained): DinoVisionTransformer(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))\n",
       "      (norm): Identity()\n",
       "    )\n",
       "    (blocks): ModuleList(\n",
       "      (0-23): 24 x NestedTensorBlock(\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): MemEffAttention(\n",
       "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): LayerScale()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): LayerScale()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "    (head): Identity()\n",
       "  )\n",
       "  (depth_head): DPTHead(\n",
       "    (projects): ModuleList(\n",
       "      (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (2-3): 2 x Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (resize_layers): ModuleList(\n",
       "      (0): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(4, 4))\n",
       "      (1): ConvTranspose2d(512, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (2): Identity()\n",
       "      (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    )\n",
       "    (scratch): Module(\n",
       "      (layer1_rn): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (layer2_rn): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (layer3_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (layer4_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (refinenet1): FeatureFusionBlock(\n",
       "        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (resConfUnit1): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (resConfUnit2): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (refinenet2): FeatureFusionBlock(\n",
       "        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (resConfUnit1): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (resConfUnit2): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (refinenet3): FeatureFusionBlock(\n",
       "        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (resConfUnit1): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (resConfUnit2): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (refinenet4): FeatureFusionBlock(\n",
       "        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (resConfUnit1): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (resConfUnit2): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (output_conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (output_conv2): Sequential(\n",
       "        (0): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "\n",
    "from depth_anything_v2.dpt import DepthAnythingV2\n",
    "\n",
    "model_configs = {\n",
    "    'vits': {'encoder': 'vits', 'features': 64, 'out_channels': [48, 96, 192, 384]},\n",
    "    'vitb': {'encoder': 'vitb', 'features': 128, 'out_channels': [96, 192, 384, 768]},\n",
    "    'vitl': {'encoder': 'vitl', 'features': 256, 'out_channels': [256, 512, 1024, 1024]}\n",
    "}\n",
    "\n",
    "encoder = 'vitl' # 'vitb' #or 'vits', 'vitb'\n",
    "dataset = 'hypersim' # 'hypersim' for indoor model, 'vkitti' for outdoor model\n",
    "# max_depth = 20 # 20 for indoor model, 80 for outdoor model\n",
    "\n",
    "#model = DepthAnythingV2(**{**model_configs[encoder], 'max_depth': max_depth})\n",
    "model = DepthAnythingV2(**model_configs[encoder])\n",
    "model.load_state_dict(torch.load(f'checkpoints/depth_anything_v2_metric_{dataset}_{encoder}.pth', map_location='cpu'))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your raw image\n",
    "raw_image = raw_img\n",
    "input_tensor, _ =  model.image2tensor(raw_image)#, input_size=1440/16*14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass through the DINO model to get embeddings\n",
    "with torch.no_grad():\n",
    "    features_all = model.pretrained.get_intermediate_layers(input_tensor, model.intermediate_layer_idx[model.encoder], reshape=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 49, 37])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_all[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vitl'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.intermediate_layer_idx[model.encoder][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass through the DINO model to get embeddings\n",
    "with torch.no_grad():\n",
    "    features = model.pretrained.get_intermediate_layers(input_tensor, reshape=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "resized_features = F.interpolate(features[0], size=(120, 90), mode='bilinear', align_corners=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 49, 37])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 120, 90])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resized_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, h, w = resized_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1440/16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1440//16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass through the DINO model to get embeddings\n",
    "with torch.no_grad():\n",
    "    features = model.pretrained.get_intermediate_layers(input_tensor, model.intermediate_layer_idx[model.encoder], reshape=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 49, 37])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 49, 37])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 49, 37])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 49, 37])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 49, 37])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfeatures\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "features[4].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([49, 37])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[0][0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "686"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "49*14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "518"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "37*14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3243243243243243"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "49/37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3333333333333333"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_img.shape[0]/raw_img.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39.183673469387756"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_img.shape[0]/49"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1813, 1024])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[0][0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-6.0077e-01,  2.1814e+00,  8.9524e-01,  ...,  5.4303e-01,\n",
       "           -3.8767e-02, -4.4782e+00],\n",
       "          [-1.7773e+00,  9.4267e-01, -4.0352e-03,  ...,  3.1797e-02,\n",
       "           -4.2511e-01,  3.3014e+00],\n",
       "          [-2.5500e-01,  1.5134e-02,  4.5019e-01,  ..., -1.4100e+00,\n",
       "            7.5691e-01,  6.6905e+00],\n",
       "          ...,\n",
       "          [ 1.3716e+00, -9.6508e-01,  6.5209e-03,  ...,  8.0838e-01,\n",
       "            6.3469e-01, -3.3081e+00],\n",
       "          [ 1.1796e+00, -5.3309e-01, -8.7790e-01,  ...,  2.0329e+00,\n",
       "            1.5308e-01, -1.2342e+00],\n",
       "          [ 8.1156e-01, -1.9211e+00, -4.4640e-01,  ...,  2.3958e+00,\n",
       "            8.6098e-01, -5.4680e+00]]]),\n",
       " tensor([[ 0.1375,  0.4776,  1.0067,  ...,  0.2970, -0.4340, -0.0955]]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 11, 17, 23]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.intermediate_layer_idx[model.encoder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[ 1.2214,  1.2214,  1.2200,  ..., -0.1271, -0.1314, -0.1314],\n",
       "           [ 1.2375,  1.2375,  1.2374,  ..., -0.1285, -0.1314, -0.1314],\n",
       "           [ 1.2573,  1.2573,  1.2588,  ..., -0.1282, -0.1332, -0.1313],\n",
       "           ...,\n",
       "           [-0.7361, -0.7926, -0.7160,  ...,  0.1769,  0.1933,  0.2145],\n",
       "           [-0.6499, -0.7348, -0.7767,  ...,  0.1737,  0.1949,  0.2111],\n",
       "           [-0.6457, -0.6694, -0.7334,  ...,  0.1785,  0.1949,  0.2111]],\n",
       " \n",
       "          [[ 0.9580,  0.9580,  0.9565,  ..., -0.4032, -0.4076, -0.4076],\n",
       "           [ 0.9744,  0.9744,  0.9743,  ..., -0.4045, -0.4076, -0.4076],\n",
       "           [ 0.9946,  0.9946,  0.9962,  ..., -0.4043, -0.4094, -0.4075],\n",
       "           ...,\n",
       "           [-0.7981, -0.8559, -0.7776,  ...,  0.0302,  0.0470,  0.0686],\n",
       "           [-0.7100, -0.7968, -0.8397,  ...,  0.0270,  0.0486,  0.0651],\n",
       "           [-0.7058, -0.7300, -0.7953,  ...,  0.0319,  0.0486,  0.0651]],\n",
       " \n",
       "          [[ 0.5834,  0.5834,  0.5819,  ..., -0.7718, -0.7761, -0.7761],\n",
       "           [ 0.5997,  0.5997,  0.5996,  ..., -0.7731, -0.7761, -0.7761],\n",
       "           [ 0.6198,  0.6198,  0.6214,  ..., -0.7729, -0.7779, -0.7760],\n",
       "           ...,\n",
       "           [-0.6072, -0.6647, -0.5868,  ..., -0.0092,  0.0076,  0.0291],\n",
       "           [-0.5195, -0.6059, -0.6486,  ..., -0.0124,  0.0092,  0.0256],\n",
       "           [-0.5153, -0.5394, -0.6044,  ..., -0.0075,  0.0092,  0.0256]]]]),\n",
       " (1920, 1440))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'DepthAnythingV2' has no attribute 'image_to_tensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m raw_image \u001b[38;5;241m=\u001b[39m raw_img\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Prepare the image tensor\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mDepthAnythingV2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_to_tensor\u001b[49m(raw_image)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Forward pass through the DINO model to get embeddings\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'DepthAnythingV2' has no attribute 'image_to_tensor'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision.transforms import Compose\n",
    "\n",
    "# Assuming DepthAnythingV2 is already defined as above\n",
    "# Instantiate your model\n",
    "depth_model = DepthAnythingV2()  # Add necessary parameters here if required\n",
    "\n",
    "# Load your raw image (example: raw_image = np.random.rand(1920, 1440, 3) * 255)\n",
    "raw_image = raw_img\n",
    "\n",
    "# Prepare the image tensor\n",
    "input_tensor = DepthAnythingV2.image_to_tensor(raw_image)\n",
    "\n",
    "# Forward pass through the DINO model to get embeddings\n",
    "with torch.no_grad():\n",
    "    features = depth_model.pretrained.get_intermediate_layers(input_tensor, depth_model.intermediate_layer_idx[depth_model.encoder], return_class_token=True)\n",
    "\n",
    "# Extract the final embeddings from the features\n",
    "final_embeddings = features[-1]  # Assuming the last layer corresponds to final embeddings\n",
    "print(final_embeddings.shape)  # This will give you the shape of the embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.0000 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "image_path_ls = [\n",
    "    \"/cluster/home/wueestm/f3loc/metric3d/data/hge_customized_complete/non-aligned/rgb/00000-0.png\",\n",
    "    \"/cluster/home/wueestm/f3loc/metric3d/data/hge_customized_complete/non-aligned/rgb/00120-0.png\",\n",
    "    \"/cluster/home/wueestm/f3loc/metric3d/data/hge_customized_complete/non-aligned/rgb/00090-0.png\",\n",
    "    \"/cluster/home/wueestm/f3loc/metric3d/data/hge_customized_complete/non-aligned/rgb/00171-0.jpg\"\n",
    "]\n",
    "\n",
    "for image_path in image_path_ls:\n",
    "    #image_path = \"/cluster/home/wueestm/f3loc/metric3d/data/hge_customized_complete/non-aligned/rgb/\"\n",
    "    raw_img = cv2.imread(image_path)\n",
    "    start_time = time.time()\n",
    "    #depth = model.infer_image(raw_img) # HxW depth map in meters in numpy\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Time taken: {elapsed_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DinoVisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (blocks): ModuleList(\n",
       "    (0-23): 24 x NestedTensorBlock(\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): MemEffAttention(\n",
       "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "  (head): Identity()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model initialized\n",
      "image preprocessed\n",
      "before final_layer_idx\n",
      "before final features\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Initialize your DepthAnythingV2 model (using DINOv2 'vitl' for example)\n",
    "model = DepthAnythingV2(encoder='vitl')  # Adjust encoder if necessary\n",
    "\n",
    "print(\"model initialized\")\n",
    "\n",
    "# Ensure the model is on the appropriate device (CPU, CUDA, or MPS)\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# Assuming raw_image is the (1920, 1440, 3) image (loaded as a NumPy array)\n",
    "raw_image = raw_img  # Your image here\n",
    "\n",
    "# Step 1: Preprocess the image using the image2tensor method (already part of the DepthAnythingV2 class)\n",
    "input_tensor, (h, w) = model.image2tensor(raw_image)\n",
    "\n",
    "print(\"image preprocessed\")\n",
    "\n",
    "# Step 2: Pass the image through the pretrained DINO model to get final layer features\n",
    "with torch.no_grad():\n",
    "    # Get the patch size based on input image\n",
    "    patch_h, patch_w = input_tensor.shape[-2] // 14, input_tensor.shape[-1] // 14\n",
    "    \n",
    "    # Extract features from the final layer of the DINO transformer\n",
    "    # Here, we use the final index for the transformer based on the encoder type ('vitl' in this case)\n",
    "    print(\"before final_layer_idx\")\n",
    "    final_layer_idx = [model.intermediate_layer_idx[model.encoder][-1]]  # Only take the final layer index\n",
    "    \n",
    "    # Get the output from the final layer\n",
    "    print(\"before final features\")\n",
    "    final_features = model.pretrained.get_intermediate_layers(\n",
    "        input_tensor, \n",
    "        final_layer_idx, \n",
    "        return_class_token=True  # Return both patch tokens and class token\n",
    "    )\n",
    "\n",
    "# The variable `final_features` now holds the final layer output of the DINO transformer.\n",
    "# You can process it further based on your application (e.g., classification, feature extraction, etc.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model initialized\n",
      "image preprocessed\n",
      "before final features\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# Only pass through the 'pretrained' (DinoVisionTransformer) part of the model\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbefore final features\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m     transformer_output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Step 3: Extract the final embeddings\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# The output of the pretrained DINO transformer is typically a tuple where the first element is the patch embeddings\u001b[39;00m\n\u001b[1;32m     29\u001b[0m final_embeddings \u001b[38;5;241m=\u001b[39m transformer_output\n",
      "File \u001b[0;32m~/anaconda3/envs/f3loc/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/f3loc/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/f3loc/Depth-Anything-V2/depth_anything_v2/dinov2.py:324\u001b[0m, in \u001b[0;36mDinoVisionTransformer.forward\u001b[0;34m(self, is_training, *args, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, is_training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 324\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_training:\n\u001b[1;32m    326\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m~/f3loc/Depth-Anything-V2/depth_anything_v2/dinov2.py:257\u001b[0m, in \u001b[0;36mDinoVisionTransformer.forward_features\u001b[0;34m(self, x, masks)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_features_list(x, masks)\n\u001b[0;32m--> 257\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_tokens_with_masks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[1;32m    260\u001b[0m     x \u001b[38;5;241m=\u001b[39m blk(x)\n",
      "File \u001b[0;32m~/f3loc/Depth-Anything-V2/depth_anything_v2/dinov2.py:213\u001b[0m, in \u001b[0;36mDinoVisionTransformer.prepare_tokens_with_masks\u001b[0;34m(self, x, masks)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare_tokens_with_masks\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, masks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 213\u001b[0m     B, nc, w, h \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    214\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_embed(x)\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m masks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming your DepthAnythingV2 is already instantiated\n",
    "model = DepthAnythingV2(encoder='vitl')\n",
    "\n",
    "# Send model to appropriate device (GPU or CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"model initialized\")\n",
    "\n",
    "# Your input image of shape (1920, 1440, 3)\n",
    "raw_image = raw_img  # Provide your image here (e.g., as a numpy array or tensor)\n",
    "\n",
    "# Step 1: Preprocess the image\n",
    "input_tensor, (h, w) = model.image2tensor(raw_image)\n",
    "input_tensor = input_tensor.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "print(\"image preprocessed\")\n",
    "\n",
    "# Step 2: Pass the input tensor through the pretrained DinoVisionTransformer\n",
    "with torch.no_grad():\n",
    "    # Only pass through the 'pretrained' (DinoVisionTransformer) part of the model\n",
    "    print(\"before final features\")\n",
    "    transformer_output = model.pretrained(input_tensor.to(device))\n",
    "\n",
    "# Step 3: Extract the final embeddings\n",
    "# The output of the pretrained DINO transformer is typically a tuple where the first element is the patch embeddings\n",
    "final_embeddings = transformer_output\n",
    "\n",
    "print(f\"Shape of final embeddings: {final_embeddings.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vitl'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(final_features[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1813, 1024])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_features[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(raw_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1920, 1440, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0938826"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depth.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depth.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine tune model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DepthAnythingV2(\n",
      "  (pretrained): DinoVisionTransformer(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))\n",
      "      (norm): Identity()\n",
      "    )\n",
      "    (blocks): ModuleList(\n",
      "      (0-23): 24 x NestedTensorBlock(\n",
      "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): MemEffAttention(\n",
      "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): LayerScale()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): LayerScale()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (head): Identity()\n",
      "  )\n",
      "  (depth_head): DPTHead(\n",
      "    (projects): ModuleList(\n",
      "      (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (2-3): 2 x Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (resize_layers): ModuleList(\n",
      "      (0): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(4, 4))\n",
      "      (1): ConvTranspose2d(512, 512, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (2): Identity()\n",
      "      (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    )\n",
      "    (scratch): Module(\n",
      "      (layer1_rn): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (layer2_rn): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (layer3_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (layer4_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (refinenet1): FeatureFusionBlock(\n",
      "        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (resConfUnit1): ResidualConvUnit(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (skip_add): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (resConfUnit2): ResidualConvUnit(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (skip_add): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (skip_add): FloatFunctional(\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "      (refinenet2): FeatureFusionBlock(\n",
      "        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (resConfUnit1): ResidualConvUnit(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (skip_add): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (resConfUnit2): ResidualConvUnit(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (skip_add): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (skip_add): FloatFunctional(\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "      (refinenet3): FeatureFusionBlock(\n",
      "        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (resConfUnit1): ResidualConvUnit(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (skip_add): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (resConfUnit2): ResidualConvUnit(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (skip_add): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (skip_add): FloatFunctional(\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "      (refinenet4): FeatureFusionBlock(\n",
      "        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (resConfUnit1): ResidualConvUnit(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (skip_add): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (resConfUnit2): ResidualConvUnit(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (skip_add): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (skip_add): FloatFunctional(\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "      (output_conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (output_conv2): Sequential(\n",
      "        (0): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "        (2): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DinoVisionTransformer(\n",
       "   (patch_embed): PatchEmbed(\n",
       "     (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))\n",
       "     (norm): Identity()\n",
       "   )\n",
       "   (blocks): ModuleList(\n",
       "     (0-23): 24 x NestedTensorBlock(\n",
       "       (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "       (attn): MemEffAttention(\n",
       "         (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "         (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "         (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "         (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "       )\n",
       "       (ls1): LayerScale()\n",
       "       (drop_path1): Identity()\n",
       "       (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "       (mlp): Mlp(\n",
       "         (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "         (act): GELU(approximate='none')\n",
       "         (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "         (drop): Dropout(p=0.0, inplace=False)\n",
       "       )\n",
       "       (ls2): LayerScale()\n",
       "       (drop_path2): Identity()\n",
       "     )\n",
       "   )\n",
       "   (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "   (head): Identity()\n",
       " ),\n",
       " DPTHead(\n",
       "   (projects): ModuleList(\n",
       "     (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "     (1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "     (2-3): 2 x Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "   )\n",
       "   (resize_layers): ModuleList(\n",
       "     (0): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(4, 4))\n",
       "     (1): ConvTranspose2d(512, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "     (2): Identity()\n",
       "     (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "   )\n",
       "   (scratch): Module(\n",
       "     (layer1_rn): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (layer2_rn): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (layer3_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (layer4_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (refinenet1): FeatureFusionBlock(\n",
       "       (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "       (resConfUnit1): ResidualConvUnit(\n",
       "         (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "         (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "         (activation): ReLU()\n",
       "         (skip_add): FloatFunctional(\n",
       "           (activation_post_process): Identity()\n",
       "         )\n",
       "       )\n",
       "       (resConfUnit2): ResidualConvUnit(\n",
       "         (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "         (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "         (activation): ReLU()\n",
       "         (skip_add): FloatFunctional(\n",
       "           (activation_post_process): Identity()\n",
       "         )\n",
       "       )\n",
       "       (skip_add): FloatFunctional(\n",
       "         (activation_post_process): Identity()\n",
       "       )\n",
       "     )\n",
       "     (refinenet2): FeatureFusionBlock(\n",
       "       (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "       (resConfUnit1): ResidualConvUnit(\n",
       "         (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "         (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "         (activation): ReLU()\n",
       "         (skip_add): FloatFunctional(\n",
       "           (activation_post_process): Identity()\n",
       "         )\n",
       "       )\n",
       "       (resConfUnit2): ResidualConvUnit(\n",
       "         (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "         (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "         (activation): ReLU()\n",
       "         (skip_add): FloatFunctional(\n",
       "           (activation_post_process): Identity()\n",
       "         )\n",
       "       )\n",
       "       (skip_add): FloatFunctional(\n",
       "         (activation_post_process): Identity()\n",
       "       )\n",
       "     )\n",
       "     (refinenet3): FeatureFusionBlock(\n",
       "       (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "       (resConfUnit1): ResidualConvUnit(\n",
       "         (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "         (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "         (activation): ReLU()\n",
       "         (skip_add): FloatFunctional(\n",
       "           (activation_post_process): Identity()\n",
       "         )\n",
       "       )\n",
       "       (resConfUnit2): ResidualConvUnit(\n",
       "         (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "         (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "         (activation): ReLU()\n",
       "         (skip_add): FloatFunctional(\n",
       "           (activation_post_process): Identity()\n",
       "         )\n",
       "       )\n",
       "       (skip_add): FloatFunctional(\n",
       "         (activation_post_process): Identity()\n",
       "       )\n",
       "     )\n",
       "     (refinenet4): FeatureFusionBlock(\n",
       "       (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "       (resConfUnit1): ResidualConvUnit(\n",
       "         (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "         (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "         (activation): ReLU()\n",
       "         (skip_add): FloatFunctional(\n",
       "           (activation_post_process): Identity()\n",
       "         )\n",
       "       )\n",
       "       (resConfUnit2): ResidualConvUnit(\n",
       "         (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "         (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "         (activation): ReLU()\n",
       "         (skip_add): FloatFunctional(\n",
       "           (activation_post_process): Identity()\n",
       "         )\n",
       "       )\n",
       "       (skip_add): FloatFunctional(\n",
       "         (activation_post_process): Identity()\n",
       "       )\n",
       "     )\n",
       "     (output_conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "     (output_conv2): Sequential(\n",
       "       (0): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "       (1): ReLU(inplace=True)\n",
       "       (2): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "       (3): ReLU(inplace=True)\n",
       "       (4): Identity()\n",
       "     )\n",
       "   )\n",
       " )]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DinoVisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (blocks): ModuleList(\n",
       "    (0-23): 24 x NestedTensorBlock(\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): MemEffAttention(\n",
       "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "  (head): Identity()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.children())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(model.children())[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
