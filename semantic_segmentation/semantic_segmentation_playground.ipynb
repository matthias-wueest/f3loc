{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/cluster/home/wueestm/f3loc')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "current_directory = Path.cwd()\n",
    "if current_directory.name == \"semantic_segmentation\":\n",
    "    # This means that the notebook is run from the main anomalib directory.\n",
    "    root_directory = current_directory.parent\n",
    "elif current_directory.name == \"f3loc\":\n",
    "    # This means that the notebook is run from the main anomalib directory.\n",
    "    root_directory = current_directory\n",
    "\n",
    "os.chdir(root_directory)\n",
    "root_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def gravity_align_segmentation(\n",
    "    seg_map,\n",
    "    r,\n",
    "    p,\n",
    "    K=np.array([[240, 0, 320], [0, 240, 240], [0, 0, 1]]).astype(np.float32),\n",
    "    mode=0\n",
    "):\n",
    "    \"\"\"\n",
    "    Align the segmentation map with gravity direction.\n",
    "    \n",
    "    Input:\n",
    "        seg_map: input segmentation map of shape (N, H, W), where N is the number of channels.\n",
    "        r: roll angle in radians.\n",
    "        p: pitch angle in radians.\n",
    "        K: camera intrinsics.\n",
    "        mode: interpolation mode for warping, default: 0 - 'linear', else 1 - 'nearest'\n",
    "    \n",
    "    Output:\n",
    "        aligned_seg_map: gravity-aligned segmentation map.\n",
    "    \"\"\"\n",
    "    # Validate input shape\n",
    "    if seg_map.ndim != 3:\n",
    "        raise ValueError(\"Segmentation map must be a 3D array with shape (N, H, W).\")\n",
    "    \n",
    "    N, h, w = seg_map.shape\n",
    "    \n",
    "    # Calculate R_gc from roll and pitch\n",
    "    p = -p  # This is because the pitch axis of robot and camera is in the opposite direction\n",
    "    cr = np.cos(r)\n",
    "    sr = np.sin(r)\n",
    "    cp = np.cos(p)\n",
    "    sp = np.sin(p)\n",
    "\n",
    "    # Compute R_cg first\n",
    "    R_x = np.array([[1, 0, 0], [0, cp, sp], [0, -sp, cp]])  # Pitch\n",
    "    R_z = np.array([[cr, sr, 0], [-sr, cr, 0], [0, 0, 1]])  # Roll\n",
    "\n",
    "    R_cg = R_z @ R_x\n",
    "    R_gc = R_cg.T\n",
    "\n",
    "    # Compute the homography matrix\n",
    "    persp_M = K @ R_gc @ np.linalg.inv(K)\n",
    "\n",
    "    # Create an empty array for the aligned segmentation map\n",
    "    aligned_seg_map = np.zeros_like(seg_map)\n",
    "    \n",
    "    # Process each channel independently\n",
    "    for i in range(N):\n",
    "        # Align the current channel\n",
    "        aligned_channel = cv2.warpPerspective(\n",
    "            seg_map[i, :, :], persp_M, (w, h), flags=cv2.INTER_NEAREST if mode == 1 else cv2.INTER_LINEAR\n",
    "        )\n",
    "        aligned_seg_map[i, :, :] = aligned_channel\n",
    "    \n",
    "    return aligned_seg_map\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example segmentation map (e.g., 150 channels, H=150, W=200)\n",
    "    seg_map = np.random.randint(0, 10, (150, 150, 200), dtype=np.uint8)\n",
    "    \n",
    "    # Example roll and pitch\n",
    "    roll = 0.1  # in radians\n",
    "    pitch = 0.2  # in radians\n",
    "    \n",
    "    # Align the segmentation map\n",
    "    aligned_seg_map = gravity_align_segmentation(seg_map, roll, pitch)\n",
    "    \n",
    "    # aligned_seg_map now contains the segmentation map aligned to gravity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gravity_align(\n",
    "    img,\n",
    "    r,\n",
    "    p,\n",
    "    K=np.array([[240, 0, 320], [0, 240, 240], [0, 0, 1]]).astype(np.float32),\n",
    "    mode=0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Align the image with gravity direction\n",
    "    Input:\n",
    "        img: input image\n",
    "        r: roll\n",
    "        p: pitch\n",
    "        K: camera intrisics\n",
    "        mode: interpolation mode for warping, default: 0 - 'linear', else 1 - 'nearest'\n",
    "    Output:\n",
    "        aligned_img: gravity aligned image\n",
    "    \"\"\"\n",
    "    # calculate R_gc from roll and pitch\n",
    "    # From gravity to camera, yaw->pitch->roll\n",
    "    # From camera to gravity, roll->pitch->yaw\n",
    "    p = (\n",
    "        -p\n",
    "    )  # this is because the pitch axis of robot and camera is in the opposite direction\n",
    "    cr = np.cos(r)\n",
    "    sr = np.sin(r)\n",
    "    cp = np.cos(p)\n",
    "    sp = np.sin(p)\n",
    "\n",
    "    # compute R_cg first\n",
    "    # pitch\n",
    "    R_x = np.array([[1, 0, 0], [0, cp, sp], [0, -sp, cp]])\n",
    "\n",
    "    # roll\n",
    "    R_z = np.array([[cr, sr, 0], [-sr, cr, 0], [0, 0, 1]])\n",
    "\n",
    "    R_cg = R_z @ R_x\n",
    "    R_gc = R_cg.T\n",
    "\n",
    "    # get shape\n",
    "    h, w = list(img.shape[:2])\n",
    "\n",
    "    # directly compute the homography\n",
    "    persp_M = K @ R_gc @ np.linalg.inv(K)\n",
    "\n",
    "    aligned_img = cv2.warpPerspective(\n",
    "        img, persp_M, (w, h), flags=cv2.INTER_NEAREST if mode == 1 else cv2.INTER_LINEAR\n",
    "    )\n",
    "\n",
    "    return aligned_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roll = ref_euler_angles[0]\n",
    "        pitch = ref_euler_angles[1]\n",
    "        ref_img = gravity_align(ref_img, r=pitch, p=-(roll+np.pi/2),  mode=1, K=self.K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoImageProcessor, SegformerForSemanticSegmentation\n",
    "# from PIL import Image\n",
    "# import requests\n",
    "# \n",
    "# image_processor = AutoImageProcessor.from_pretrained(\"nvidia/segformer-b0-finetuned-ade-512-512\")\n",
    "# model = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/segformer-b0-finetuned-ade-512-512\")\n",
    "# \n",
    "# url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "# image = Image.open(requests.get(url, stream=True).raw)\n",
    "# \n",
    "# inputs = image_processor(images=image, return_tensors=\"pt\")\n",
    "# outputs = model(**inputs)\n",
    "# logits = outputs.logits  # shape (batch_size, num_labels, height/4, width/4)\n",
    "# list(logits.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoImageProcessor, SegformerForSemanticSegmentation\n",
    "from PIL import Image\n",
    "\n",
    "# Load the image processor and model\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"nvidia/segformer-b0-finetuned-ade-512-512\")\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/segformer-b0-finetuned-ade-512-512\")\n",
    "#image_processor = AutoImageProcessor.from_pretrained(\"nvidia/segformer-b1-finetuned-ade-512-512\")\n",
    "#model = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/segformer-b1-finetuned-ade-512-512\")\n",
    "#image_processor = AutoImageProcessor.from_pretrained(\"nvidia/segformer-b2-finetuned-ade-512-512\")\n",
    "#model = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/segformer-b2-finetuned-ade-512-512\")\n",
    "\n",
    "#image_processor = AutoImageProcessor.from_pretrained(\"nvidia/segformer-b4-finetuned-ade-512-512\")\n",
    "#model = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/segformer-b4-finetuned-ade-512-512\")\n",
    "#image_processor = AutoImageProcessor.from_pretrained(\"nvidia/segformer-b5-finetuned-ade-640-640\")\n",
    "#model = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/segformer-b5-finetuned-ade-640-640\")\n",
    "\n",
    "image_path_stem = \"/cluster/home/wueestm/f3loc/metric3d/data/hge_customized_complete/non-aligned/rgb/\"\n",
    "#image_path_ls = [image_path_stem + \"00000-0.jpg\", image_path_stem + \"00090-0.jpg\", image_path_stem + \"00120-0.jpg\", image_path_stem + \"00173-0.jpg\", image_path_stem + \"00334-0.jpg\", image_path_stem + \"00342-0.jpg\"]\n",
    "image_path_ls = [image_path_stem + \"00120-0.jpg\"]\n",
    "\n",
    "# Load an image\n",
    "for image_path in image_path_ls:\n",
    "    #image_path = \"/cluster/home/wueestm/f3loc/metric3d/data/hge_customized_complete/non-aligned/rgb/00120-0.jpg\"\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    # Preprocess the image and forward it through the model\n",
    "    inputs = image_processor(images=image, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits  # shape (batch_size, num_labels, height/4, width/4)\n",
    "\n",
    "    # Get the predicted class for each pixel\n",
    "    predicted_class = logits.argmax(dim=1).squeeze().cpu().numpy()\n",
    "\n",
    "    # Resize the predicted class indices to match the original image size\n",
    "    predicted_class_resized = Image.fromarray(predicted_class.astype(np.uint8)).resize(image.size, resample=Image.NEAREST)\n",
    "    predicted_class_resized = np.array(predicted_class_resized)\n",
    "\n",
    "    # Manually define the ADE20K label mapping\n",
    "    ade20k_labels = {\n",
    "        \"0\": \"wall\", \"1\": \"building\", \"2\": \"sky\", \"3\": \"floor\", \"4\": \"tree\", \n",
    "        \"5\": \"ceiling\", \"6\": \"road\", \"7\": \"bed\", \"8\": \"windowpane\", \"9\": \"grass\", \n",
    "        \"10\": \"cabinet\", \"11\": \"sidewalk\", \"12\": \"person\", \"13\": \"earth\", \"14\": \"door\", \n",
    "        \"15\": \"table\", \"16\": \"mountain\", \"17\": \"plant\", \"18\": \"curtain\", \"19\": \"chair\", \n",
    "        \"20\": \"car\", \"21\": \"water\", \"22\": \"painting\", \"23\": \"sofa\", \"24\": \"shelf\", \n",
    "        \"25\": \"house\", \"26\": \"sea\", \"27\": \"mirror\", \"28\": \"rug\", \"29\": \"field\", \n",
    "        \"30\": \"armchair\", \"31\": \"seat\", \"32\": \"fence\", \"33\": \"desk\", \"34\": \"rock\", \n",
    "        \"35\": \"wardrobe\", \"36\": \"lamp\", \"37\": \"bathtub\", \"38\": \"railing\", \"39\": \"cushion\", \n",
    "        \"40\": \"base\", \"41\": \"box\", \"42\": \"column\", \"43\": \"signboard\", \"44\": \"chest of drawers\", \n",
    "        \"45\": \"counter\", \"46\": \"sand\", \"47\": \"sink\", \"48\": \"skyscraper\", \"49\": \"fireplace\", \n",
    "        \"50\": \"refrigerator\", \"51\": \"grandstand\", \"52\": \"path\", \"53\": \"stairs\", \"54\": \"runway\", \n",
    "        \"55\": \"case\", \"56\": \"pool table\", \"57\": \"pillow\", \"58\": \"screen door\", \"59\": \"stairway\", \n",
    "        \"60\": \"river\", \"61\": \"bridge\", \"62\": \"bookcase\", \"63\": \"blind\", \"64\": \"coffee table\", \n",
    "        \"65\": \"toilet\", \"66\": \"flower\", \"67\": \"book\", \"68\": \"hill\", \"69\": \"bench\", \n",
    "        \"70\": \"countertop\", \"71\": \"stove\", \"72\": \"palm\", \"73\": \"kitchen island\", \"74\": \"computer\", \n",
    "        \"75\": \"swivel chair\", \"76\": \"boat\", \"77\": \"bar\", \"78\": \"arcade machine\", \"79\": \"hovel\", \n",
    "        \"80\": \"bus\", \"81\": \"towel\", \"82\": \"light\", \"83\": \"truck\", \"84\": \"tower\", \n",
    "        \"85\": \"chandelier\", \"86\": \"awning\", \"87\": \"streetlight\", \"88\": \"booth\", \"89\": \"television receiver\", \n",
    "        \"90\": \"airplane\", \"91\": \"dirt track\", \"92\": \"apparel\", \"93\": \"pole\", \"94\": \"land\", \n",
    "        \"95\": \"bannister\", \"96\": \"escalator\", \"97\": \"ottoman\", \"98\": \"bottle\", \"99\": \"buffet\", \n",
    "        \"100\": \"poster\", \"101\": \"stage\", \"102\": \"van\", \"103\": \"ship\", \"104\": \"fountain\", \n",
    "        \"105\": \"conveyer belt\", \"106\": \"canopy\", \"107\": \"washer\", \"108\": \"plaything\", \"109\": \"swimming pool\", \n",
    "        \"110\": \"stool\", \"111\": \"barrel\", \"112\": \"basket\", \"113\": \"waterfall\", \"114\": \"tent\", \n",
    "        \"115\": \"bag\", \"116\": \"minibike\", \"117\": \"cradle\", \"118\": \"oven\", \"119\": \"ball\", \n",
    "        \"120\": \"food\", \"121\": \"step\", \"122\": \"tank\", \"123\": \"trade name\", \"124\": \"microwave\", \n",
    "        \"125\": \"pot\", \"126\": \"animal\", \"127\": \"bicycle\", \"128\": \"lake\", \"129\": \"dishwasher\", \n",
    "        \"130\": \"screen\", \"131\": \"blanket\", \"132\": \"sculpture\", \"133\": \"hood\", \"134\": \"sconce\", \n",
    "        \"135\": \"vase\", \"136\": \"traffic light\", \"137\": \"tray\", \"138\": \"ashcan\", \"139\": \"fan\", \n",
    "        \"140\": \"pier\", \"141\": \"crt screen\", \"142\": \"plate\", \"143\": \"monitor\", \"144\": \"bulletin board\", \n",
    "        \"145\": \"shower\", \"146\": \"radiator\", \"147\": \"glass\", \"148\": \"clock\", \"149\": \"flag\"\n",
    "    }\n",
    "\n",
    "    # Classes to keep and display in color\n",
    "    classes_to_display = [0, 1, 3, 5, 8, 12, 14, 15, 19, 42, 53, 59, 97, 104, 132, 147]\n",
    "    class_names = {i: ade20k_labels[str(i)] for i in classes_to_display}\n",
    "\n",
    "    # Manually specify distinct colors for each class\n",
    "    class_colors = {\n",
    "        0: [255, 0, 0],        # Red for 'wall'\n",
    "        1: [0, 255, 0],        # Green for 'building'\n",
    "        3: [0, 0, 255],        # Blue for 'floor'\n",
    "        5: [255, 255, 0],      # Yellow for 'ceiling'\n",
    "        8: [0, 255, 255],      # Cyan for 'windowpane'\n",
    "        12: [255, 0, 255],     # Magenta for 'person'\n",
    "        14: [128, 0, 128],     # Purple for 'door'\n",
    "        15: [128, 128, 0],     # Olive for 'table'\n",
    "        19: [0, 128, 128],     # Teal for 'chair'\n",
    "        42: [90, 90, 90],   # Dark gray for 'column'\n",
    "        53: [192, 192, 192],   # Light gray for 'stairs'\n",
    "        59: [255, 165, 0],     # Orange for 'stairway'\n",
    "        97: [255, 105, 180],   # Pink for 'ottoman'\n",
    "        104: [255, 20, 147],   # Deep pink for 'fountain'\n",
    "        132: [255, 69, 0],     # Red orange for 'sculpture'\n",
    "        147: [139, 69, 19]     # Saddle brown for 'glass'\n",
    "    }\n",
    "\n",
    "    # Create a blank segmentation map (all black)\n",
    "    segmentation_map = np.zeros((*predicted_class_resized.shape, 3), dtype=np.uint8)\n",
    "\n",
    "    # Fill in the segmentation map with colors only for the specified classes\n",
    "    for class_id, color in class_colors.items():\n",
    "        mask = predicted_class_resized == class_id\n",
    "        segmentation_map[mask] = color\n",
    "\n",
    "    # Convert to image format\n",
    "    segmentation_image = Image.fromarray(segmentation_map)\n",
    "\n",
    "    # Display the original image, segmentation map, and legend\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.imshow(image)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(\"Segmentation Map\")\n",
    "    plt.imshow(segmentation_image)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # Create a legend plot\n",
    "    for idx, (class_id, class_name) in enumerate(class_names.items()):\n",
    "        plt.fill_between([0, 1], idx + 0.5, idx + 1.5, color=np.array(class_colors[class_id]) / 255.0, label=class_name)\n",
    "\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), title=\"Classes\", title_fontsize='13')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoImageProcessor, SegformerForSemanticSegmentation\n",
    "from PIL import Image\n",
    "\n",
    "# Load the image processor and model\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"nvidia/segformer-b0-finetuned-ade-512-512\")\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/segformer-b0-finetuned-ade-512-512\")\n",
    "\n",
    "image_path_stem = \"/cluster/home/wueestm/f3loc/metric3d/data/hge_customized_complete/non-aligned/rgb/\"\n",
    "image_path = image_path_stem + \"00120-0.jpg\"\n",
    "\n",
    "\n",
    "#image_path = \"/cluster/home/wueestm/f3loc/metric3d/data/hge_customized_complete/non-aligned/rgb/00120-0.jpg\"\n",
    "image = Image.open(image_path)\n",
    "image_rgb = image.convert('RGB')\n",
    "\n",
    "\n",
    "# Preprocess the image and forward it through the model\n",
    "inputs = image_processor(images=image_rgb, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits  # shape (batch_size, num_labels, height/4, width/4)\n",
    "\n",
    "# Get the predicted class for each pixel\n",
    "predicted_class = logits.argmax(dim=1).squeeze().cpu().numpy()\n",
    "\n",
    "# Resize the predicted class indices to match the original image size\n",
    "predicted_class_resized = Image.fromarray(predicted_class.astype(np.uint8)).resize(image.size, resample=Image.NEAREST)\n",
    "predicted_class_resized = np.array(predicted_class_resized)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_rgb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_np = np.array(image_rgb)\n",
    "image_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resized logits shape: (150, 1920, 1440)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoImageProcessor, SegformerForSemanticSegmentation\n",
    "from PIL import Image\n",
    "\n",
    "# Load the image processor and model\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"nvidia/segformer-b0-finetuned-ade-512-512\")\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/segformer-b0-finetuned-ade-512-512\")\n",
    "\n",
    "image_path_stem = \"/cluster/home/wueestm/f3loc/metric3d/data/hge_customized_complete/non-aligned/rgb/\"\n",
    "image_path = image_path_stem + \"00120-0.jpg\"\n",
    "\n",
    "image = Image.open(image_path)\n",
    "image_rgb = image.convert('RGB')\n",
    "rgb_origin = np.array(image_rgb)\n",
    "\n",
    "\n",
    "# Preprocess the image and forward it through the model\n",
    "inputs = image_processor(images=rgb_origin, return_tensors=\"pt\")\n",
    "#inputs = image_processor(images=image, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits  # shape (batch_size, num_labels, height/4, width/4)\n",
    "\n",
    "# Get the logits for the first image in the batch\n",
    "logits = logits.squeeze(0)  # Remove batch dimension, shape (num_labels, height/4, width/4)\n",
    "\n",
    "# Get the dimensions of the original image\n",
    "original_width, original_height = image.size\n",
    "\n",
    "# Resize the logits to the original image size\n",
    "resize_factor = (original_height // logits.shape[1], original_width // logits.shape[2])\n",
    "logits_resized = torch.nn.functional.interpolate(\n",
    "    logits.unsqueeze(0),  # Add batch dimension back for interpolation\n",
    "    size=(original_height, original_width),\n",
    "    mode='bilinear',\n",
    "    align_corners=False\n",
    ").squeeze(0)  # Remove batch dimension\n",
    "\n",
    "# Convert the resized logits to a NumPy array\n",
    "logits_np = logits_resized.detach().cpu().numpy()  # shape (num_labels, original_height, original_width)\n",
    "\n",
    "print(\"Resized logits shape:\", logits_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_origin.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_origin.shape[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compute the predicted class for each pixel\n",
    "# predicted_class = logits_np.argmax(axis=0)  # shape (original_height, original_width)\n",
    "# \n",
    "# # Define a color map for visualization\n",
    "# def get_color_map(num_classes):\n",
    "#     cmap = plt.get_cmap('tab20', num_classes)\n",
    "#     return (cmap(np.arange(num_classes)) * 255).astype(np.uint8)\n",
    "# \n",
    "# # Get a color map for the classes\n",
    "# num_classes = logits_np.shape[0]\n",
    "# color_map = get_color_map(num_classes)\n",
    "# \n",
    "# # Create a blank segmentation map (all black)\n",
    "# segmentation_map = np.zeros((*predicted_class.shape, 3), dtype=np.uint8)\n",
    "# \n",
    "# # Fill in the segmentation map with colors only for the specified classes\n",
    "# for class_id in range(num_classes):\n",
    "#     mask = (predicted_class == class_id)\n",
    "#     segmentation_map[mask] = color_map[class_id][:3]  # Ensure we use only the RGB channels\n",
    "# \n",
    "# # Convert to image format\n",
    "# segmentation_image = Image.fromarray(segmentation_map)\n",
    "# \n",
    "# # Display the original image and segmentation map\n",
    "# plt.figure(figsize=(12, 8))\n",
    "# \n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.title(\"Original Image\")\n",
    "# plt.imshow(image)\n",
    "# plt.axis(\"off\")\n",
    "# \n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.title(\"Segmentation Map\")\n",
    "# plt.imshow(segmentation_image)\n",
    "# plt.axis(\"off\")\n",
    "# \n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ref_euler_angles = [-2.05386654, 0.05212421, -1.59732346]\n",
    "ref_euler_angles = [-1.78765814, -0.00908211, 1.51359217]\n",
    "K = np.array([[1596, 0, 960], [0, 1596, 720], [0, 0, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 1920, 1440)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r=ref_euler_angles[1]\n",
    "p=-(ref_euler_angles[0]+np.pi/2)\n",
    "aligned_logits = gravity_align_segmentation(logits_np, r, p, K)\n",
    "aligned_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.ones(list(aligned_logits.shape[1:3]))\n",
    "#mask = gravity_align(mask, r, p, visualize=False, mode=1)\n",
    "mask = gravity_align(mask, r, p, mode=1, K=K)\n",
    "mask[mask < 1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned_logits[:, mask == 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(logits):\n",
    "    \"\"\"\n",
    "    Apply softmax to the logits to get probabilities.\n",
    "    \n",
    "    Input:\n",
    "        logits: input segmentation map of shape (N, H, W), where N is the number of channels.\n",
    "    \n",
    "    Output:\n",
    "        probabilities: output probabilities of shape (N, H, W), where N is the number of channels.\n",
    "    \"\"\"\n",
    "    # Ensure logits are of type float32 for numerical stability\n",
    "    logits = logits.astype(np.float32)\n",
    "    \n",
    "    # Shift logits for numerical stability\n",
    "    logits_max = np.max(logits, axis=0, keepdims=True)\n",
    "    exp_logits = np.exp(logits - logits_max)\n",
    "    sum_exp_logits = np.sum(exp_logits, axis=0, keepdims=True)\n",
    "    probabilities = exp_logits / sum_exp_logits\n",
    "    \n",
    "    return probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "def softmax_pytorch(logits, mask=None, device=None):\n",
    "    \"\"\"\n",
    "    Apply softmax to the logits to get probabilities using PyTorch.\n",
    "    \n",
    "    Input:\n",
    "        logits: input segmentation map of shape (N, H, W), where N is the number of channels.\n",
    "        mask: optional mask indicating which pixels should be set to 0 in the output probabilities.\n",
    "        device: the device to perform the computation on ('cpu' or 'cuda').\n",
    "    \n",
    "    Output:\n",
    "        probabilities: output probabilities of shape (N, H, W), where N is the number of channels.\n",
    "    \"\"\"\n",
    "\n",
    "    # Choose device: 'cuda' if available, else 'cpu'\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    \n",
    "    # Convert logits to PyTorch tensor and move to device\n",
    "    logits = torch.tensor(logits, dtype=torch.float32).to(device)\n",
    "    \n",
    "    # Apply softmax along the channel dimension (dim=0)\n",
    "    probabilities = F.softmax(logits, dim=0)\n",
    "    \n",
    "    # Apply mask if provided\n",
    "    if mask is not None:\n",
    "        mask = torch.tensor(mask, dtype=torch.float32).to(device)\n",
    "        probabilities[:, mask == 0] = 0\n",
    "    \n",
    "    return probabilities.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute probabilities\n",
    "probabilities = softmax(aligned_logits)\n",
    "probabilities[:, mask == 0] = 0\n",
    "print(probabilities.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute probabilities\n",
    "probabilities = softmax_pytorch(aligned_logits)\n",
    "probabilities[:, mask == 0] = 0\n",
    "print(probabilities.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the predicted class for each pixel\n",
    "predicted_class = probabilities.argmax(axis=0)  # shape (original_height, original_width)\n",
    "\n",
    "# Define a color map for visualization\n",
    "def get_color_map(num_classes):\n",
    "    cmap = plt.get_cmap('tab20', num_classes)\n",
    "    return (cmap(np.arange(num_classes)) * 255).astype(np.uint8)\n",
    "\n",
    "# Get a color map for the classes\n",
    "num_classes = logits_np.shape[0]\n",
    "color_map = get_color_map(num_classes)\n",
    "\n",
    "# Create a blank segmentation map (all black)\n",
    "segmentation_map = np.zeros((*predicted_class.shape, 3), dtype=np.uint8)\n",
    "\n",
    "# Fill in the segmentation map with colors only for the specified classes\n",
    "for class_id in range(num_classes):\n",
    "    mask_class = (predicted_class == class_id)\n",
    "    segmentation_map[mask_class] = color_map[class_id][:3]  # Ensure we use only the RGB channels\n",
    "\n",
    "segmentation_map[mask == 0, :] = 0\n",
    "\n",
    "# Convert to image format\n",
    "segmentation_image = Image.fromarray(segmentation_map)\n",
    "\n",
    "# Display the original image and segmentation map\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Original Image\")\n",
    "plt.imshow(image)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Segmentation Map\")\n",
    "plt.imshow(segmentation_image)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the predicted class for each pixel\n",
    "predicted_class = aligned_seg_map.argmax(axis=0)  # shape (original_height, original_width)\n",
    "\n",
    "# Define a color map for visualization\n",
    "def get_color_map(num_classes):\n",
    "    cmap = plt.get_cmap('tab20', num_classes)\n",
    "    return (cmap(np.arange(num_classes)) * 255).astype(np.uint8)\n",
    "\n",
    "# Get a color map for the classes\n",
    "num_classes = logits_np.shape[0]\n",
    "color_map = get_color_map(num_classes)\n",
    "\n",
    "# Create a blank segmentation map (all black)\n",
    "segmentation_map = np.zeros((*predicted_class.shape, 3), dtype=np.uint8)\n",
    "\n",
    "# Fill in the segmentation map with colors only for the specified classes\n",
    "for class_id in range(num_classes):\n",
    "    mask = (predicted_class == class_id)\n",
    "    segmentation_map[mask] = color_map[class_id][:3]  # Ensure we use only the RGB channels\n",
    "\n",
    "# Convert to image format\n",
    "segmentation_image = Image.fromarray(segmentation_map)\n",
    "\n",
    "# Display the original image and segmentation map\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Original Image\")\n",
    "plt.imshow(image)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Segmentation Map\")\n",
    "plt.imshow(segmentation_image)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example segmentation map (e.g., 150 channels, H=150, W=200)\n",
    "    seg_map = np.random.randint(0, 10, (150, 150, 200), dtype=np.uint8)\n",
    "    \n",
    "    # Example roll and pitch\n",
    "    roll = 0.1  # in radians\n",
    "    pitch = 0.2  # in radians\n",
    "    \n",
    "    # Align the segmentation map\n",
    "    aligned_seg_map = gravity_align_segmentation(seg_map, roll, pitch)\n",
    "    \n",
    "    # aligned_seg_map now contains the segmentation map aligned to gravity\n",
    "\n",
    "roll = ref_euler_angles[0]\n",
    "pitch = ref_euler_angles[1]\n",
    "ref_img = gravity_align(ref_img, r=pitch, p=-(roll+np.pi/2),  mode=1, K=self.K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoImageProcessor, SegformerForSemanticSegmentation\n",
    "from PIL import Image\n",
    "\n",
    "# Load the image processor and model\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"nvidia/segformer-b0-finetuned-ade-512-512\")\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/segformer-b0-finetuned-ade-512-512\")\n",
    "\n",
    "# Path to the image\n",
    "image_path_stem = \"/cluster/home/wueestm/f3loc/metric3d/data/hge_customized_complete/non-aligned/rgb/\"\n",
    "image_path = image_path_stem + \"00120-0.jpg\"\n",
    "\n",
    "# Load and preprocess the image\n",
    "image = Image.open(image_path)\n",
    "inputs = image_processor(images=image, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits  # shape (batch_size, num_labels, height/4, width/4)\n",
    "\n",
    "# Get the logits for the first image in the batch\n",
    "logits = logits.squeeze(0)  # Remove batch dimension, shape (num_labels, height/4, width/4)\n",
    "\n",
    "# Get the dimensions of the original image\n",
    "original_width, original_height = image.size\n",
    "\n",
    "# Resize the logits to the original image size\n",
    "resize_factor = (original_height // logits.shape[1], original_width // logits.shape[2])\n",
    "logits_resized = torch.nn.functional.interpolate(\n",
    "    logits.unsqueeze(0),  # Add batch dimension back for interpolation\n",
    "    size=(original_height, original_width),\n",
    "    mode='bilinear',\n",
    "    align_corners=False\n",
    ").squeeze(0)  # Remove batch dimension\n",
    "\n",
    "# Convert the resized logits to a NumPy array\n",
    "logits_np = logits_resized.detach().cpu().numpy()  # shape (num_labels, original_height, original_width)\n",
    "\n",
    "# Compute the predicted class for each pixel\n",
    "predicted_class = logits_np.argmax(axis=0)  # shape (original_height, original_width)\n",
    "\n",
    "# Manually define the ADE20K label mapping\n",
    "ade20k_labels = {\n",
    "    \"0\": \"wall\", \"1\": \"building\", \"2\": \"sky\", \"3\": \"floor\", \"4\": \"tree\", \n",
    "    \"5\": \"ceiling\", \"6\": \"road\", \"7\": \"bed\", \"8\": \"windowpane\", \"9\": \"grass\", \n",
    "    \"10\": \"cabinet\", \"11\": \"sidewalk\", \"12\": \"person\", \"13\": \"earth\", \"14\": \"door\", \n",
    "    \"15\": \"table\", \"16\": \"mountain\", \"17\": \"plant\", \"18\": \"curtain\", \"19\": \"chair\", \n",
    "    \"20\": \"car\", \"21\": \"water\", \"22\": \"painting\", \"23\": \"sofa\", \"24\": \"shelf\", \n",
    "    \"25\": \"house\", \"26\": \"sea\", \"27\": \"mirror\", \"28\": \"rug\", \"29\": \"field\", \n",
    "    \"30\": \"armchair\", \"31\": \"seat\", \"32\": \"fence\", \"33\": \"desk\", \"34\": \"rock\", \n",
    "    \"35\": \"wardrobe\", \"36\": \"lamp\", \"37\": \"bathtub\", \"38\": \"railing\", \"39\": \"cushion\", \n",
    "    \"40\": \"base\", \"41\": \"box\", \"42\": \"column\", \"43\": \"signboard\", \"44\": \"chest of drawers\", \n",
    "    \"45\": \"counter\", \"46\": \"sand\", \"47\": \"sink\", \"48\": \"skyscraper\", \"49\": \"fireplace\", \n",
    "    \"50\": \"refrigerator\", \"51\": \"grandstand\", \"52\": \"path\", \"53\": \"stairs\", \"54\": \"runway\", \n",
    "    \"55\": \"case\", \"56\": \"pool table\", \"57\": \"pillow\", \"58\": \"screen door\", \"59\": \"stairway\", \n",
    "    \"60\": \"river\", \"61\": \"bridge\", \"62\": \"bookcase\", \"63\": \"blind\", \"64\": \"coffee table\", \n",
    "    \"65\": \"toilet\", \"66\": \"flower\", \"67\": \"book\", \"68\": \"hill\", \"69\": \"bench\", \n",
    "    \"70\": \"countertop\", \"71\": \"stove\", \"72\": \"palm\", \"73\": \"kitchen island\", \"74\": \"computer\", \n",
    "    \"75\": \"swivel chair\", \"76\": \"boat\", \"77\": \"bar\", \"78\": \"arcade machine\", \"79\": \"hovel\", \n",
    "    \"80\": \"bus\", \"81\": \"towel\", \"82\": \"light\", \"83\": \"truck\", \"84\": \"tower\", \n",
    "    \"85\": \"chandelier\", \"86\": \"awning\", \"87\": \"streetlight\", \"88\": \"booth\", \"89\": \"television receiver\", \n",
    "    \"90\": \"airplane\", \"91\": \"dirt track\", \"92\": \"apparel\", \"93\": \"pole\", \"94\": \"land\", \n",
    "    \"95\": \"bannister\", \"96\": \"escalator\", \"97\": \"ottoman\", \"98\": \"bottle\", \"99\": \"buffet\", \n",
    "    \"100\": \"poster\", \"101\": \"stage\", \"102\": \"van\", \"103\": \"ship\", \"104\": \"fountain\", \n",
    "    \"105\": \"conveyer belt\", \"106\": \"canopy\", \"107\": \"washer\", \"108\": \"plaything\", \"109\": \"swimming pool\", \n",
    "    \"110\": \"stool\", \"111\": \"barrel\", \"112\": \"basket\", \"113\": \"waterfall\", \"114\": \"tent\", \n",
    "    \"115\": \"bag\", \"116\": \"minibike\", \"117\": \"cradle\", \"118\": \"oven\", \"119\": \"ball\", \n",
    "    \"120\": \"food\", \"121\": \"step\", \"122\": \"tank\", \"123\": \"trade name\", \"124\": \"microwave\", \n",
    "    \"125\": \"pot\", \"126\": \"animal\", \"127\": \"bicycle\", \"128\": \"lake\", \"129\": \"dishwasher\", \n",
    "    \"130\": \"screen\", \"131\": \"blanket\", \"132\": \"sculpture\", \"133\": \"hood\", \"134\": \"sconce\", \n",
    "    \"135\": \"vase\", \"136\": \"traffic light\", \"137\": \"tray\", \"138\": \"ashcan\", \"139\": \"fan\", \n",
    "    \"140\": \"pier\", \"141\": \"crt screen\", \"142\": \"plate\", \"143\": \"monitor\", \"144\": \"bulletin board\", \n",
    "    \"145\": \"shower\", \"146\": \"radiator\", \"147\": \"glass\", \"148\": \"clock\", \"149\": \"flag\"\n",
    "}\n",
    "\n",
    "# Classes to keep and display in color\n",
    "classes_to_display = [0, 1, 3, 5, 8, 12, 14, 15, 19, 42, 53, 59, 97, 104, 132, 147]\n",
    "class_names = {i: ade20k_labels[str(i)] for i in classes_to_display}\n",
    "\n",
    "# Manually specify distinct colors for each class\n",
    "class_colors = {\n",
    "    0: [255, 0, 0],        # Red for 'wall'\n",
    "    1: [0, 255, 0],        # Green for 'building'\n",
    "    3: [0, 0, 255],        # Blue for 'floor'\n",
    "    5: [255, 255, 0],      # Yellow for 'ceiling'\n",
    "    8: [0, 255, 255],      # Cyan for 'windowpane'\n",
    "    12: [255, 0, 255],     # Magenta for 'person'\n",
    "    14: [128, 0, 128],     # Purple for 'door'\n",
    "    15: [128, 128, 0],     # Olive for 'table'\n",
    "    19: [0, 128, 128],     # Teal for 'chair'\n",
    "    42: [90, 90, 90],   # Dark gray for 'column'\n",
    "    53: [192, 192, 192],   # Light gray for 'stairs'\n",
    "    59: [255, 165, 0],     # Orange for 'stairway'\n",
    "    97: [255, 105, 180],   # Pink for 'ottoman'\n",
    "    104: [255, 20, 147],   # Deep pink for 'fountain'\n",
    "    132: [255, 69, 0],     # Red orange for 'sculpture'\n",
    "    147: [139, 69, 19]     # Saddle brown for 'glass'\n",
    "}\n",
    "\n",
    "# Create a blank segmentation map (all black)\n",
    "segmentation_map = np.zeros((*predicted_class.shape, 3), dtype=np.uint8)\n",
    "\n",
    "# Fill in the segmentation map with colors only for the specified classes\n",
    "for class_id, color in class_colors.items():\n",
    "    mask = (predicted_class == class_id)\n",
    "    segmentation_map[mask] = color\n",
    "\n",
    "# Convert to image format\n",
    "segmentation_image = Image.fromarray(segmentation_map)\n",
    "\n",
    "# Display the original image and segmentation map\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Original Image\")\n",
    "plt.imshow(image)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Segmentation Map\")\n",
    "plt.imshow(segmentation_image)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Create a legend plot\n",
    "legend_elements = [plt.Line2D([0], [0], color=np.array(color) / 255.0, lw=4) for color in class_colors.values()]\n",
    "plt.legend(legend_elements, [class_names[class_id] for class_id in class_colors.keys()], loc='center left', bbox_to_anchor=(1, 0.5), title=\"Classes\", title_fontsize='13')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class_resized = Image.fromarray(logits.astype(np.uint8)).resize(image.size, resample=Image.NEAREST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoImageProcessor, SegformerForSemanticSegmentation\n",
    "from PIL import Image\n",
    "\n",
    "# Load the image processor and model\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"nvidia/segformer-b0-finetuned-ade-512-512\")\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/segformer-b0-finetuned-ade-512-512\")\n",
    "\n",
    "# Define the path to the images\n",
    "image_path_stem = \"/cluster/home/wueestm/f3loc/metric3d/data/hge_customized_complete/non-aligned/rgb/\"\n",
    "image_path_ls = [image_path_stem + \"00120-0.jpg\"]\n",
    "\n",
    "# Load and process each image\n",
    "for image_path in image_path_ls:\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    # Preprocess the image and forward it through the model\n",
    "    inputs = image_processor(images=image, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits  # shape (batch_size, num_labels, height/4, width/4)\n",
    "\n",
    "    # Get the predicted class for each pixel\n",
    "    predicted_class = logits.argmax(dim=1).squeeze().cpu().numpy()\n",
    "\n",
    "    # Resize the predicted class indices to match the original image size\n",
    "    predicted_class_resized = Image.fromarray(predicted_class.astype(np.uint8)).resize(image.size, resample=Image.NEAREST)\n",
    "    predicted_class_resized = np.array(predicted_class_resized)\n",
    "\n",
    "    # Manually define the ADE20K label mapping\n",
    "    ade20k_labels = {\n",
    "        \"0\": \"wall\", \"1\": \"building\", \"2\": \"sky\", \"3\": \"floor\", \"4\": \"tree\", \n",
    "        \"5\": \"ceiling\", \"6\": \"road\", \"7\": \"bed\", \"8\": \"windowpane\", \"9\": \"grass\", \n",
    "        \"10\": \"cabinet\", \"11\": \"sidewalk\", \"12\": \"person\", \"13\": \"earth\", \"14\": \"door\", \n",
    "        \"15\": \"table\", \"16\": \"mountain\", \"17\": \"plant\", \"18\": \"curtain\", \"19\": \"chair\", \n",
    "        \"20\": \"car\", \"21\": \"water\", \"22\": \"painting\", \"23\": \"sofa\", \"24\": \"shelf\", \n",
    "        \"25\": \"house\", \"26\": \"sea\", \"27\": \"mirror\", \"28\": \"rug\", \"29\": \"field\", \n",
    "        \"30\": \"armchair\", \"31\": \"seat\", \"32\": \"fence\", \"33\": \"desk\", \"34\": \"rock\", \n",
    "        \"35\": \"wardrobe\", \"36\": \"lamp\", \"37\": \"bathtub\", \"38\": \"railing\", \"39\": \"cushion\", \n",
    "        \"40\": \"base\", \"41\": \"box\", \"42\": \"column\", \"43\": \"signboard\", \"44\": \"chest of drawers\", \n",
    "        \"45\": \"counter\", \"46\": \"sand\", \"47\": \"sink\", \"48\": \"skyscraper\", \"49\": \"fireplace\", \n",
    "        \"50\": \"refrigerator\", \"51\": \"grandstand\", \"52\": \"path\", \"53\": \"stairs\", \"54\": \"runway\", \n",
    "        \"55\": \"case\", \"56\": \"pool table\", \"57\": \"pillow\", \"58\": \"screen door\", \"59\": \"stairway\", \n",
    "        \"60\": \"river\", \"61\": \"bridge\", \"62\": \"bookcase\", \"63\": \"blind\", \"64\": \"coffee table\", \n",
    "        \"65\": \"toilet\", \"66\": \"flower\", \"67\": \"book\", \"68\": \"hill\", \"69\": \"bench\", \n",
    "        \"70\": \"countertop\", \"71\": \"stove\", \"72\": \"palm\", \"73\": \"kitchen island\", \"74\": \"computer\", \n",
    "        \"75\": \"swivel chair\", \"76\": \"boat\", \"77\": \"bar\", \"78\": \"arcade machine\", \"79\": \"hovel\", \n",
    "        \"80\": \"bus\", \"81\": \"towel\", \"82\": \"light\", \"83\": \"truck\", \"84\": \"tower\", \n",
    "        \"85\": \"chandelier\", \"86\": \"awning\", \"87\": \"streetlight\", \"88\": \"booth\", \"89\": \"television receiver\", \n",
    "        \"90\": \"airplane\", \"91\": \"dirt track\", \"92\": \"apparel\", \"93\": \"pole\", \"94\": \"land\", \n",
    "        \"95\": \"bannister\", \"96\": \"escalator\", \"97\": \"ottoman\", \"98\": \"bottle\", \"99\": \"buffet\", \n",
    "        \"100\": \"poster\", \"101\": \"stage\", \"102\": \"van\", \"103\": \"ship\", \"104\": \"fountain\", \n",
    "        \"105\": \"conveyer belt\", \"106\": \"canopy\", \"107\": \"washer\", \"108\": \"plaything\", \"109\": \"swimming pool\", \n",
    "        \"110\": \"stool\", \"111\": \"barrel\", \"112\": \"basket\", \"113\": \"waterfall\", \"114\": \"tent\", \n",
    "        \"115\": \"bag\", \"116\": \"minibike\", \"117\": \"cradle\", \"118\": \"oven\", \"119\": \"ball\", \n",
    "        \"120\": \"food\", \"121\": \"step\", \"122\": \"tank\", \"123\": \"trade name\", \"124\": \"microwave\", \n",
    "        \"125\": \"pot\", \"126\": \"animal\", \"127\": \"bicycle\", \"128\": \"lake\", \"129\": \"dishwasher\", \n",
    "        \"130\": \"screen\", \"131\": \"blanket\", \"132\": \"sculpture\", \"133\": \"hood\", \"134\": \"sconce\", \n",
    "        \"135\": \"vase\", \"136\": \"traffic light\", \"137\": \"tray\", \"138\": \"ashcan\", \"139\": \"fan\", \n",
    "        \"140\": \"pier\", \"141\": \"crt screen\", \"142\": \"plate\", \"143\": \"monitor\", \"144\": \"bulletin board\", \n",
    "        \"145\": \"shower\", \"146\": \"radiator\", \"147\": \"glass\", \"148\": \"clock\", \"149\": \"flag\"\n",
    "    }\n",
    "\n",
    "    # Classes to keep and display in color\n",
    "    classes_to_display = [0, 1, 3, 5, 8, 12, 14, 15, 19, 42, 53, 59, 97, 104, 132, 147]\n",
    "    class_names = {i: ade20k_labels[str(i)] for i in classes_to_display}\n",
    "\n",
    "    # Manually specify distinct colors for each class\n",
    "    class_colors = {\n",
    "        0: [255, 0, 0],        # Red for 'wall'\n",
    "        1: [0, 255, 0],        # Green for 'building'\n",
    "        3: [0, 0, 255],        # Blue for 'floor'\n",
    "        5: [255, 255, 0],      # Yellow for 'ceiling'\n",
    "        8: [0, 255, 255],      # Cyan for 'windowpane'\n",
    "        12: [255, 0, 255],     # Magenta for 'person'\n",
    "        14: [128, 0, 128],     # Purple for 'door'\n",
    "        15: [128, 128, 0],     # Olive for 'table'\n",
    "        19: [0, 128, 128],     # Teal for 'chair'\n",
    "        42: [90, 90, 90],   # Dark gray for 'column'\n",
    "        53: [192, 192, 192],   # Light gray for 'stairs'\n",
    "        59: [255, 165, 0],     # Orange for 'stairway'\n",
    "        97: [255, 105, 180],   # Pink for 'ottoman'\n",
    "        104: [255, 20, 147],   # Deep pink for 'fountain'\n",
    "        132: [255, 69, 0],     # Red orange for 'sculpture'\n",
    "        147: [139, 69, 19]     # Saddle brown for 'glass'\n",
    "    }\n",
    "\n",
    "    # Create a blank segmentation map (all black)\n",
    "    segmentation_map = np.zeros((*predicted_class_resized.shape, 3), dtype=np.uint8)\n",
    "\n",
    "    # Fill in the segmentation map with colors only for the specified classes\n",
    "    for class_id, color in class_colors.items():\n",
    "        mask = predicted_class_resized == class_id\n",
    "        segmentation_map[mask] = color\n",
    "\n",
    "    # Convert to image format\n",
    "    segmentation_image = Image.fromarray(segmentation_map)\n",
    "\n",
    "    # Display the original image, segmentation map, and legend\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.imshow(image)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(\"Segmentation Map\")\n",
    "    plt.imshow(segmentation_image)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # Create a legend plot\n",
    "    for idx, (class_id, class_name) in enumerate(class_names.items()):\n",
    "        plt.fill_between([0, 1], idx + 0.5, idx + 1.5, color=np.array(class_colors[class_id]) / 255.0, label=class_name)\n",
    "\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), title=\"Classes\", title_fontsize='13')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the array to count occurrences of each class\n",
    "flat_predicted_class = predicted_class.flatten()\n",
    "\n",
    "# Count the occurrences of each class\n",
    "class_counts = Counter(flat_predicted_class)\n",
    "\n",
    "# Get the top 5 most common classes\n",
    "top5_classes = class_counts.most_common(5)\n",
    "\n",
    "# Print the top 5 most common classes with their names and counts\n",
    "print(\"Top 5 most common classes and their counts:\")\n",
    "for class_idx, count in top5_classes:\n",
    "    class_name = ade20k_labels.get(str(class_idx), \"Unknown\")\n",
    "    print(f\"Class '{class_name}' (Index {class_idx}): {count} occurrences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0\n",
    "1\n",
    "3\n",
    "5\n",
    "8\n",
    "12\n",
    "14\n",
    "15\n",
    "19\n",
    "53\n",
    "59\n",
    "97\n",
    "104\n",
    "132\n",
    "147"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Manually define the ADE20K label mapping\n",
    "ade20k_labels = {\"0\": \"wall\", \"1\": \"building\", \"2\": \"sky\", \"3\": \"floor\", \"4\": \"tree\", \n",
    "    \"5\": \"ceiling\", \"6\": \"road\", \"7\": \"bed \", \"8\": \"windowpane\", \"9\": \"grass\", \n",
    "    \"10\": \"cabinet\", \"11\": \"sidewalk\", \"12\": \"person\", \"13\": \"earth\", \"14\": \"door\", \n",
    "    \"15\": \"table\", \"16\": \"mountain\", \"17\": \"plant\", \"18\": \"curtain\", \"19\": \"chair\", \n",
    "    \"20\": \"car\", \"21\": \"water\", \"22\": \"painting\", \"23\": \"sofa\", \"24\": \"shelf\", \n",
    "    \"25\": \"house\", \"26\": \"sea\", \"27\": \"mirror\", \"28\": \"rug\", \"29\": \"field\", \n",
    "    \"30\": \"armchair\", \"31\": \"seat\", \"32\": \"fence\", \"33\": \"desk\", \"34\": \"rock\", \n",
    "    \"35\": \"wardrobe\", \"36\": \"lamp\", \"37\": \"bathtub\", \"38\": \"railing\", \"39\": \"cushion\", \n",
    "    \"40\": \"base\", \"41\": \"box\", \"42\": \"column\", \"43\": \"signboard\", \"44\": \"chest of drawers\", \n",
    "    \"45\": \"counter\", \"46\": \"sand\", \"47\": \"sink\", \"48\": \"skyscraper\", \"49\": \"fireplace\", \n",
    "    \"50\": \"refrigerator\", \"51\": \"grandstand\", \"52\": \"path\", \"53\": \"stairs\", \"54\": \"runway\", \n",
    "    \"55\": \"case\", \"56\": \"pool table\", \"57\": \"pillow\", \"58\": \"screen door\", \"59\": \"stairway\", \n",
    "    \"60\": \"river\", \"61\": \"bridge\", \"62\": \"bookcase\", \"63\": \"blind\", \"64\": \"coffee table\", \n",
    "    \"65\": \"toilet\", \"66\": \"flower\", \"67\": \"book\", \"68\": \"hill\", \"69\": \"bench\", \n",
    "    \"70\": \"countertop\", \"71\": \"stove\", \"72\": \"palm\", \"73\": \"kitchen island\", \"74\": \"computer\", \n",
    "    \"75\": \"swivel chair\", \"76\": \"boat\", \"77\": \"bar\", \"78\": \"arcade machine\", \"79\": \"hovel\", \n",
    "    \"80\": \"bus\", \"81\": \"towel\", \"82\": \"light\", \"83\": \"truck\", \"84\": \"tower\", \n",
    "    \"85\": \"chandelier\", \"86\": \"awning\", \"87\": \"streetlight\", \"88\": \"booth\", \"89\": \"television receiver\", \n",
    "    \"90\": \"airplane\", \"91\": \"dirt track\", \"92\": \"apparel\", \"93\": \"pole\", \"94\": \"land\", \n",
    "    \"95\": \"bannister\", \"96\": \"escalator\", \"97\": \"ottoman\", \"98\": \"bottle\", \"99\": \"buffet\", \n",
    "    \"100\": \"poster\", \"101\": \"stage\", \"102\": \"van\", \"103\": \"ship\", \"104\": \"fountain\", \n",
    "    \"105\": \"conveyer belt\", \"106\": \"canopy\", \"107\": \"washer\", \"108\": \"plaything\", \"109\": \"swimming pool\", \n",
    "    \"110\": \"stool\", \"111\": \"barrel\", \"112\": \"basket\", \"113\": \"waterfall\", \"114\": \"tent\", \n",
    "    \"115\": \"bag\", \"116\": \"minibike\", \"117\": \"cradle\", \"118\": \"oven\", \"119\": \"ball\", \n",
    "    \"120\": \"food\", \"121\": \"step\", \"122\": \"tank\", \"123\": \"trade name\", \"124\": \"microwave\", \n",
    "    \"125\": \"pot\", \"126\": \"animal\", \"127\": \"bicycle\", \"128\": \"lake\", \"129\": \"dishwasher\", \n",
    "    \"130\": \"screen\", \"131\": \"blanket\", \"132\": \"sculpture\", \"133\": \"hood\", \"134\": \"sconce\", \n",
    "    \"135\": \"vase\", \"136\": \"traffic light\", \"137\": \"tray\", \"138\": \"ashcan\", \"139\": \"fan\", \n",
    "    \"140\": \"pier\", \"141\": \"crt screen\", \"142\": \"plate\", \"143\": \"monitor\", \"144\": \"bulletin board\", \n",
    "    \"145\": \"shower\", \"146\": \"radiator\", \"147\": \"glass\", \"148\": \"clock\", \"149\": \"flag\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the array to count occurrences of each class\n",
    "flat_predicted_class = predicted_class.flatten()\n",
    "\n",
    "# Count the occurrences of each class\n",
    "class_counts = Counter(flat_predicted_class)\n",
    "\n",
    "# Get the top 5 most common classes\n",
    "top5_classes = class_counts.most_common(5)\n",
    "\n",
    "# Print the top 5 most common classes with their names and counts\n",
    "print(\"Top 5 most common classes and their counts:\")\n",
    "for class_idx, count in top5_classes:\n",
    "    class_name = ade20k_labels.get(str(class_idx), \"Unknown\")\n",
    "    print(f\"Class '{class_name}' (Index {class_idx}): {count} occurrences\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top5_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HuggingFace Tutorial: Image Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from PIL import Image\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image_path = \"/cluster/home/wueestm/f3loc/metric3d/data/hge_customized_complete/non-aligned/rgb/00000-0.jpg\"\n",
    "#image = Image.open(image_path)\n",
    "image = Image.open(\"segmentation_input.jpg\")\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/cluster/home/wueestm/.cache/huggingface/hub/models--nvidia--segformer-b1-finetuned-cityscapes-1024-1024/snapshots/ec86afeba68e656629ccf47e0c8d2902f964917b\"\n",
    "semantic_segmentation = pipeline(\"image-segmentation\", model=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = semantic_segmentation(image)\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[-1][\"mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Measure inference time\n",
    "start_time = time.time()\n",
    "results = semantic_segmentation(image)\n",
    "end_time = time.time()\n",
    "\n",
    "inference_time = end_time - start_time\n",
    "print(f\"Inference time: {inference_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OneFormer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import OneFormerProcessor, OneFormerForUniversalSegmentation\n",
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "\n",
    "# load OneFormer fine-tuned on ADE20k for universal segmentation\n",
    "processor = OneFormerProcessor.from_pretrained(\"shi-labs/oneformer_ade20k_swin_tiny\")\n",
    "model = OneFormerForUniversalSegmentation.from_pretrained(\"shi-labs/oneformer_ade20k_swin_tiny\")\n",
    "\n",
    "url = (\n",
    "    \"https://huggingface.co/datasets/hf-internal-testing/fixtures_ade20k/resolve/main/ADE_val_00000001.jpg\"\n",
    ")\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semantic Segmentation\n",
    "inputs = processor(image, [\"semantic\"], return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model predicts class_queries_logits of shape `(batch_size, num_queries)`\n",
    "# and masks_queries_logits of shape `(batch_size, num_queries, height, width)`\n",
    "class_queries_logits = outputs.class_queries_logits\n",
    "masks_queries_logits = outputs.masks_queries_logits\n",
    "\n",
    "# you can pass them to processor for semantic postprocessing\n",
    "predicted_semantic_map = processor.post_process_semantic_segmentation(\n",
    "    outputs, target_sizes=[image.size[::-1]]\n",
    ")[0]\n",
    "f\" Semantic Predictions Shape: {list(predicted_semantic_map.shape)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_semantic_map.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_semantic_map.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instance Segmentation\n",
    "inputs = processor(image, [\"instance\"], return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "# model predicts class_queries_logits of shape `(batch_size, num_queries)`\n",
    "# and masks_queries_logits of shape `(batch_size, num_queries, height, width)`\n",
    "class_queries_logits = outputs.class_queries_logits\n",
    "masks_queries_logits = outputs.masks_queries_logits\n",
    "\n",
    "# you can pass them to processor for instance postprocessing\n",
    "predicted_instance_map = processor.post_process_instance_segmentation(\n",
    "    outputs, target_sizes=[image.size[::-1]]\n",
    ")[0][\"segmentation\"]\n",
    "f\" Instance Predictions Shape: {list(predicted_instance_map.shape)}\"\n",
    "\n",
    "# Panoptic Segmentation\n",
    "inputs = processor(image, [\"panoptic\"], return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "# model predicts class_queries_logits of shape `(batch_size, num_queries)`\n",
    "# and masks_queries_logits of shape `(batch_size, num_queries, height, width)`\n",
    "class_queries_logits = outputs.class_queries_logits\n",
    "masks_queries_logits = outputs.masks_queries_logits\n",
    "\n",
    "# you can pass them to processor for panoptic postprocessing\n",
    "predicted_panoptic_map = processor.post_process_panoptic_segmentation(\n",
    "    outputs, target_sizes=[image.size[::-1]]\n",
    ")[0][\"segmentation\"]\n",
    "f\" Panoptic Predictions Shape: {list(predicted_panoptic_map.shape)}\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
